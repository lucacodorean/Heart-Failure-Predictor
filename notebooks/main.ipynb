{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfedae6",
   "metadata": {},
   "source": [
    "# **Global Music Streaming Trends**\n",
    "Luca-Andrei Codorean, 30233-1 CTI-RO @2025\n",
    "\n",
    "This projects consists of an implementation of a text classifer that wishes to succesfully predict cases of heart failure.\n",
    "The used dataset can be found at: https://www.kaggle.com/datasets/atharvasoundankar/global-music-streaming-trends-and-listener-insights\n",
    "\n",
    "In order to proceed with the solution, the dependencies found in ``requirements.txt`` should be installed, using the following command ```pip install -r requirements.\n",
    "txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd937ee1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The really first step is realted to data-preprocessing and visualization. The first function will just take one of the three datasets obtained after ```scr/data_loader.py``` script has been run.\n",
    "The ```data_loader``` script splitted the dataset in three datasets as follows: train, test, and val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4fd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "def preprocess_data(dataset_path: str):\n",
    "    dataset_df = pd.read_csv(dataset_path)\n",
    "\n",
    "    scaler  = MinMaxScaler()\n",
    "    le_dict = {}\n",
    "\n",
    "    for column in dataset_df.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder() \n",
    "        dataset_df[column] = le.fit_transform(dataset_df[column])  \n",
    "        le_dict[column] = le  \n",
    "\n",
    "    y = dataset_df[\"Listening Time (Morning/Afternoon/Night)\"]\n",
    "    X = dataset_df.drop(columns=[\"Listening Time (Morning/Afternoon/Night)\"])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    return X_scaled_df, y, le_dict, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633faa3",
   "metadata": {},
   "source": [
    "### Plotting the histograms and class distribution\n",
    "\n",
    "An issue reagrading plotting the histograms has been identified. In the early stages of the development, the columns containing strings instead of numbers were unable to be plotted as histograms. For that, they were plotted as class distribution diagrams, firstly bars, then pie charts. \n",
    "\n",
    "It's been a problem with understanding the meaning of these columns so they were mapped accordingly. See ```preprocess_data``` function.\n",
    "\n",
    "Basically, the histograms are used for numerical columns whereas the class distribution diagrams are used for the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1421f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_visualization(X, le_dict, scaler, output_dir: str):\n",
    "\n",
    "\n",
    "    temp = scaler.inverse_transform(X)\n",
    "    df = pd.DataFrame(temp, columns=X.columns)\n",
    "\n",
    "    \n",
    "    for key  in le_dict:\n",
    "        if key in df.columns:\n",
    "            label_encoder = le_dict[key]\n",
    "            df[key] = label_encoder.inverse_transform(df[key].astype(int))\n",
    "\n",
    "        \n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            plt.title(f\"Histogram of {column}\")\n",
    "            plt.xlabel(column)\n",
    "            df[column].plot(kind='hist', bins=30, color='skyblue', edgecolor='black')\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        else:\n",
    "            plt.title(f\"Class distribution of {column}\")\n",
    "            category_counts = df[column].value_counts()\n",
    "            category_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "            plt.ylabel(\"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir+\"/\"}{column}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5790652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_formatted_file_path = \"/home/luca/SI/Project/data/preprocessed/\"\n",
    "\n",
    "train_dataset_path = \"/home/luca/SI/Project/data/raw/train.csv\"\n",
    "dataset_formatted_file_name = \"train.csv\"\n",
    "\n",
    "validation_dataset_path = \"/home/luca/SI/Project/data/raw/val.csv\"\n",
    "val_formtted_file_name = \"val.csv\"\n",
    "\n",
    "test_dataset_path = \"/home/luca/SI/Project/data/raw/test.csv\"\n",
    "test_formtted_file_name = \"test.csv\"\n",
    "\n",
    "(X, y, le_dict, scaler) = preprocess_data(dataset_path=train_dataset_path)\n",
    "(X_val, y_val, _, _)    = preprocess_data(dataset_path=validation_dataset_path)  \n",
    "(X_test, y_test, _, _)  = preprocess_data(dataset_path=test_dataset_path)\n",
    "\n",
    "# plot_visualization(X=X, le_dict=le_dict, scaler=scaler, output_dir=\"/home/luca/SI/Project/outputs/data_vizualization\")\n",
    "X.to_csv(os.path.join(dataset_formatted_file_path, dataset_formatted_file_name), index=False)\n",
    "X_val.to_csv(os.path.join(dataset_formatted_file_path, val_formtted_file_name), index=False)\n",
    "X_test.to_csv(os.path.join(dataset_formatted_file_path, test_formtted_file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e9e8a",
   "metadata": {},
   "source": [
    "## Constructing the model and the training process\n",
    " \n",
    "After the data has been pre-processed, the very first step is to combine the output of the pre-processing phase into a ```StreamingPreferencesDataset``` object. This way, we will be able to structure a MLNN easier. For this, the ```StreamingPreferencesDatasetMLP``` class has been created. It's implementation can be found in ```src.data_set.py```.\n",
    "\n",
    "Once the dataset object is set-up, it's attributes can be used to inialized the ```HeartFailureMLP``` object that is responsible to implement the training model. Its implementation is available in ```src.model.py```. \n",
    "\n",
    "An important hyperparameter for the training process is the ```batch_size``` used by the dataloader. The model should be tested using multiple values for the ```BATCH_SIZE``` parameter in order to get the best results. Same goes for the ```LEARNINIG_RATE``` parameter.\n",
    "\n",
    "```EPOCHS``` parameter denotes the number of times the algorithm goes through the dataset.\n",
    "\n",
    "Thus, the first code fragment will handle initalization of diferent hyperparameters and of the model.\n",
    "Another analisys will be done in order to observe model's reaction to different optimizers such as Adams, SDG, SDG with momentum.\n",
    "The ```scheduler``` is used to provide the model with an already-implemented learning-rate scheduler. Its purpose is to reduce LR's value in order to get better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57988a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamingPreferencesDatasetMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=16, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from src.data_set       import StreamingPreferencesDataset\n",
    "from src.model          import StreamingPreferencesDatasetMLP\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    " \n",
    "BATCH_SIZE                  = 16\n",
    "DROPOUT_PERCENTAGE          = 0.3\n",
    "LEARNING_RATE               = 1e-3\n",
    "OPTIMIZER_STEP_SIZE         = 50\n",
    "EARLY_STOPPING_PATIENCE     = 1000 # anuleaza early stoppingul\n",
    "EPOCHS                      = 1000\n",
    "DEVICE                      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "train_dataset   = StreamingPreferencesDataset(X, y)\n",
    "val_dataset     = StreamingPreferencesDataset(X_val, y_val)\n",
    "train_loader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader      = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels = [int(label) for _, label in train_dataset]\n",
    "classes = np.unique(all_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "model           = StreamingPreferencesDatasetMLP(dropout_percentage=DROPOUT_PERCENTAGE).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "criterion       = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.3)  \n",
    "# optimizer       = optim.SGD(model.parameters(), momentum=0.9, lr=LEARNING_RATE) \n",
    "optimizer     = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "# optimizer       = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.9, eps=1e-8)\n",
    "\n",
    "# scheduler      = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.7, patience=20, threshold_mode='rel',threshold=1e-4)\n",
    "scheduler     = optim.lr_scheduler.StepLR(step_size=OPTIMIZER_STEP_SIZE, optimizer=optimizer, gamma=0.4)\n",
    "# scheduler       = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f4fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics            import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.tensorboard    import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/64,CrossEntropyLoss,dp=0.3,StepLR,Adam(wd=1e-2, lr=2e-3),batch_size=16,EPOCHS=1000,2 layer\")\n",
    "\n",
    "def compute_metrics(all_preds, all_labels, phase, epoch):\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds,   average='macro', zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds,      average='macro', zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds,          average='macro', zero_division=0)\n",
    "\n",
    "    writer.add_scalar(f\"{phase}/Accuracy\", acc, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Precision\", prec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Recall\", rec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/F1-Score\", f1, epoch)\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa3f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Training Loss: 1.1097 Validation Loss: 1.1003\n",
      "Epoch [2/1000] Training Loss: 1.1014 Validation Loss: 1.1002\n",
      "Epoch [3/1000] Training Loss: 1.1006 Validation Loss: 1.1009\n",
      "Epoch [4/1000] Training Loss: 1.0998 Validation Loss: 1.1018\n",
      "Epoch [5/1000] Training Loss: 1.0977 Validation Loss: 1.1002\n",
      "Epoch [6/1000] Training Loss: 1.0982 Validation Loss: 1.1001\n",
      "Epoch [7/1000] Training Loss: 1.0985 Validation Loss: 1.1001\n",
      "Epoch [8/1000] Training Loss: 1.0978 Validation Loss: 1.1009\n",
      "Epoch [9/1000] Training Loss: 1.0982 Validation Loss: 1.1005\n",
      "Epoch [10/1000] Training Loss: 1.0980 Validation Loss: 1.1011\n",
      "Epoch [11/1000] Training Loss: 1.0967 Validation Loss: 1.1011\n",
      "Epoch [12/1000] Training Loss: 1.0970 Validation Loss: 1.1019\n",
      "Epoch [13/1000] Training Loss: 1.0969 Validation Loss: 1.1026\n",
      "Epoch [14/1000] Training Loss: 1.0970 Validation Loss: 1.1016\n",
      "Epoch [15/1000] Training Loss: 1.0970 Validation Loss: 1.1012\n",
      "Epoch [16/1000] Training Loss: 1.0969 Validation Loss: 1.1005\n",
      "Epoch [17/1000] Training Loss: 1.0973 Validation Loss: 1.1015\n",
      "Epoch [18/1000] Training Loss: 1.0970 Validation Loss: 1.1010\n",
      "Epoch [19/1000] Training Loss: 1.0973 Validation Loss: 1.1011\n",
      "Epoch [20/1000] Training Loss: 1.0962 Validation Loss: 1.1018\n",
      "Epoch [21/1000] Training Loss: 1.0970 Validation Loss: 1.1024\n",
      "Epoch [22/1000] Training Loss: 1.0976 Validation Loss: 1.1016\n",
      "Epoch [23/1000] Training Loss: 1.0967 Validation Loss: 1.1018\n",
      "Epoch [24/1000] Training Loss: 1.0980 Validation Loss: 1.1009\n",
      "Epoch [25/1000] Training Loss: 1.0975 Validation Loss: 1.1010\n",
      "Epoch [26/1000] Training Loss: 1.0973 Validation Loss: 1.1008\n",
      "Epoch [27/1000] Training Loss: 1.0971 Validation Loss: 1.1011\n",
      "Epoch [28/1000] Training Loss: 1.0979 Validation Loss: 1.1009\n",
      "Epoch [29/1000] Training Loss: 1.0976 Validation Loss: 1.1005\n",
      "Epoch [30/1000] Training Loss: 1.0975 Validation Loss: 1.1003\n",
      "Epoch [31/1000] Training Loss: 1.0975 Validation Loss: 1.1016\n",
      "Epoch [32/1000] Training Loss: 1.0978 Validation Loss: 1.1015\n",
      "Epoch [33/1000] Training Loss: 1.0970 Validation Loss: 1.1015\n",
      "Epoch [34/1000] Training Loss: 1.0981 Validation Loss: 1.1005\n",
      "Epoch [35/1000] Training Loss: 1.0982 Validation Loss: 1.1002\n",
      "Epoch [36/1000] Training Loss: 1.0973 Validation Loss: 1.1009\n",
      "Epoch [37/1000] Training Loss: 1.0973 Validation Loss: 1.1011\n",
      "Epoch [38/1000] Training Loss: 1.0971 Validation Loss: 1.1020\n",
      "Epoch [39/1000] Training Loss: 1.0971 Validation Loss: 1.1013\n",
      "Epoch [40/1000] Training Loss: 1.0976 Validation Loss: 1.1005\n",
      "Epoch [41/1000] Training Loss: 1.0976 Validation Loss: 1.1003\n",
      "Epoch [42/1000] Training Loss: 1.0976 Validation Loss: 1.1011\n",
      "Epoch [43/1000] Training Loss: 1.0981 Validation Loss: 1.1007\n",
      "Epoch [44/1000] Training Loss: 1.0978 Validation Loss: 1.0999\n",
      "Epoch [45/1000] Training Loss: 1.0978 Validation Loss: 1.1002\n",
      "Epoch [46/1000] Training Loss: 1.0978 Validation Loss: 1.1005\n",
      "Epoch [47/1000] Training Loss: 1.0979 Validation Loss: 1.0999\n",
      "Epoch [48/1000] Training Loss: 1.0981 Validation Loss: 1.1001\n",
      "Epoch [49/1000] Training Loss: 1.0978 Validation Loss: 1.1006\n",
      "Epoch [50/1000] Training Loss: 1.0985 Validation Loss: 1.1004\n",
      "Epoch [51/1000] Training Loss: 1.0983 Validation Loss: 1.1004\n",
      "Epoch [52/1000] Training Loss: 1.0980 Validation Loss: 1.1001\n",
      "Epoch [53/1000] Training Loss: 1.0976 Validation Loss: 1.1000\n",
      "Epoch [54/1000] Training Loss: 1.0980 Validation Loss: 1.1006\n",
      "Epoch [55/1000] Training Loss: 1.0974 Validation Loss: 1.1007\n",
      "Epoch [56/1000] Training Loss: 1.0981 Validation Loss: 1.1005\n",
      "Epoch [57/1000] Training Loss: 1.0976 Validation Loss: 1.1005\n",
      "Epoch [58/1000] Training Loss: 1.0977 Validation Loss: 1.1008\n",
      "Epoch [59/1000] Training Loss: 1.0980 Validation Loss: 1.1005\n",
      "Epoch [60/1000] Training Loss: 1.0976 Validation Loss: 1.1003\n",
      "Epoch [61/1000] Training Loss: 1.0980 Validation Loss: 1.1005\n",
      "Epoch [62/1000] Training Loss: 1.0977 Validation Loss: 1.1004\n",
      "Epoch [63/1000] Training Loss: 1.0976 Validation Loss: 1.1006\n",
      "Epoch [64/1000] Training Loss: 1.0970 Validation Loss: 1.1002\n",
      "Epoch [65/1000] Training Loss: 1.0976 Validation Loss: 1.1002\n",
      "Epoch [66/1000] Training Loss: 1.0971 Validation Loss: 1.1000\n",
      "Epoch [67/1000] Training Loss: 1.0973 Validation Loss: 1.0998\n",
      "Epoch [68/1000] Training Loss: 1.0974 Validation Loss: 1.1002\n",
      "Epoch [69/1000] Training Loss: 1.0971 Validation Loss: 1.1002\n",
      "Epoch [70/1000] Training Loss: 1.0967 Validation Loss: 1.0999\n",
      "Epoch [71/1000] Training Loss: 1.0972 Validation Loss: 1.1005\n",
      "Epoch [72/1000] Training Loss: 1.0965 Validation Loss: 1.1001\n",
      "Epoch [73/1000] Training Loss: 1.0968 Validation Loss: 1.1003\n",
      "Epoch [74/1000] Training Loss: 1.0971 Validation Loss: 1.0996\n",
      "Epoch [75/1000] Training Loss: 1.0965 Validation Loss: 1.0998\n",
      "Epoch [76/1000] Training Loss: 1.0979 Validation Loss: 1.1001\n",
      "Epoch [77/1000] Training Loss: 1.0966 Validation Loss: 1.1006\n",
      "Epoch [78/1000] Training Loss: 1.0971 Validation Loss: 1.1001\n",
      "Epoch [79/1000] Training Loss: 1.0969 Validation Loss: 1.1007\n",
      "Epoch [80/1000] Training Loss: 1.0964 Validation Loss: 1.1014\n",
      "Epoch [81/1000] Training Loss: 1.0968 Validation Loss: 1.1006\n",
      "Epoch [82/1000] Training Loss: 1.0964 Validation Loss: 1.1008\n",
      "Epoch [83/1000] Training Loss: 1.0966 Validation Loss: 1.1009\n",
      "Epoch [84/1000] Training Loss: 1.0966 Validation Loss: 1.1011\n",
      "Epoch [85/1000] Training Loss: 1.0962 Validation Loss: 1.1013\n",
      "Epoch [86/1000] Training Loss: 1.0962 Validation Loss: 1.1015\n",
      "Epoch [87/1000] Training Loss: 1.0966 Validation Loss: 1.1016\n",
      "Epoch [88/1000] Training Loss: 1.0960 Validation Loss: 1.1019\n",
      "Epoch [89/1000] Training Loss: 1.0961 Validation Loss: 1.1020\n",
      "Epoch [90/1000] Training Loss: 1.0972 Validation Loss: 1.1014\n",
      "Epoch [91/1000] Training Loss: 1.0974 Validation Loss: 1.1011\n",
      "Epoch [92/1000] Training Loss: 1.0978 Validation Loss: 1.1014\n",
      "Epoch [93/1000] Training Loss: 1.0966 Validation Loss: 1.1013\n",
      "Epoch [94/1000] Training Loss: 1.0966 Validation Loss: 1.1018\n",
      "Epoch [95/1000] Training Loss: 1.0965 Validation Loss: 1.1019\n",
      "Epoch [96/1000] Training Loss: 1.0971 Validation Loss: 1.1011\n",
      "Epoch [97/1000] Training Loss: 1.0970 Validation Loss: 1.1013\n",
      "Epoch [98/1000] Training Loss: 1.0967 Validation Loss: 1.1008\n",
      "Epoch [99/1000] Training Loss: 1.0969 Validation Loss: 1.1008\n",
      "Epoch [100/1000] Training Loss: 1.0966 Validation Loss: 1.1009\n",
      "Epoch [101/1000] Training Loss: 1.0956 Validation Loss: 1.1010\n",
      "Epoch [102/1000] Training Loss: 1.0964 Validation Loss: 1.1014\n",
      "Epoch [103/1000] Training Loss: 1.0964 Validation Loss: 1.1013\n",
      "Epoch [104/1000] Training Loss: 1.0962 Validation Loss: 1.1014\n",
      "Epoch [105/1000] Training Loss: 1.0972 Validation Loss: 1.1013\n",
      "Epoch [106/1000] Training Loss: 1.0957 Validation Loss: 1.1011\n",
      "Epoch [107/1000] Training Loss: 1.0958 Validation Loss: 1.1009\n",
      "Epoch [108/1000] Training Loss: 1.0956 Validation Loss: 1.1013\n",
      "Epoch [109/1000] Training Loss: 1.0963 Validation Loss: 1.1015\n",
      "Epoch [110/1000] Training Loss: 1.0960 Validation Loss: 1.1014\n",
      "Epoch [111/1000] Training Loss: 1.0967 Validation Loss: 1.1012\n",
      "Epoch [112/1000] Training Loss: 1.0965 Validation Loss: 1.1012\n",
      "Epoch [113/1000] Training Loss: 1.0946 Validation Loss: 1.1012\n",
      "Epoch [114/1000] Training Loss: 1.0956 Validation Loss: 1.1014\n",
      "Epoch [115/1000] Training Loss: 1.0951 Validation Loss: 1.1022\n",
      "Epoch [116/1000] Training Loss: 1.0953 Validation Loss: 1.1019\n",
      "Epoch [117/1000] Training Loss: 1.0961 Validation Loss: 1.1016\n",
      "Epoch [118/1000] Training Loss: 1.0955 Validation Loss: 1.1017\n",
      "Epoch [119/1000] Training Loss: 1.0956 Validation Loss: 1.1019\n",
      "Epoch [120/1000] Training Loss: 1.0954 Validation Loss: 1.1023\n",
      "Epoch [121/1000] Training Loss: 1.0953 Validation Loss: 1.1019\n",
      "Epoch [122/1000] Training Loss: 1.0958 Validation Loss: 1.1015\n",
      "Epoch [123/1000] Training Loss: 1.0955 Validation Loss: 1.1020\n",
      "Epoch [124/1000] Training Loss: 1.0949 Validation Loss: 1.1015\n",
      "Early stopping at epoch 124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    epochs_without_improvement = 0 \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss, total = 0.0, 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)  \n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        compute_metrics(train_preds, train_labels, \"train\", epoch)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_total = 0.0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)  \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        compute_metrics(val_preds, val_labels, \"val\", epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar('LearningRate', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/val', val_epoch_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {epoch_loss:.4f} Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "        if val_epoch_loss < best_loss:\n",
    "            best_loss = val_epoch_loss\n",
    "            epochs_without_improvement = 0\n",
    "            os.makedirs(\"./models\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"./models/predat_model_best.pth\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        if epoch > 30 and epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, EARLY_STOPPING_PATIENCE)\n",
    "torch.save(model.state_dict(), \"./models/codiax_model_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10200360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label  Percentage\n",
      "1  Afternoon        42.5\n",
      "0    Morning        41.9\n",
      "2      Night        15.6\n",
      "Accuracy: 0.342\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "predictions = []\n",
    "\n",
    "test_dataset     = StreamingPreferencesDataset(X_test, y_test)\n",
    "test_loader    = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:  \n",
    "        inputs, _ = batch  \n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  \n",
    "        predictions.append(preds.cpu())\n",
    "\n",
    "final_preds = torch.cat(predictions)\n",
    "final_preds_np = final_preds.numpy()\n",
    "\n",
    "label_map = {0: \"Morning\", 1: \"Afternoon\", 2: \"Night\"}\n",
    "decoded_preds = [label_map[int(p)] for p in final_preds_np]\n",
    "\n",
    "label_counts = Counter(decoded_preds)\n",
    "\n",
    "total = sum(label_counts.values())\n",
    "label_percentages = {label: (count / total) * 100 for label, count in label_counts.items()}\n",
    "\n",
    "percentages = {\n",
    "    'Label': list(label_percentages.keys()),\n",
    "    'Percentage': list(label_percentages.values())\n",
    "}\n",
    "\n",
    "df_percentages = pd.DataFrame(percentages)\n",
    "df_percentages_sorted = df_percentages.sort_values(by='Label')\n",
    "\n",
    "print(df_percentages_sorted)\n",
    "\n",
    "accuracy = accuracy_score(y_test, final_preds_np)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Heart Project)",
   "language": "python",
   "name": "heart-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
