{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfedae6",
   "metadata": {},
   "source": [
    "# **Heart Failure Predictor**\n",
    "Luca-Andrei Codorean, 30233-1 CTI-RO @2025\n",
    "\n",
    "This projects consists of an implementation of a text classifer that wishes to succesfully predict cases of heart failure.\n",
    "The used dataset can be found at: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data\n",
    "\n",
    "In order to proceed with the solution, the dependencies found in ``requirements.txt`` should be installed, using the following command ```pip install -r requirements.\n",
    "txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd937ee1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The really first step is realted to data-preprocessing and visualization. The first function will just take one of the three datasets obtained after ```scr/data_loader.py``` script has been run.\n",
    "The ```data_loader``` script splitted the dataset in three datasets as follows: train, test, and val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4fd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def preprocess_data(dataset_path: str):\n",
    "    dataset_df = pd.read_csv(dataset_path)\n",
    "\n",
    "    scaler  = StandardScaler()\n",
    "    le_dict = {}\n",
    "\n",
    "    for column in dataset_df.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder() \n",
    "        dataset_df[column] = le.fit_transform(dataset_df[column])  \n",
    "        le_dict[column] = le  \n",
    "\n",
    "    y = dataset_df[\"HeartDisease\"]\n",
    "    X = dataset_df.drop(columns=[\"HeartDisease\"])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    return X_scaled_df, y, le_dict, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633faa3",
   "metadata": {},
   "source": [
    "### Plotting the histograms and class distribution\n",
    "\n",
    "An issue reagrading plotting the histograms has been identified. In the early stages of the development, the columns containing strings instead of numbers were unable to be plotted as histograms. For that, they were plotted as class distribution diagrams, firstly bars, then pie charts. \n",
    "\n",
    "It's been a problem with understanding the meaning of these columns so they were mapped accordingly. See ```preprocess_data``` function.\n",
    "\n",
    "Basically, the histograms are used for numerical columns whereas the class distribution diagrams are used for the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1421f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_visualization(X, le_dict, scaler, output_dir: str):\n",
    "\n",
    "\n",
    "    temp = scaler.inverse_transform(X)\n",
    "    df = pd.DataFrame(temp, columns=X.columns)\n",
    "\n",
    "    for key  in le_dict:\n",
    "        label_encoder = le_dict[key]\n",
    "        df[key] = label_encoder.inverse_transform(df[key].astype(int))\n",
    "        \n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            plt.title(f\"Histogram of {column}\")\n",
    "            plt.xlabel(column)\n",
    "            df[column].plot(kind='hist', bins=30, color='skyblue', edgecolor='black')\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        else:\n",
    "            plt.title(f\"Class distribution of {column}\")\n",
    "            category_counts = df[column].value_counts()\n",
    "            category_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'skyblue', 'lightcoral', \"yellow\"])\n",
    "            plt.ylabel(\"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir+\"/\"}{column}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5790652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_formatted_file_path = \"/home/luca/SI/Project/data/preprocessed/\"\n",
    "\n",
    "train_dataset_path = \"/home/luca/SI/Project/data/raw/train.csv\"\n",
    "dataset_formatted_file_name = \"train.csv\"\n",
    "\n",
    "validation_dataset_path = \"/home/luca/SI/Project/data/raw/val.csv\"\n",
    "dataset_formtted_file_name = \"val.csv\"\n",
    "\n",
    "(X, y, le_dict, scaler) = preprocess_data(dataset_path=train_dataset_path)\n",
    "(X_val, y_val, _, _)    = preprocess_data(dataset_path=validation_dataset_path)  \n",
    "\n",
    "plot_visualization(X=X, le_dict=le_dict, scaler=scaler, output_dir=\"/home/luca/SI/Project/outputs/data_vizualization\")\n",
    "X.to_csv(os.path.join(dataset_formatted_file_path, dataset_formatted_file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e9e8a",
   "metadata": {},
   "source": [
    "## Constructing the model\n",
    "\n",
    "After the data has been pre-processed, the very first step is to combine the output of the pre-processing phase into a ```HeartFailureDataset``` object. This way, we will be able to structure a MLNN easier. For this, the ```HeartFailureDataset``` class has been created. It's implementation can be found in ```src.data_set.py```.\n",
    "\n",
    "Once the dataset object is set-up, it's attributes can be used to inialized the ```HeartFailureMLP``` object that is responsible to implement the training model. Its implementation is available in ```src.model.py```. \n",
    "\n",
    "An important hyperparameter for the training process is the ```batch_size``` used by the dataloader. The model should be tested using multiple values for the ```BATCH_SIZE``` parameter in order to get the best results. Same goes for the ```LEARNINIG_RATE``` parameter.\n",
    "\n",
    "```EPOCHS``` parameter denotes the number of times the algorithm goes through the dataset.\n",
    "\n",
    "Thus, the first code fragment will handle initalization of diferent hyperparameters and of the model.\n",
    "Another analisys will be done in order to observe model's reaction to different optimizers such as Adams, SDG, SDG with momentum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57988a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from src.data_set       import HeartFailureDataset\n",
    "from src.model          import HeartFailureMLP\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    " \n",
    "BATCH_SIZE      = 16\n",
    "LEARNING_RATE   = 1e-3\n",
    "EPOCHS          = 1000\n",
    "DEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "train_dataset   = HeartFailureDataset(X, y)\n",
    "val_dataset     = HeartFailureDataset(X_val, y_val)\n",
    "train_loader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader      = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = HeartFailureMLP().to(DEVICE)\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f4fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics            import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.tensorboard    import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/heart_failure_prediction\")\n",
    "\n",
    "def compute_metrics(all_preds, all_labels, phase, epoch):\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec  = recall_score(all_labels, all_preds)\n",
    "    f1   = f1_score(all_labels, all_preds)\n",
    "\n",
    "    writer.add_scalar(f\"{phase}/Accuracy\", acc, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Precision\", prec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Recall\", rec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/F1-Score\", f1, epoch)\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Training Loss: 0.6359\n",
      "Validation Loss: 0.5609\n",
      "Model saved!\n",
      "Epoch [2/1000] Training Loss: 0.4858\n",
      "Validation Loss: 0.4256\n",
      "Model saved!\n",
      "Epoch [3/1000] Training Loss: 0.3647\n",
      "Validation Loss: 0.3783\n",
      "Model saved!\n",
      "Epoch [4/1000] Training Loss: 0.3494\n",
      "Validation Loss: 0.3738\n",
      "Model saved!\n",
      "Epoch [5/1000] Training Loss: 0.3277\n",
      "Validation Loss: 0.3700\n",
      "Model saved!\n",
      "Epoch [6/1000] Training Loss: 0.3304\n",
      "Validation Loss: 0.3654\n",
      "Model saved!\n",
      "Epoch [7/1000] Training Loss: 0.3100\n",
      "Validation Loss: 0.3599\n",
      "Model saved!\n",
      "Epoch [8/1000] Training Loss: 0.3017\n",
      "Validation Loss: 0.3603\n",
      "Epoch [9/1000] Training Loss: 0.3005\n",
      "Validation Loss: 0.3523\n",
      "Model saved!\n",
      "Epoch [10/1000] Training Loss: 0.2909\n",
      "Validation Loss: 0.3552\n",
      "Epoch [11/1000] Training Loss: 0.2984\n",
      "Validation Loss: 0.3493\n",
      "Model saved!\n",
      "Epoch [12/1000] Training Loss: 0.2877\n",
      "Validation Loss: 0.3466\n",
      "Model saved!\n",
      "Epoch [13/1000] Training Loss: 0.2976\n",
      "Validation Loss: 0.3468\n",
      "Epoch [14/1000] Training Loss: 0.2872\n",
      "Validation Loss: 0.3407\n",
      "Model saved!\n",
      "Epoch [15/1000] Training Loss: 0.2903\n",
      "Validation Loss: 0.3383\n",
      "Model saved!\n",
      "Epoch [16/1000] Training Loss: 0.2621\n",
      "Validation Loss: 0.3402\n",
      "Epoch [17/1000] Training Loss: 0.2803\n",
      "Validation Loss: 0.3362\n",
      "Model saved!\n",
      "Epoch [18/1000] Training Loss: 0.2689\n",
      "Validation Loss: 0.3385\n",
      "Epoch [19/1000] Training Loss: 0.2725\n",
      "Validation Loss: 0.3390\n",
      "Epoch [20/1000] Training Loss: 0.2833\n",
      "Validation Loss: 0.3289\n",
      "Model saved!\n",
      "Epoch [21/1000] Training Loss: 0.2693\n",
      "Validation Loss: 0.3359\n",
      "Epoch [22/1000] Training Loss: 0.2610\n",
      "Validation Loss: 0.3331\n",
      "Epoch [23/1000] Training Loss: 0.2670\n",
      "Validation Loss: 0.3298\n",
      "Epoch [24/1000] Training Loss: 0.2505\n",
      "Validation Loss: 0.3375\n",
      "Epoch [25/1000] Training Loss: 0.2583\n",
      "Validation Loss: 0.3344\n",
      "Epoch [26/1000] Training Loss: 0.2525\n",
      "Validation Loss: 0.3300\n",
      "Epoch [27/1000] Training Loss: 0.2629\n",
      "Validation Loss: 0.3258\n",
      "Model saved!\n",
      "Epoch [28/1000] Training Loss: 0.2341\n",
      "Validation Loss: 0.3265\n",
      "Epoch [29/1000] Training Loss: 0.2422\n",
      "Validation Loss: 0.3240\n",
      "Model saved!\n",
      "Epoch [30/1000] Training Loss: 0.2511\n",
      "Validation Loss: 0.3246\n",
      "Epoch [31/1000] Training Loss: 0.2465\n",
      "Validation Loss: 0.3232\n",
      "Model saved!\n",
      "Epoch [32/1000] Training Loss: 0.2440\n",
      "Validation Loss: 0.3300\n",
      "Epoch [33/1000] Training Loss: 0.2573\n",
      "Validation Loss: 0.3312\n",
      "Epoch [34/1000] Training Loss: 0.2490\n",
      "Validation Loss: 0.3356\n",
      "Epoch [35/1000] Training Loss: 0.2489\n",
      "Validation Loss: 0.3230\n",
      "Model saved!\n",
      "Epoch [36/1000] Training Loss: 0.2410\n",
      "Validation Loss: 0.3190\n",
      "Model saved!\n",
      "Epoch [37/1000] Training Loss: 0.2292\n",
      "Validation Loss: 0.3303\n",
      "Epoch [38/1000] Training Loss: 0.2394\n",
      "Validation Loss: 0.3288\n",
      "Epoch [39/1000] Training Loss: 0.2273\n",
      "Validation Loss: 0.3278\n",
      "Epoch [40/1000] Training Loss: 0.2288\n",
      "Validation Loss: 0.3262\n",
      "Epoch [41/1000] Training Loss: 0.2213\n",
      "Validation Loss: 0.3208\n",
      "Epoch [42/1000] Training Loss: 0.2489\n",
      "Validation Loss: 0.3225\n",
      "Epoch [43/1000] Training Loss: 0.2341\n",
      "Validation Loss: 0.3271\n",
      "Epoch [44/1000] Training Loss: 0.2312\n",
      "Validation Loss: 0.3286\n",
      "Epoch [45/1000] Training Loss: 0.2155\n",
      "Validation Loss: 0.3238\n",
      "Epoch [46/1000] Training Loss: 0.2167\n",
      "Validation Loss: 0.3191\n",
      "Epoch [47/1000] Training Loss: 0.2324\n",
      "Validation Loss: 0.3188\n",
      "Model saved!\n",
      "Epoch [48/1000] Training Loss: 0.2312\n",
      "Validation Loss: 0.3224\n",
      "Epoch [49/1000] Training Loss: 0.2182\n",
      "Validation Loss: 0.3188\n",
      "Epoch [50/1000] Training Loss: 0.2177\n",
      "Validation Loss: 0.3188\n",
      "Model saved!\n",
      "Epoch [51/1000] Training Loss: 0.2135\n",
      "Validation Loss: 0.3289\n",
      "Epoch [52/1000] Training Loss: 0.2170\n",
      "Validation Loss: 0.3320\n",
      "Epoch [53/1000] Training Loss: 0.2144\n",
      "Validation Loss: 0.3331\n",
      "Epoch [54/1000] Training Loss: 0.2056\n",
      "Validation Loss: 0.3181\n",
      "Model saved!\n",
      "Epoch [55/1000] Training Loss: 0.2192\n",
      "Validation Loss: 0.3335\n",
      "Epoch [56/1000] Training Loss: 0.2070\n",
      "Validation Loss: 0.3270\n",
      "Epoch [57/1000] Training Loss: 0.2108\n",
      "Validation Loss: 0.3236\n",
      "Epoch [58/1000] Training Loss: 0.2123\n",
      "Validation Loss: 0.3193\n",
      "Epoch [59/1000] Training Loss: 0.1990\n",
      "Validation Loss: 0.3224\n",
      "Epoch [60/1000] Training Loss: 0.2032\n",
      "Validation Loss: 0.3303\n",
      "Epoch [61/1000] Training Loss: 0.2072\n",
      "Validation Loss: 0.3217\n",
      "Epoch [62/1000] Training Loss: 0.2047\n",
      "Validation Loss: 0.3273\n",
      "Epoch [63/1000] Training Loss: 0.1864\n",
      "Validation Loss: 0.3313\n",
      "Epoch [64/1000] Training Loss: 0.1973\n",
      "Validation Loss: 0.3358\n",
      "Epoch [65/1000] Training Loss: 0.1973\n",
      "Validation Loss: 0.3328\n",
      "Epoch [66/1000] Training Loss: 0.1973\n",
      "Validation Loss: 0.3322\n",
      "Epoch [67/1000] Training Loss: 0.1941\n",
      "Validation Loss: 0.3288\n",
      "Epoch [68/1000] Training Loss: 0.1813\n",
      "Validation Loss: 0.3279\n",
      "Epoch [69/1000] Training Loss: 0.1984\n",
      "Validation Loss: 0.3248\n",
      "Epoch [70/1000] Training Loss: 0.1989\n",
      "Validation Loss: 0.3330\n",
      "Epoch [71/1000] Training Loss: 0.1917\n",
      "Validation Loss: 0.3383\n",
      "Epoch [72/1000] Training Loss: 0.1958\n",
      "Validation Loss: 0.3369\n",
      "Epoch [73/1000] Training Loss: 0.2038\n",
      "Validation Loss: 0.3247\n",
      "Epoch [74/1000] Training Loss: 0.1818\n",
      "Validation Loss: 0.3218\n",
      "Epoch [75/1000] Training Loss: 0.1916\n",
      "Validation Loss: 0.3190\n",
      "Epoch [76/1000] Training Loss: 0.1861\n",
      "Validation Loss: 0.3385\n",
      "Epoch [77/1000] Training Loss: 0.1949\n",
      "Validation Loss: 0.3351\n",
      "Epoch [78/1000] Training Loss: 0.1992\n",
      "Validation Loss: 0.3449\n",
      "Epoch [79/1000] Training Loss: 0.1810\n",
      "Validation Loss: 0.3298\n",
      "Epoch [80/1000] Training Loss: 0.1765\n",
      "Validation Loss: 0.3427\n",
      "Epoch [81/1000] Training Loss: 0.1898\n",
      "Validation Loss: 0.3422\n",
      "Epoch [82/1000] Training Loss: 0.2054\n",
      "Validation Loss: 0.3372\n",
      "Epoch [83/1000] Training Loss: 0.1870\n",
      "Validation Loss: 0.3292\n",
      "Epoch [84/1000] Training Loss: 0.1877\n",
      "Validation Loss: 0.3429\n",
      "Epoch [85/1000] Training Loss: 0.1782\n",
      "Validation Loss: 0.3440\n",
      "Epoch [86/1000] Training Loss: 0.2033\n",
      "Validation Loss: 0.3409\n",
      "Epoch [87/1000] Training Loss: 0.1847\n",
      "Validation Loss: 0.3348\n",
      "Epoch [88/1000] Training Loss: 0.1837\n",
      "Validation Loss: 0.3372\n",
      "Epoch [89/1000] Training Loss: 0.1957\n",
      "Validation Loss: 0.3367\n",
      "Epoch [90/1000] Training Loss: 0.1904\n",
      "Validation Loss: 0.3353\n",
      "Epoch [91/1000] Training Loss: 0.1724\n",
      "Validation Loss: 0.3495\n",
      "Epoch [92/1000] Training Loss: 0.1852\n",
      "Validation Loss: 0.3432\n",
      "Epoch [93/1000] Training Loss: 0.1791\n",
      "Validation Loss: 0.3424\n",
      "Epoch [94/1000] Training Loss: 0.1917\n",
      "Validation Loss: 0.3437\n",
      "Epoch [95/1000] Training Loss: 0.1997\n",
      "Validation Loss: 0.3411\n",
      "Epoch [96/1000] Training Loss: 0.1745\n",
      "Validation Loss: 0.3388\n",
      "Epoch [97/1000] Training Loss: 0.1796\n",
      "Validation Loss: 0.3375\n",
      "Epoch [98/1000] Training Loss: 0.1885\n",
      "Validation Loss: 0.3480\n",
      "Epoch [99/1000] Training Loss: 0.1658\n",
      "Validation Loss: 0.3548\n",
      "Epoch [100/1000] Training Loss: 0.1818\n",
      "Validation Loss: 0.3586\n",
      "Epoch [101/1000] Training Loss: 0.1724\n",
      "Validation Loss: 0.3626\n",
      "Epoch [102/1000] Training Loss: 0.1668\n",
      "Validation Loss: 0.3504\n",
      "Epoch [103/1000] Training Loss: 0.1776\n",
      "Validation Loss: 0.3599\n",
      "Epoch [104/1000] Training Loss: 0.1721\n",
      "Validation Loss: 0.3535\n",
      "Epoch [105/1000] Training Loss: 0.1636\n",
      "Validation Loss: 0.3495\n",
      "Epoch [106/1000] Training Loss: 0.1851\n",
      "Validation Loss: 0.3712\n",
      "Epoch [107/1000] Training Loss: 0.1832\n",
      "Validation Loss: 0.3559\n",
      "Epoch [108/1000] Training Loss: 0.1757\n",
      "Validation Loss: 0.3562\n",
      "Epoch [109/1000] Training Loss: 0.1690\n",
      "Validation Loss: 0.3567\n",
      "Epoch [110/1000] Training Loss: 0.1769\n",
      "Validation Loss: 0.3560\n",
      "Epoch [111/1000] Training Loss: 0.1624\n",
      "Validation Loss: 0.3628\n",
      "Epoch [112/1000] Training Loss: 0.1689\n",
      "Validation Loss: 0.3590\n",
      "Epoch [113/1000] Training Loss: 0.1638\n",
      "Validation Loss: 0.3591\n",
      "Epoch [114/1000] Training Loss: 0.1880\n",
      "Validation Loss: 0.3535\n",
      "Epoch [115/1000] Training Loss: 0.1803\n",
      "Validation Loss: 0.3623\n",
      "Epoch [116/1000] Training Loss: 0.1618\n",
      "Validation Loss: 0.3572\n",
      "Epoch [117/1000] Training Loss: 0.1637\n",
      "Validation Loss: 0.3640\n",
      "Epoch [118/1000] Training Loss: 0.1688\n",
      "Validation Loss: 0.3801\n",
      "Epoch [119/1000] Training Loss: 0.1878\n",
      "Validation Loss: 0.3546\n",
      "Epoch [120/1000] Training Loss: 0.1617\n",
      "Validation Loss: 0.3761\n",
      "Epoch [121/1000] Training Loss: 0.1592\n",
      "Validation Loss: 0.3705\n",
      "Epoch [122/1000] Training Loss: 0.1580\n",
      "Validation Loss: 0.3798\n",
      "Epoch [123/1000] Training Loss: 0.1646\n",
      "Validation Loss: 0.3716\n",
      "Epoch [124/1000] Training Loss: 0.1572\n",
      "Validation Loss: 0.3753\n",
      "Epoch [125/1000] Training Loss: 0.1578\n",
      "Validation Loss: 0.3855\n",
      "Epoch [126/1000] Training Loss: 0.1689\n",
      "Validation Loss: 0.3921\n",
      "Epoch [127/1000] Training Loss: 0.1646\n",
      "Validation Loss: 0.3775\n",
      "Epoch [128/1000] Training Loss: 0.1715\n",
      "Validation Loss: 0.3734\n",
      "Epoch [129/1000] Training Loss: 0.1544\n",
      "Validation Loss: 0.3777\n",
      "Epoch [130/1000] Training Loss: 0.1551\n",
      "Validation Loss: 0.3915\n",
      "Epoch [131/1000] Training Loss: 0.1581\n",
      "Validation Loss: 0.3885\n",
      "Epoch [132/1000] Training Loss: 0.1654\n",
      "Validation Loss: 0.3796\n",
      "Epoch [133/1000] Training Loss: 0.1576\n",
      "Validation Loss: 0.3839\n",
      "Epoch [134/1000] Training Loss: 0.1589\n",
      "Validation Loss: 0.3742\n",
      "Epoch [135/1000] Training Loss: 0.1657\n",
      "Validation Loss: 0.3761\n",
      "Epoch [136/1000] Training Loss: 0.1612\n",
      "Validation Loss: 0.3736\n",
      "Epoch [137/1000] Training Loss: 0.1541\n",
      "Validation Loss: 0.3837\n",
      "Epoch [138/1000] Training Loss: 0.1589\n",
      "Validation Loss: 0.3832\n",
      "Epoch [139/1000] Training Loss: 0.1490\n",
      "Validation Loss: 0.3832\n",
      "Epoch [140/1000] Training Loss: 0.1500\n",
      "Validation Loss: 0.3801\n",
      "Epoch [141/1000] Training Loss: 0.1576\n",
      "Validation Loss: 0.3839\n",
      "Epoch [142/1000] Training Loss: 0.1661\n",
      "Validation Loss: 0.3846\n",
      "Epoch [143/1000] Training Loss: 0.1545\n",
      "Validation Loss: 0.3830\n",
      "Epoch [144/1000] Training Loss: 0.1625\n",
      "Validation Loss: 0.3745\n",
      "Epoch [145/1000] Training Loss: 0.1505\n",
      "Validation Loss: 0.3766\n",
      "Epoch [146/1000] Training Loss: 0.1478\n",
      "Validation Loss: 0.3823\n",
      "Epoch [147/1000] Training Loss: 0.1319\n",
      "Validation Loss: 0.3647\n",
      "Epoch [148/1000] Training Loss: 0.1566\n",
      "Validation Loss: 0.3930\n",
      "Epoch [149/1000] Training Loss: 0.1357\n",
      "Validation Loss: 0.3916\n",
      "Epoch [150/1000] Training Loss: 0.1472\n",
      "Validation Loss: 0.4065\n",
      "Epoch [151/1000] Training Loss: 0.1608\n",
      "Validation Loss: 0.3944\n",
      "Epoch [152/1000] Training Loss: 0.1452\n",
      "Validation Loss: 0.3899\n",
      "Epoch [153/1000] Training Loss: 0.1429\n",
      "Validation Loss: 0.4033\n",
      "Epoch [154/1000] Training Loss: 0.1463\n",
      "Validation Loss: 0.4251\n",
      "Epoch [155/1000] Training Loss: 0.1404\n",
      "Validation Loss: 0.4227\n",
      "Epoch [156/1000] Training Loss: 0.1372\n",
      "Validation Loss: 0.4252\n",
      "Epoch [157/1000] Training Loss: 0.1530\n",
      "Validation Loss: 0.4136\n",
      "Epoch [158/1000] Training Loss: 0.1342\n",
      "Validation Loss: 0.4141\n",
      "Epoch [159/1000] Training Loss: 0.1488\n",
      "Validation Loss: 0.3968\n",
      "Epoch [160/1000] Training Loss: 0.1581\n",
      "Validation Loss: 0.4222\n",
      "Epoch [161/1000] Training Loss: 0.1387\n",
      "Validation Loss: 0.4050\n",
      "Epoch [162/1000] Training Loss: 0.1389\n",
      "Validation Loss: 0.4008\n",
      "Epoch [163/1000] Training Loss: 0.1396\n",
      "Validation Loss: 0.4142\n",
      "Epoch [164/1000] Training Loss: 0.1509\n",
      "Validation Loss: 0.4348\n",
      "Epoch [165/1000] Training Loss: 0.1417\n",
      "Validation Loss: 0.4144\n",
      "Epoch [166/1000] Training Loss: 0.1428\n",
      "Validation Loss: 0.3954\n",
      "Epoch [167/1000] Training Loss: 0.1339\n",
      "Validation Loss: 0.4014\n",
      "Epoch [168/1000] Training Loss: 0.1292\n",
      "Validation Loss: 0.3980\n",
      "Epoch [169/1000] Training Loss: 0.1321\n",
      "Validation Loss: 0.4236\n",
      "Epoch [170/1000] Training Loss: 0.1350\n",
      "Validation Loss: 0.4325\n",
      "Epoch [171/1000] Training Loss: 0.1420\n",
      "Validation Loss: 0.4435\n",
      "Epoch [172/1000] Training Loss: 0.1370\n",
      "Validation Loss: 0.4250\n",
      "Epoch [173/1000] Training Loss: 0.1354\n",
      "Validation Loss: 0.4214\n",
      "Epoch [174/1000] Training Loss: 0.1338\n",
      "Validation Loss: 0.4101\n",
      "Epoch [175/1000] Training Loss: 0.1273\n",
      "Validation Loss: 0.4107\n",
      "Epoch [176/1000] Training Loss: 0.1281\n",
      "Validation Loss: 0.4260\n",
      "Epoch [177/1000] Training Loss: 0.1358\n",
      "Validation Loss: 0.4210\n",
      "Epoch [178/1000] Training Loss: 0.1239\n",
      "Validation Loss: 0.4148\n",
      "Epoch [179/1000] Training Loss: 0.1525\n",
      "Validation Loss: 0.4124\n",
      "Epoch [180/1000] Training Loss: 0.1280\n",
      "Validation Loss: 0.4235\n",
      "Epoch [181/1000] Training Loss: 0.1315\n",
      "Validation Loss: 0.4371\n",
      "Epoch [182/1000] Training Loss: 0.1176\n",
      "Validation Loss: 0.4447\n",
      "Epoch [183/1000] Training Loss: 0.1418\n",
      "Validation Loss: 0.4463\n",
      "Epoch [184/1000] Training Loss: 0.1353\n",
      "Validation Loss: 0.4311\n",
      "Epoch [185/1000] Training Loss: 0.1310\n",
      "Validation Loss: 0.4425\n",
      "Epoch [186/1000] Training Loss: 0.1442\n",
      "Validation Loss: 0.4530\n",
      "Epoch [187/1000] Training Loss: 0.1212\n",
      "Validation Loss: 0.4363\n",
      "Epoch [188/1000] Training Loss: 0.1254\n",
      "Validation Loss: 0.4447\n",
      "Epoch [189/1000] Training Loss: 0.1462\n",
      "Validation Loss: 0.4348\n",
      "Epoch [190/1000] Training Loss: 0.1254\n",
      "Validation Loss: 0.4402\n",
      "Epoch [191/1000] Training Loss: 0.1294\n",
      "Validation Loss: 0.4381\n",
      "Epoch [192/1000] Training Loss: 0.1338\n",
      "Validation Loss: 0.4317\n",
      "Epoch [193/1000] Training Loss: 0.1205\n",
      "Validation Loss: 0.4430\n",
      "Epoch [194/1000] Training Loss: 0.1237\n",
      "Validation Loss: 0.4487\n",
      "Epoch [195/1000] Training Loss: 0.1295\n",
      "Validation Loss: 0.4520\n",
      "Epoch [196/1000] Training Loss: 0.1391\n",
      "Validation Loss: 0.4534\n",
      "Epoch [197/1000] Training Loss: 0.1360\n",
      "Validation Loss: 0.4230\n",
      "Epoch [198/1000] Training Loss: 0.1172\n",
      "Validation Loss: 0.4272\n",
      "Epoch [199/1000] Training Loss: 0.1291\n",
      "Validation Loss: 0.4328\n",
      "Epoch [200/1000] Training Loss: 0.1138\n",
      "Validation Loss: 0.4522\n",
      "Epoch [201/1000] Training Loss: 0.1225\n",
      "Validation Loss: 0.5083\n",
      "Epoch [202/1000] Training Loss: 0.1269\n",
      "Validation Loss: 0.5010\n",
      "Epoch [203/1000] Training Loss: 0.1086\n",
      "Validation Loss: 0.4888\n",
      "Epoch [204/1000] Training Loss: 0.1320\n",
      "Validation Loss: 0.4910\n",
      "Epoch [205/1000] Training Loss: 0.1110\n",
      "Validation Loss: 0.4639\n",
      "Epoch [206/1000] Training Loss: 0.1162\n",
      "Validation Loss: 0.4712\n",
      "Epoch [207/1000] Training Loss: 0.1175\n",
      "Validation Loss: 0.4717\n",
      "Epoch [208/1000] Training Loss: 0.1371\n",
      "Validation Loss: 0.4636\n",
      "Epoch [209/1000] Training Loss: 0.1293\n",
      "Validation Loss: 0.4536\n",
      "Epoch [210/1000] Training Loss: 0.1287\n",
      "Validation Loss: 0.4977\n",
      "Epoch [211/1000] Training Loss: 0.1236\n",
      "Validation Loss: 0.4830\n",
      "Epoch [212/1000] Training Loss: 0.1164\n",
      "Validation Loss: 0.4742\n",
      "Epoch [213/1000] Training Loss: 0.0951\n",
      "Validation Loss: 0.4761\n",
      "Epoch [214/1000] Training Loss: 0.1278\n",
      "Validation Loss: 0.4927\n",
      "Epoch [215/1000] Training Loss: 0.1163\n",
      "Validation Loss: 0.4979\n",
      "Epoch [216/1000] Training Loss: 0.1077\n",
      "Validation Loss: 0.4859\n",
      "Epoch [217/1000] Training Loss: 0.1136\n",
      "Validation Loss: 0.5012\n",
      "Epoch [218/1000] Training Loss: 0.1195\n",
      "Validation Loss: 0.4758\n",
      "Epoch [219/1000] Training Loss: 0.1110\n",
      "Validation Loss: 0.4659\n",
      "Epoch [220/1000] Training Loss: 0.1326\n",
      "Validation Loss: 0.5002\n",
      "Epoch [221/1000] Training Loss: 0.1081\n",
      "Validation Loss: 0.5173\n",
      "Epoch [222/1000] Training Loss: 0.1245\n",
      "Validation Loss: 0.5051\n",
      "Epoch [223/1000] Training Loss: 0.1064\n",
      "Validation Loss: 0.5074\n",
      "Epoch [224/1000] Training Loss: 0.1089\n",
      "Validation Loss: 0.4945\n",
      "Epoch [225/1000] Training Loss: 0.1223\n",
      "Validation Loss: 0.4861\n",
      "Epoch [226/1000] Training Loss: 0.1206\n",
      "Validation Loss: 0.5131\n",
      "Epoch [227/1000] Training Loss: 0.1153\n",
      "Validation Loss: 0.5159\n",
      "Epoch [228/1000] Training Loss: 0.1194\n",
      "Validation Loss: 0.4832\n",
      "Epoch [229/1000] Training Loss: 0.1053\n",
      "Validation Loss: 0.4789\n",
      "Epoch [230/1000] Training Loss: 0.1202\n",
      "Validation Loss: 0.4644\n",
      "Epoch [231/1000] Training Loss: 0.1081\n",
      "Validation Loss: 0.4719\n",
      "Epoch [232/1000] Training Loss: 0.1169\n",
      "Validation Loss: 0.5024\n",
      "Epoch [233/1000] Training Loss: 0.1093\n",
      "Validation Loss: 0.5068\n",
      "Epoch [234/1000] Training Loss: 0.1150\n",
      "Validation Loss: 0.4921\n",
      "Epoch [235/1000] Training Loss: 0.1030\n",
      "Validation Loss: 0.4945\n",
      "Epoch [236/1000] Training Loss: 0.1093\n",
      "Validation Loss: 0.4982\n",
      "Epoch [237/1000] Training Loss: 0.1224\n",
      "Validation Loss: 0.4995\n",
      "Epoch [238/1000] Training Loss: 0.1367\n",
      "Validation Loss: 0.4889\n",
      "Epoch [239/1000] Training Loss: 0.1172\n",
      "Validation Loss: 0.4770\n",
      "Epoch [240/1000] Training Loss: 0.1059\n",
      "Validation Loss: 0.5038\n",
      "Epoch [241/1000] Training Loss: 0.1046\n",
      "Validation Loss: 0.4955\n",
      "Epoch [242/1000] Training Loss: 0.1355\n",
      "Validation Loss: 0.4980\n",
      "Epoch [243/1000] Training Loss: 0.1119\n",
      "Validation Loss: 0.5256\n",
      "Epoch [244/1000] Training Loss: 0.1041\n",
      "Validation Loss: 0.5227\n",
      "Epoch [245/1000] Training Loss: 0.1009\n",
      "Validation Loss: 0.5009\n",
      "Epoch [246/1000] Training Loss: 0.1085\n",
      "Validation Loss: 0.5023\n",
      "Epoch [247/1000] Training Loss: 0.0999\n",
      "Validation Loss: 0.5135\n",
      "Epoch [248/1000] Training Loss: 0.1054\n",
      "Validation Loss: 0.5200\n",
      "Epoch [249/1000] Training Loss: 0.0887\n",
      "Validation Loss: 0.5277\n",
      "Epoch [250/1000] Training Loss: 0.1007\n",
      "Validation Loss: 0.5347\n",
      "Epoch [251/1000] Training Loss: 0.1006\n",
      "Validation Loss: 0.5364\n",
      "Epoch [252/1000] Training Loss: 0.1281\n",
      "Validation Loss: 0.5417\n",
      "Epoch [253/1000] Training Loss: 0.1086\n",
      "Validation Loss: 0.5368\n",
      "Epoch [254/1000] Training Loss: 0.1091\n",
      "Validation Loss: 0.5623\n",
      "Epoch [255/1000] Training Loss: 0.1162\n",
      "Validation Loss: 0.5264\n",
      "Epoch [256/1000] Training Loss: 0.0999\n",
      "Validation Loss: 0.5220\n",
      "Epoch [257/1000] Training Loss: 0.0994\n",
      "Validation Loss: 0.5279\n",
      "Epoch [258/1000] Training Loss: 0.0977\n",
      "Validation Loss: 0.5292\n",
      "Epoch [259/1000] Training Loss: 0.1032\n",
      "Validation Loss: 0.5232\n",
      "Epoch [260/1000] Training Loss: 0.0949\n",
      "Validation Loss: 0.5435\n",
      "Epoch [261/1000] Training Loss: 0.0903\n",
      "Validation Loss: 0.5672\n",
      "Epoch [262/1000] Training Loss: 0.0952\n",
      "Validation Loss: 0.5735\n",
      "Epoch [263/1000] Training Loss: 0.0975\n",
      "Validation Loss: 0.5425\n",
      "Epoch [264/1000] Training Loss: 0.0998\n",
      "Validation Loss: 0.5505\n",
      "Epoch [265/1000] Training Loss: 0.1229\n",
      "Validation Loss: 0.5363\n",
      "Epoch [266/1000] Training Loss: 0.0989\n",
      "Validation Loss: 0.5220\n",
      "Epoch [267/1000] Training Loss: 0.1073\n",
      "Validation Loss: 0.5203\n",
      "Epoch [268/1000] Training Loss: 0.1184\n",
      "Validation Loss: 0.5276\n",
      "Epoch [269/1000] Training Loss: 0.1061\n",
      "Validation Loss: 0.5238\n",
      "Epoch [270/1000] Training Loss: 0.0964\n",
      "Validation Loss: 0.5311\n",
      "Epoch [271/1000] Training Loss: 0.0779\n",
      "Validation Loss: 0.5405\n",
      "Epoch [272/1000] Training Loss: 0.1088\n",
      "Validation Loss: 0.5399\n",
      "Epoch [273/1000] Training Loss: 0.1004\n",
      "Validation Loss: 0.5621\n",
      "Epoch [274/1000] Training Loss: 0.0950\n",
      "Validation Loss: 0.5671\n",
      "Epoch [275/1000] Training Loss: 0.0934\n",
      "Validation Loss: 0.5705\n",
      "Epoch [276/1000] Training Loss: 0.0747\n",
      "Validation Loss: 0.5863\n",
      "Epoch [277/1000] Training Loss: 0.1007\n",
      "Validation Loss: 0.5697\n",
      "Epoch [278/1000] Training Loss: 0.1041\n",
      "Validation Loss: 0.5770\n",
      "Epoch [279/1000] Training Loss: 0.0968\n",
      "Validation Loss: 0.5835\n",
      "Epoch [280/1000] Training Loss: 0.0911\n",
      "Validation Loss: 0.5758\n",
      "Epoch [281/1000] Training Loss: 0.1096\n",
      "Validation Loss: 1.1476\n",
      "Epoch [282/1000] Training Loss: 0.1181\n",
      "Validation Loss: 1.1293\n",
      "Epoch [283/1000] Training Loss: 0.0967\n",
      "Validation Loss: 1.1468\n",
      "Epoch [284/1000] Training Loss: 0.1045\n",
      "Validation Loss: 0.5852\n",
      "Epoch [285/1000] Training Loss: 0.0954\n",
      "Validation Loss: 0.5818\n",
      "Epoch [286/1000] Training Loss: 0.1067\n",
      "Validation Loss: 0.5833\n",
      "Epoch [287/1000] Training Loss: 0.0823\n",
      "Validation Loss: 1.1603\n",
      "Epoch [288/1000] Training Loss: 0.0874\n",
      "Validation Loss: 0.5789\n",
      "Epoch [289/1000] Training Loss: 0.0942\n",
      "Validation Loss: 0.5677\n",
      "Epoch [290/1000] Training Loss: 0.0965\n",
      "Validation Loss: 0.5624\n",
      "Epoch [291/1000] Training Loss: 0.1014\n",
      "Validation Loss: 0.5528\n",
      "Epoch [292/1000] Training Loss: 0.1147\n",
      "Validation Loss: 0.5456\n",
      "Epoch [293/1000] Training Loss: 0.0878\n",
      "Validation Loss: 0.5326\n",
      "Epoch [294/1000] Training Loss: 0.0894\n",
      "Validation Loss: 0.5446\n",
      "Epoch [295/1000] Training Loss: 0.0949\n",
      "Validation Loss: 0.6004\n",
      "Epoch [296/1000] Training Loss: 0.0842\n",
      "Validation Loss: 0.5964\n",
      "Epoch [297/1000] Training Loss: 0.0807\n",
      "Validation Loss: 0.5697\n",
      "Epoch [298/1000] Training Loss: 0.0876\n",
      "Validation Loss: 0.5655\n",
      "Epoch [299/1000] Training Loss: 0.0926\n",
      "Validation Loss: 0.5793\n",
      "Epoch [300/1000] Training Loss: 0.0947\n",
      "Validation Loss: 0.5870\n",
      "Epoch [301/1000] Training Loss: 0.0962\n",
      "Validation Loss: 0.5458\n",
      "Epoch [302/1000] Training Loss: 0.0820\n",
      "Validation Loss: 0.5560\n",
      "Epoch [303/1000] Training Loss: 0.0840\n",
      "Validation Loss: 0.5615\n",
      "Epoch [304/1000] Training Loss: 0.0927\n",
      "Validation Loss: 0.5686\n",
      "Epoch [305/1000] Training Loss: 0.0894\n",
      "Validation Loss: 0.5752\n",
      "Epoch [306/1000] Training Loss: 0.0960\n",
      "Validation Loss: 0.5693\n",
      "Epoch [307/1000] Training Loss: 0.1066\n",
      "Validation Loss: 0.5712\n",
      "Epoch [308/1000] Training Loss: 0.0928\n",
      "Validation Loss: 0.5749\n",
      "Epoch [309/1000] Training Loss: 0.1157\n",
      "Validation Loss: 0.5397\n",
      "Epoch [310/1000] Training Loss: 0.0904\n",
      "Validation Loss: 0.5630\n",
      "Epoch [311/1000] Training Loss: 0.0933\n",
      "Validation Loss: 0.5705\n",
      "Epoch [312/1000] Training Loss: 0.0854\n",
      "Validation Loss: 0.5714\n",
      "Epoch [313/1000] Training Loss: 0.0855\n",
      "Validation Loss: 0.5867\n",
      "Epoch [314/1000] Training Loss: 0.0774\n",
      "Validation Loss: 1.1582\n",
      "Epoch [315/1000] Training Loss: 0.0732\n",
      "Validation Loss: 1.1517\n",
      "Epoch [316/1000] Training Loss: 0.0855\n",
      "Validation Loss: 1.1659\n",
      "Epoch [317/1000] Training Loss: 0.0850\n",
      "Validation Loss: 1.1736\n",
      "Epoch [318/1000] Training Loss: 0.0990\n",
      "Validation Loss: 1.1774\n",
      "Epoch [319/1000] Training Loss: 0.0874\n",
      "Validation Loss: 1.1784\n",
      "Epoch [320/1000] Training Loss: 0.0765\n",
      "Validation Loss: 1.1882\n",
      "Epoch [321/1000] Training Loss: 0.0967\n",
      "Validation Loss: 1.1751\n",
      "Epoch [322/1000] Training Loss: 0.0912\n",
      "Validation Loss: 1.1672\n",
      "Epoch [323/1000] Training Loss: 0.0778\n",
      "Validation Loss: 1.1748\n",
      "Epoch [324/1000] Training Loss: 0.0781\n",
      "Validation Loss: 1.1886\n",
      "Epoch [325/1000] Training Loss: 0.0898\n",
      "Validation Loss: 1.1873\n",
      "Epoch [326/1000] Training Loss: 0.0845\n",
      "Validation Loss: 1.1851\n",
      "Epoch [327/1000] Training Loss: 0.0706\n",
      "Validation Loss: 1.1811\n",
      "Epoch [328/1000] Training Loss: 0.0895\n",
      "Validation Loss: 1.1855\n",
      "Epoch [329/1000] Training Loss: 0.0808\n",
      "Validation Loss: 1.2015\n",
      "Epoch [330/1000] Training Loss: 0.1031\n",
      "Validation Loss: 1.1797\n",
      "Epoch [331/1000] Training Loss: 0.0906\n",
      "Validation Loss: 1.1760\n",
      "Epoch [332/1000] Training Loss: 0.0692\n",
      "Validation Loss: 1.1813\n",
      "Epoch [333/1000] Training Loss: 0.0851\n",
      "Validation Loss: 1.1599\n",
      "Epoch [334/1000] Training Loss: 0.0656\n",
      "Validation Loss: 1.1703\n",
      "Epoch [335/1000] Training Loss: 0.0688\n",
      "Validation Loss: 1.1807\n",
      "Epoch [336/1000] Training Loss: 0.0929\n",
      "Validation Loss: 1.1669\n",
      "Epoch [337/1000] Training Loss: 0.0819\n",
      "Validation Loss: 1.1724\n",
      "Epoch [338/1000] Training Loss: 0.0741\n",
      "Validation Loss: 1.1657\n",
      "Epoch [339/1000] Training Loss: 0.0738\n",
      "Validation Loss: 1.1822\n",
      "Epoch [340/1000] Training Loss: 0.0750\n",
      "Validation Loss: 1.1728\n",
      "Epoch [341/1000] Training Loss: 0.0733\n",
      "Validation Loss: 1.1755\n",
      "Epoch [342/1000] Training Loss: 0.0643\n",
      "Validation Loss: 1.2082\n",
      "Epoch [343/1000] Training Loss: 0.0720\n",
      "Validation Loss: 1.1980\n",
      "Epoch [344/1000] Training Loss: 0.0761\n",
      "Validation Loss: 1.2015\n",
      "Epoch [345/1000] Training Loss: 0.0941\n",
      "Validation Loss: 1.1716\n",
      "Epoch [346/1000] Training Loss: 0.0806\n",
      "Validation Loss: 1.1832\n",
      "Epoch [347/1000] Training Loss: 0.0881\n",
      "Validation Loss: 1.2036\n",
      "Epoch [348/1000] Training Loss: 0.0977\n",
      "Validation Loss: 1.1913\n",
      "Epoch [349/1000] Training Loss: 0.0873\n",
      "Validation Loss: 1.1945\n",
      "Epoch [350/1000] Training Loss: 0.0826\n",
      "Validation Loss: 1.1926\n",
      "Epoch [351/1000] Training Loss: 0.0817\n",
      "Validation Loss: 1.1987\n",
      "Epoch [352/1000] Training Loss: 0.0774\n",
      "Validation Loss: 1.2152\n",
      "Epoch [353/1000] Training Loss: 0.0695\n",
      "Validation Loss: 1.2397\n",
      "Epoch [354/1000] Training Loss: 0.1003\n",
      "Validation Loss: 1.2314\n",
      "Epoch [355/1000] Training Loss: 0.0754\n",
      "Validation Loss: 1.2107\n",
      "Epoch [356/1000] Training Loss: 0.0906\n",
      "Validation Loss: 1.2056\n",
      "Epoch [357/1000] Training Loss: 0.0859\n",
      "Validation Loss: 1.2072\n",
      "Epoch [358/1000] Training Loss: 0.0704\n",
      "Validation Loss: 1.1959\n",
      "Epoch [359/1000] Training Loss: 0.0801\n",
      "Validation Loss: 1.1798\n",
      "Epoch [360/1000] Training Loss: 0.0630\n",
      "Validation Loss: 1.1798\n",
      "Epoch [361/1000] Training Loss: 0.0873\n",
      "Validation Loss: 1.2021\n",
      "Epoch [362/1000] Training Loss: 0.1062\n",
      "Validation Loss: 1.1697\n",
      "Epoch [363/1000] Training Loss: 0.0681\n",
      "Validation Loss: 1.1582\n",
      "Epoch [364/1000] Training Loss: 0.0645\n",
      "Validation Loss: 1.1697\n",
      "Epoch [365/1000] Training Loss: 0.0801\n",
      "Validation Loss: 0.5862\n",
      "Epoch [366/1000] Training Loss: 0.0695\n",
      "Validation Loss: 1.1723\n",
      "Epoch [367/1000] Training Loss: 0.0791\n",
      "Validation Loss: 1.1782\n",
      "Epoch [368/1000] Training Loss: 0.0728\n",
      "Validation Loss: 1.1735\n",
      "Epoch [369/1000] Training Loss: 0.0792\n",
      "Validation Loss: 1.1555\n",
      "Epoch [370/1000] Training Loss: 0.0634\n",
      "Validation Loss: 1.1684\n",
      "Epoch [371/1000] Training Loss: 0.0783\n",
      "Validation Loss: 1.1806\n",
      "Epoch [372/1000] Training Loss: 0.0734\n",
      "Validation Loss: 1.2022\n",
      "Epoch [373/1000] Training Loss: 0.0832\n",
      "Validation Loss: 1.2258\n",
      "Epoch [374/1000] Training Loss: 0.0576\n",
      "Validation Loss: 1.2284\n",
      "Epoch [375/1000] Training Loss: 0.0655\n",
      "Validation Loss: 1.2346\n",
      "Epoch [376/1000] Training Loss: 0.0785\n",
      "Validation Loss: 1.2189\n",
      "Epoch [377/1000] Training Loss: 0.0678\n",
      "Validation Loss: 1.2165\n",
      "Epoch [378/1000] Training Loss: 0.0577\n",
      "Validation Loss: 1.2297\n",
      "Epoch [379/1000] Training Loss: 0.0603\n",
      "Validation Loss: 1.2594\n",
      "Epoch [380/1000] Training Loss: 0.0718\n",
      "Validation Loss: 1.2323\n",
      "Epoch [381/1000] Training Loss: 0.0597\n",
      "Validation Loss: 1.2213\n",
      "Epoch [382/1000] Training Loss: 0.0785\n",
      "Validation Loss: 1.1957\n",
      "Epoch [383/1000] Training Loss: 0.0922\n",
      "Validation Loss: 1.1960\n",
      "Epoch [384/1000] Training Loss: 0.0732\n",
      "Validation Loss: 1.2207\n",
      "Epoch [385/1000] Training Loss: 0.0606\n",
      "Validation Loss: 1.2349\n",
      "Epoch [386/1000] Training Loss: 0.0579\n",
      "Validation Loss: 1.2381\n",
      "Epoch [387/1000] Training Loss: 0.0659\n",
      "Validation Loss: 1.2559\n",
      "Epoch [388/1000] Training Loss: 0.0650\n",
      "Validation Loss: 1.2634\n",
      "Epoch [389/1000] Training Loss: 0.0744\n",
      "Validation Loss: 1.2643\n",
      "Epoch [390/1000] Training Loss: 0.0960\n",
      "Validation Loss: 1.2481\n",
      "Epoch [391/1000] Training Loss: 0.0858\n",
      "Validation Loss: 1.2605\n",
      "Epoch [392/1000] Training Loss: 0.0677\n",
      "Validation Loss: 1.2354\n",
      "Epoch [393/1000] Training Loss: 0.0509\n",
      "Validation Loss: 1.2438\n",
      "Epoch [394/1000] Training Loss: 0.0757\n",
      "Validation Loss: 1.2131\n",
      "Epoch [395/1000] Training Loss: 0.0553\n",
      "Validation Loss: 1.2181\n",
      "Epoch [396/1000] Training Loss: 0.0710\n",
      "Validation Loss: 1.2323\n",
      "Epoch [397/1000] Training Loss: 0.0695\n",
      "Validation Loss: 1.2380\n",
      "Epoch [398/1000] Training Loss: 0.0694\n",
      "Validation Loss: 1.2419\n",
      "Epoch [399/1000] Training Loss: 0.0697\n",
      "Validation Loss: 1.2340\n",
      "Epoch [400/1000] Training Loss: 0.0619\n",
      "Validation Loss: 1.2435\n",
      "Epoch [401/1000] Training Loss: 0.0855\n",
      "Validation Loss: 1.2364\n",
      "Epoch [402/1000] Training Loss: 0.0676\n",
      "Validation Loss: 1.2495\n",
      "Epoch [403/1000] Training Loss: 0.0859\n",
      "Validation Loss: 1.2643\n",
      "Epoch [404/1000] Training Loss: 0.0680\n",
      "Validation Loss: 1.3092\n",
      "Epoch [405/1000] Training Loss: 0.0845\n",
      "Validation Loss: 1.2843\n",
      "Epoch [406/1000] Training Loss: 0.0807\n",
      "Validation Loss: 1.2449\n",
      "Epoch [407/1000] Training Loss: 0.0772\n",
      "Validation Loss: 1.2711\n",
      "Epoch [408/1000] Training Loss: 0.0653\n",
      "Validation Loss: 1.2649\n",
      "Epoch [409/1000] Training Loss: 0.0807\n",
      "Validation Loss: 1.2590\n",
      "Epoch [410/1000] Training Loss: 0.0548\n",
      "Validation Loss: 1.8541\n",
      "Epoch [411/1000] Training Loss: 0.0580\n",
      "Validation Loss: 1.2811\n",
      "Epoch [412/1000] Training Loss: 0.0559\n",
      "Validation Loss: 1.8658\n",
      "Epoch [413/1000] Training Loss: 0.0651\n",
      "Validation Loss: 1.2799\n",
      "Epoch [414/1000] Training Loss: 0.0859\n",
      "Validation Loss: 1.2523\n",
      "Epoch [415/1000] Training Loss: 0.0945\n",
      "Validation Loss: 1.2488\n",
      "Epoch [416/1000] Training Loss: 0.0908\n",
      "Validation Loss: 1.2391\n",
      "Epoch [417/1000] Training Loss: 0.0731\n",
      "Validation Loss: 1.2475\n",
      "Epoch [418/1000] Training Loss: 0.0638\n",
      "Validation Loss: 1.2483\n",
      "Epoch [419/1000] Training Loss: 0.0582\n",
      "Validation Loss: 1.2525\n",
      "Epoch [420/1000] Training Loss: 0.0867\n",
      "Validation Loss: 1.2322\n",
      "Epoch [421/1000] Training Loss: 0.0906\n",
      "Validation Loss: 1.2224\n",
      "Epoch [422/1000] Training Loss: 0.0922\n",
      "Validation Loss: 1.8198\n",
      "Epoch [423/1000] Training Loss: 0.0694\n",
      "Validation Loss: 1.2636\n",
      "Epoch [424/1000] Training Loss: 0.0726\n",
      "Validation Loss: 1.2741\n",
      "Epoch [425/1000] Training Loss: 0.0597\n",
      "Validation Loss: 1.2940\n",
      "Epoch [426/1000] Training Loss: 0.0737\n",
      "Validation Loss: 1.2820\n",
      "Epoch [427/1000] Training Loss: 0.0725\n",
      "Validation Loss: 1.2734\n",
      "Epoch [428/1000] Training Loss: 0.0631\n",
      "Validation Loss: 1.2867\n",
      "Epoch [429/1000] Training Loss: 0.0627\n",
      "Validation Loss: 1.3013\n",
      "Epoch [430/1000] Training Loss: 0.0564\n",
      "Validation Loss: 1.2671\n",
      "Epoch [431/1000] Training Loss: 0.0869\n",
      "Validation Loss: 1.2854\n",
      "Epoch [432/1000] Training Loss: 0.0758\n",
      "Validation Loss: 1.3280\n",
      "Epoch [433/1000] Training Loss: 0.0705\n",
      "Validation Loss: 1.3053\n",
      "Epoch [434/1000] Training Loss: 0.0535\n",
      "Validation Loss: 1.2959\n",
      "Epoch [435/1000] Training Loss: 0.0728\n",
      "Validation Loss: 1.3050\n",
      "Epoch [436/1000] Training Loss: 0.0791\n",
      "Validation Loss: 1.2997\n",
      "Epoch [437/1000] Training Loss: 0.0815\n",
      "Validation Loss: 1.2786\n",
      "Epoch [438/1000] Training Loss: 0.0812\n",
      "Validation Loss: 1.2747\n",
      "Epoch [439/1000] Training Loss: 0.0596\n",
      "Validation Loss: 1.3068\n",
      "Epoch [440/1000] Training Loss: 0.0642\n",
      "Validation Loss: 1.3192\n",
      "Epoch [441/1000] Training Loss: 0.0780\n",
      "Validation Loss: 1.9266\n",
      "Epoch [442/1000] Training Loss: 0.0777\n",
      "Validation Loss: 1.9240\n",
      "Epoch [443/1000] Training Loss: 0.0698\n",
      "Validation Loss: 1.3359\n",
      "Epoch [444/1000] Training Loss: 0.0558\n",
      "Validation Loss: 1.3429\n",
      "Epoch [445/1000] Training Loss: 0.0744\n",
      "Validation Loss: 1.3098\n",
      "Epoch [446/1000] Training Loss: 0.0651\n",
      "Validation Loss: 1.3072\n",
      "Epoch [447/1000] Training Loss: 0.0810\n",
      "Validation Loss: 1.3131\n",
      "Epoch [448/1000] Training Loss: 0.0451\n",
      "Validation Loss: 1.3173\n",
      "Epoch [449/1000] Training Loss: 0.0720\n",
      "Validation Loss: 1.3075\n",
      "Epoch [450/1000] Training Loss: 0.0741\n",
      "Validation Loss: 1.2959\n",
      "Epoch [451/1000] Training Loss: 0.0684\n",
      "Validation Loss: 1.2850\n",
      "Epoch [452/1000] Training Loss: 0.0374\n",
      "Validation Loss: 1.2897\n",
      "Epoch [453/1000] Training Loss: 0.0711\n",
      "Validation Loss: 1.2995\n",
      "Epoch [454/1000] Training Loss: 0.0695\n",
      "Validation Loss: 1.3060\n",
      "Epoch [455/1000] Training Loss: 0.0675\n",
      "Validation Loss: 1.8910\n",
      "Epoch [456/1000] Training Loss: 0.0833\n",
      "Validation Loss: 1.3209\n",
      "Epoch [457/1000] Training Loss: 0.0612\n",
      "Validation Loss: 1.9258\n",
      "Epoch [458/1000] Training Loss: 0.0637\n",
      "Validation Loss: 1.9208\n",
      "Epoch [459/1000] Training Loss: 0.0700\n",
      "Validation Loss: 1.3398\n",
      "Epoch [460/1000] Training Loss: 0.0650\n",
      "Validation Loss: 1.9459\n",
      "Epoch [461/1000] Training Loss: 0.0678\n",
      "Validation Loss: 1.9309\n",
      "Epoch [462/1000] Training Loss: 0.0532\n",
      "Validation Loss: 1.9220\n",
      "Epoch [463/1000] Training Loss: 0.0656\n",
      "Validation Loss: 1.9624\n",
      "Epoch [464/1000] Training Loss: 0.0616\n",
      "Validation Loss: 1.9348\n",
      "Epoch [465/1000] Training Loss: 0.0509\n",
      "Validation Loss: 1.9129\n",
      "Epoch [466/1000] Training Loss: 0.0628\n",
      "Validation Loss: 1.9320\n",
      "Epoch [467/1000] Training Loss: 0.0681\n",
      "Validation Loss: 1.3555\n",
      "Epoch [468/1000] Training Loss: 0.0557\n",
      "Validation Loss: 1.3505\n",
      "Epoch [469/1000] Training Loss: 0.0812\n",
      "Validation Loss: 1.9449\n",
      "Epoch [470/1000] Training Loss: 0.0665\n",
      "Validation Loss: 1.9441\n",
      "Epoch [471/1000] Training Loss: 0.0725\n",
      "Validation Loss: 1.9456\n",
      "Epoch [472/1000] Training Loss: 0.0558\n",
      "Validation Loss: 1.9367\n",
      "Epoch [473/1000] Training Loss: 0.0530\n",
      "Validation Loss: 1.9513\n",
      "Epoch [474/1000] Training Loss: 0.0474\n",
      "Validation Loss: 1.9435\n",
      "Epoch [475/1000] Training Loss: 0.0607\n",
      "Validation Loss: 1.9317\n",
      "Epoch [476/1000] Training Loss: 0.0594\n",
      "Validation Loss: 1.9283\n",
      "Epoch [477/1000] Training Loss: 0.0679\n",
      "Validation Loss: 1.9310\n",
      "Epoch [478/1000] Training Loss: 0.0668\n",
      "Validation Loss: 1.9588\n",
      "Epoch [479/1000] Training Loss: 0.0451\n",
      "Validation Loss: 1.9691\n",
      "Epoch [480/1000] Training Loss: 0.0502\n",
      "Validation Loss: 1.9584\n",
      "Epoch [481/1000] Training Loss: 0.0785\n",
      "Validation Loss: 1.9300\n",
      "Epoch [482/1000] Training Loss: 0.0542\n",
      "Validation Loss: 1.9615\n",
      "Epoch [483/1000] Training Loss: 0.0574\n",
      "Validation Loss: 1.9819\n",
      "Epoch [484/1000] Training Loss: 0.0696\n",
      "Validation Loss: 1.9634\n",
      "Epoch [485/1000] Training Loss: 0.0578\n",
      "Validation Loss: 1.9445\n",
      "Epoch [486/1000] Training Loss: 0.0796\n",
      "Validation Loss: 1.9221\n",
      "Epoch [487/1000] Training Loss: 0.0614\n",
      "Validation Loss: 1.9338\n",
      "Epoch [488/1000] Training Loss: 0.0515\n",
      "Validation Loss: 1.9478\n",
      "Epoch [489/1000] Training Loss: 0.0443\n",
      "Validation Loss: 1.9568\n",
      "Epoch [490/1000] Training Loss: 0.0790\n",
      "Validation Loss: 1.9411\n",
      "Epoch [491/1000] Training Loss: 0.0559\n",
      "Validation Loss: 1.9258\n",
      "Epoch [492/1000] Training Loss: 0.0595\n",
      "Validation Loss: 1.9525\n",
      "Epoch [493/1000] Training Loss: 0.0424\n",
      "Validation Loss: 1.9608\n",
      "Epoch [494/1000] Training Loss: 0.0497\n",
      "Validation Loss: 1.9340\n",
      "Epoch [495/1000] Training Loss: 0.0615\n",
      "Validation Loss: 1.9509\n",
      "Epoch [496/1000] Training Loss: 0.0400\n",
      "Validation Loss: 1.9612\n",
      "Epoch [497/1000] Training Loss: 0.0579\n",
      "Validation Loss: 1.9596\n",
      "Epoch [498/1000] Training Loss: 0.0593\n",
      "Validation Loss: 1.9545\n",
      "Epoch [499/1000] Training Loss: 0.0785\n",
      "Validation Loss: 1.9388\n",
      "Epoch [500/1000] Training Loss: 0.0539\n",
      "Validation Loss: 1.9537\n",
      "Epoch [501/1000] Training Loss: 0.0403\n",
      "Validation Loss: 1.9644\n",
      "Epoch [502/1000] Training Loss: 0.0528\n",
      "Validation Loss: 2.0031\n",
      "Epoch [503/1000] Training Loss: 0.0473\n",
      "Validation Loss: 2.0001\n",
      "Epoch [504/1000] Training Loss: 0.0470\n",
      "Validation Loss: 2.0005\n",
      "Epoch [505/1000] Training Loss: 0.0606\n",
      "Validation Loss: 1.9676\n",
      "Epoch [506/1000] Training Loss: 0.0811\n",
      "Validation Loss: 1.9805\n",
      "Epoch [507/1000] Training Loss: 0.0658\n",
      "Validation Loss: 1.9608\n",
      "Epoch [508/1000] Training Loss: 0.0601\n",
      "Validation Loss: 1.9415\n",
      "Epoch [509/1000] Training Loss: 0.0560\n",
      "Validation Loss: 1.9486\n",
      "Epoch [510/1000] Training Loss: 0.0687\n",
      "Validation Loss: 1.9737\n",
      "Epoch [511/1000] Training Loss: 0.0699\n",
      "Validation Loss: 1.9593\n",
      "Epoch [512/1000] Training Loss: 0.0555\n",
      "Validation Loss: 1.9539\n",
      "Epoch [513/1000] Training Loss: 0.0496\n",
      "Validation Loss: 1.9118\n",
      "Epoch [514/1000] Training Loss: 0.0597\n",
      "Validation Loss: 1.8984\n",
      "Epoch [515/1000] Training Loss: 0.0549\n",
      "Validation Loss: 1.8995\n",
      "Epoch [516/1000] Training Loss: 0.0771\n",
      "Validation Loss: 1.9049\n",
      "Epoch [517/1000] Training Loss: 0.0527\n",
      "Validation Loss: 1.9078\n",
      "Epoch [518/1000] Training Loss: 0.0679\n",
      "Validation Loss: 1.8980\n",
      "Epoch [519/1000] Training Loss: 0.0728\n",
      "Validation Loss: 1.9069\n",
      "Epoch [520/1000] Training Loss: 0.0572\n",
      "Validation Loss: 1.9225\n",
      "Epoch [521/1000] Training Loss: 0.0486\n",
      "Validation Loss: 1.9274\n",
      "Epoch [522/1000] Training Loss: 0.0499\n",
      "Validation Loss: 1.9126\n",
      "Epoch [523/1000] Training Loss: 0.0605\n",
      "Validation Loss: 1.9232\n",
      "Epoch [524/1000] Training Loss: 0.0456\n",
      "Validation Loss: 1.9113\n",
      "Epoch [525/1000] Training Loss: 0.0555\n",
      "Validation Loss: 1.9143\n",
      "Epoch [526/1000] Training Loss: 0.0646\n",
      "Validation Loss: 1.9088\n",
      "Epoch [527/1000] Training Loss: 0.0516\n",
      "Validation Loss: 1.9295\n",
      "Epoch [528/1000] Training Loss: 0.0629\n",
      "Validation Loss: 1.9319\n",
      "Epoch [529/1000] Training Loss: 0.0513\n",
      "Validation Loss: 1.9339\n",
      "Epoch [530/1000] Training Loss: 0.0758\n",
      "Validation Loss: 1.9220\n",
      "Epoch [531/1000] Training Loss: 0.0569\n",
      "Validation Loss: 1.9418\n",
      "Epoch [532/1000] Training Loss: 0.0581\n",
      "Validation Loss: 1.9574\n",
      "Epoch [533/1000] Training Loss: 0.0606\n",
      "Validation Loss: 1.9418\n",
      "Epoch [534/1000] Training Loss: 0.0491\n",
      "Validation Loss: 1.9343\n",
      "Epoch [535/1000] Training Loss: 0.0483\n",
      "Validation Loss: 1.9376\n",
      "Epoch [536/1000] Training Loss: 0.0474\n",
      "Validation Loss: 1.9623\n",
      "Epoch [537/1000] Training Loss: 0.0614\n",
      "Validation Loss: 1.9726\n",
      "Epoch [538/1000] Training Loss: 0.0645\n",
      "Validation Loss: 1.9863\n",
      "Epoch [539/1000] Training Loss: 0.0491\n",
      "Validation Loss: 2.0017\n",
      "Epoch [540/1000] Training Loss: 0.0779\n",
      "Validation Loss: 1.9683\n",
      "Epoch [541/1000] Training Loss: 0.0396\n",
      "Validation Loss: 1.9867\n",
      "Epoch [542/1000] Training Loss: 0.0632\n",
      "Validation Loss: 1.9778\n",
      "Epoch [543/1000] Training Loss: 0.0705\n",
      "Validation Loss: 2.5600\n",
      "Epoch [544/1000] Training Loss: 0.0633\n",
      "Validation Loss: 2.5694\n",
      "Epoch [545/1000] Training Loss: 0.0656\n",
      "Validation Loss: 2.5245\n",
      "Epoch [546/1000] Training Loss: 0.0652\n",
      "Validation Loss: 2.5124\n",
      "Epoch [547/1000] Training Loss: 0.0488\n",
      "Validation Loss: 1.9939\n",
      "Epoch [548/1000] Training Loss: 0.0411\n",
      "Validation Loss: 2.0013\n",
      "Epoch [549/1000] Training Loss: 0.0670\n",
      "Validation Loss: 2.0105\n",
      "Epoch [550/1000] Training Loss: 0.0404\n",
      "Validation Loss: 2.0522\n",
      "Epoch [551/1000] Training Loss: 0.0576\n",
      "Validation Loss: 2.0501\n",
      "Epoch [552/1000] Training Loss: 0.0673\n",
      "Validation Loss: 2.0528\n",
      "Epoch [553/1000] Training Loss: 0.0471\n",
      "Validation Loss: 2.0286\n",
      "Epoch [554/1000] Training Loss: 0.0440\n",
      "Validation Loss: 2.0127\n",
      "Epoch [555/1000] Training Loss: 0.0617\n",
      "Validation Loss: 2.0169\n",
      "Epoch [556/1000] Training Loss: 0.0401\n",
      "Validation Loss: 2.0027\n",
      "Epoch [557/1000] Training Loss: 0.0713\n",
      "Validation Loss: 2.0019\n",
      "Epoch [558/1000] Training Loss: 0.0531\n",
      "Validation Loss: 1.9992\n",
      "Epoch [559/1000] Training Loss: 0.0400\n",
      "Validation Loss: 2.0085\n",
      "Epoch [560/1000] Training Loss: 0.0562\n",
      "Validation Loss: 1.9937\n",
      "Epoch [561/1000] Training Loss: 0.0630\n",
      "Validation Loss: 1.9953\n",
      "Epoch [562/1000] Training Loss: 0.0457\n",
      "Validation Loss: 1.9846\n",
      "Epoch [563/1000] Training Loss: 0.0583\n",
      "Validation Loss: 1.9932\n",
      "Epoch [564/1000] Training Loss: 0.0466\n",
      "Validation Loss: 2.5684\n",
      "Epoch [565/1000] Training Loss: 0.0754\n",
      "Validation Loss: 2.5865\n",
      "Epoch [566/1000] Training Loss: 0.0463\n",
      "Validation Loss: 2.0335\n",
      "Epoch [567/1000] Training Loss: 0.0605\n",
      "Validation Loss: 2.0314\n",
      "Epoch [568/1000] Training Loss: 0.0588\n",
      "Validation Loss: 2.0092\n",
      "Epoch [569/1000] Training Loss: 0.0559\n",
      "Validation Loss: 2.0482\n",
      "Epoch [570/1000] Training Loss: 0.0654\n",
      "Validation Loss: 2.6094\n",
      "Epoch [571/1000] Training Loss: 0.0442\n",
      "Validation Loss: 2.5929\n",
      "Epoch [572/1000] Training Loss: 0.0316\n",
      "Validation Loss: 2.5887\n",
      "Epoch [573/1000] Training Loss: 0.0562\n",
      "Validation Loss: 2.6163\n",
      "Epoch [574/1000] Training Loss: 0.0496\n",
      "Validation Loss: 2.6155\n",
      "Epoch [575/1000] Training Loss: 0.0543\n",
      "Validation Loss: 2.5869\n",
      "Epoch [576/1000] Training Loss: 0.0601\n",
      "Validation Loss: 1.9986\n",
      "Epoch [577/1000] Training Loss: 0.0548\n",
      "Validation Loss: 2.5811\n",
      "Epoch [578/1000] Training Loss: 0.0601\n",
      "Validation Loss: 2.5723\n",
      "Epoch [579/1000] Training Loss: 0.0667\n",
      "Validation Loss: 1.9745\n",
      "Epoch [580/1000] Training Loss: 0.0592\n",
      "Validation Loss: 1.9755\n",
      "Epoch [581/1000] Training Loss: 0.0535\n",
      "Validation Loss: 1.9771\n",
      "Epoch [582/1000] Training Loss: 0.0692\n",
      "Validation Loss: 1.9710\n",
      "Epoch [583/1000] Training Loss: 0.0407\n",
      "Validation Loss: 1.9707\n",
      "Epoch [584/1000] Training Loss: 0.0641\n",
      "Validation Loss: 1.9631\n",
      "Epoch [585/1000] Training Loss: 0.0578\n",
      "Validation Loss: 1.9648\n",
      "Epoch [586/1000] Training Loss: 0.0628\n",
      "Validation Loss: 1.9627\n",
      "Epoch [587/1000] Training Loss: 0.0425\n",
      "Validation Loss: 1.9812\n",
      "Epoch [588/1000] Training Loss: 0.0535\n",
      "Validation Loss: 1.9566\n",
      "Epoch [589/1000] Training Loss: 0.0638\n",
      "Validation Loss: 1.9775\n",
      "Epoch [590/1000] Training Loss: 0.0581\n",
      "Validation Loss: 1.9837\n",
      "Epoch [591/1000] Training Loss: 0.0640\n",
      "Validation Loss: 2.5259\n",
      "Epoch [592/1000] Training Loss: 0.0437\n",
      "Validation Loss: 1.9668\n",
      "Epoch [593/1000] Training Loss: 0.0491\n",
      "Validation Loss: 1.9569\n",
      "Epoch [594/1000] Training Loss: 0.0650\n",
      "Validation Loss: 1.9740\n",
      "Epoch [595/1000] Training Loss: 0.0514\n",
      "Validation Loss: 1.9521\n",
      "Epoch [596/1000] Training Loss: 0.0467\n",
      "Validation Loss: 1.9525\n",
      "Epoch [597/1000] Training Loss: 0.0511\n",
      "Validation Loss: 1.9416\n",
      "Epoch [598/1000] Training Loss: 0.0584\n",
      "Validation Loss: 1.9599\n",
      "Epoch [599/1000] Training Loss: 0.0530\n",
      "Validation Loss: 1.9603\n",
      "Epoch [600/1000] Training Loss: 0.0590\n",
      "Validation Loss: 2.5529\n",
      "Epoch [601/1000] Training Loss: 0.0557\n",
      "Validation Loss: 2.5518\n",
      "Epoch [602/1000] Training Loss: 0.0458\n",
      "Validation Loss: 2.5622\n",
      "Epoch [603/1000] Training Loss: 0.0404\n",
      "Validation Loss: 2.0014\n",
      "Epoch [604/1000] Training Loss: 0.0495\n",
      "Validation Loss: 2.5802\n",
      "Epoch [605/1000] Training Loss: 0.0569\n",
      "Validation Loss: 2.5594\n",
      "Epoch [606/1000] Training Loss: 0.2234\n",
      "Validation Loss: 2.5614\n",
      "Epoch [607/1000] Training Loss: 0.0416\n",
      "Validation Loss: 2.5660\n",
      "Epoch [608/1000] Training Loss: 0.0463\n",
      "Validation Loss: 2.5681\n",
      "Epoch [609/1000] Training Loss: 0.0433\n",
      "Validation Loss: 2.5721\n",
      "Epoch [610/1000] Training Loss: 0.0508\n",
      "Validation Loss: 2.5699\n",
      "Epoch [611/1000] Training Loss: 0.0547\n",
      "Validation Loss: 2.5690\n",
      "Epoch [612/1000] Training Loss: 0.0546\n",
      "Validation Loss: 1.9666\n",
      "Epoch [613/1000] Training Loss: 0.0569\n",
      "Validation Loss: 1.9556\n",
      "Epoch [614/1000] Training Loss: 0.0501\n",
      "Validation Loss: 1.9663\n",
      "Epoch [615/1000] Training Loss: 0.0738\n",
      "Validation Loss: 1.9800\n",
      "Epoch [616/1000] Training Loss: 0.0504\n",
      "Validation Loss: 1.9748\n",
      "Epoch [617/1000] Training Loss: 0.0515\n",
      "Validation Loss: 1.9824\n",
      "Epoch [618/1000] Training Loss: 0.0611\n",
      "Validation Loss: 1.9867\n",
      "Epoch [619/1000] Training Loss: 0.0698\n",
      "Validation Loss: 1.9533\n",
      "Epoch [620/1000] Training Loss: 0.0780\n",
      "Validation Loss: 1.9520\n",
      "Epoch [621/1000] Training Loss: 0.0503\n",
      "Validation Loss: 1.9621\n",
      "Epoch [622/1000] Training Loss: 0.0535\n",
      "Validation Loss: 1.9888\n",
      "Epoch [623/1000] Training Loss: 0.0596\n",
      "Validation Loss: 1.9789\n",
      "Epoch [624/1000] Training Loss: 0.0629\n",
      "Validation Loss: 1.9842\n",
      "Epoch [625/1000] Training Loss: 0.0533\n",
      "Validation Loss: 2.0114\n",
      "Epoch [626/1000] Training Loss: 0.0628\n",
      "Validation Loss: 2.0235\n",
      "Epoch [627/1000] Training Loss: 0.0593\n",
      "Validation Loss: 2.0534\n",
      "Epoch [628/1000] Training Loss: 0.0631\n",
      "Validation Loss: 2.0582\n",
      "Epoch [629/1000] Training Loss: 0.0617\n",
      "Validation Loss: 2.0271\n",
      "Epoch [630/1000] Training Loss: 0.0536\n",
      "Validation Loss: 2.0226\n",
      "Epoch [631/1000] Training Loss: 0.0586\n",
      "Validation Loss: 2.0002\n",
      "Epoch [632/1000] Training Loss: 0.0662\n",
      "Validation Loss: 1.9791\n",
      "Epoch [633/1000] Training Loss: 0.0597\n",
      "Validation Loss: 1.9719\n",
      "Epoch [634/1000] Training Loss: 0.0672\n",
      "Validation Loss: 1.9580\n",
      "Epoch [635/1000] Training Loss: 0.0619\n",
      "Validation Loss: 1.9590\n",
      "Epoch [636/1000] Training Loss: 0.0581\n",
      "Validation Loss: 2.5441\n",
      "Epoch [637/1000] Training Loss: 0.0401\n",
      "Validation Loss: 2.5486\n",
      "Epoch [638/1000] Training Loss: 0.0577\n",
      "Validation Loss: 1.9626\n",
      "Epoch [639/1000] Training Loss: 0.0621\n",
      "Validation Loss: 2.5147\n",
      "Epoch [640/1000] Training Loss: 0.0791\n",
      "Validation Loss: 2.5185\n",
      "Epoch [641/1000] Training Loss: 0.0651\n",
      "Validation Loss: 2.5130\n",
      "Epoch [642/1000] Training Loss: 0.0406\n",
      "Validation Loss: 2.5266\n",
      "Epoch [643/1000] Training Loss: 0.0440\n",
      "Validation Loss: 2.5289\n",
      "Epoch [644/1000] Training Loss: 0.0492\n",
      "Validation Loss: 2.5490\n",
      "Epoch [645/1000] Training Loss: 0.0484\n",
      "Validation Loss: 2.5400\n",
      "Epoch [646/1000] Training Loss: 0.0417\n",
      "Validation Loss: 2.5434\n",
      "Epoch [647/1000] Training Loss: 0.0446\n",
      "Validation Loss: 2.5405\n",
      "Epoch [648/1000] Training Loss: 0.0576\n",
      "Validation Loss: 2.5459\n",
      "Epoch [649/1000] Training Loss: 0.0629\n",
      "Validation Loss: 2.5282\n",
      "Epoch [650/1000] Training Loss: 0.0424\n",
      "Validation Loss: 2.5357\n",
      "Epoch [651/1000] Training Loss: 0.0489\n",
      "Validation Loss: 2.5485\n",
      "Epoch [652/1000] Training Loss: 0.0563\n",
      "Validation Loss: 2.5411\n",
      "Epoch [653/1000] Training Loss: 0.0325\n",
      "Validation Loss: 2.5596\n",
      "Epoch [654/1000] Training Loss: 0.0437\n",
      "Validation Loss: 2.5629\n",
      "Epoch [655/1000] Training Loss: 0.0632\n",
      "Validation Loss: 2.0176\n",
      "Epoch [656/1000] Training Loss: 0.0459\n",
      "Validation Loss: 2.0047\n",
      "Epoch [657/1000] Training Loss: 0.0319\n",
      "Validation Loss: 2.0010\n",
      "Epoch [658/1000] Training Loss: 0.0317\n",
      "Validation Loss: 2.5720\n",
      "Epoch [659/1000] Training Loss: 0.0301\n",
      "Validation Loss: 2.5725\n",
      "Epoch [660/1000] Training Loss: 0.0420\n",
      "Validation Loss: 2.5600\n",
      "Epoch [661/1000] Training Loss: 0.0483\n",
      "Validation Loss: 2.5725\n",
      "Epoch [662/1000] Training Loss: 0.0466\n",
      "Validation Loss: 2.5742\n",
      "Epoch [663/1000] Training Loss: 0.0456\n",
      "Validation Loss: 2.5464\n",
      "Epoch [664/1000] Training Loss: 0.0469\n",
      "Validation Loss: 2.5621\n",
      "Epoch [665/1000] Training Loss: 0.0544\n",
      "Validation Loss: 2.5556\n",
      "Epoch [666/1000] Training Loss: 0.0427\n",
      "Validation Loss: 2.5602\n",
      "Epoch [667/1000] Training Loss: 0.0380\n",
      "Validation Loss: 2.5472\n",
      "Epoch [668/1000] Training Loss: 0.0384\n",
      "Validation Loss: 2.5403\n",
      "Epoch [669/1000] Training Loss: 0.0410\n",
      "Validation Loss: 2.5541\n",
      "Epoch [670/1000] Training Loss: 0.0569\n",
      "Validation Loss: 2.5555\n",
      "Epoch [671/1000] Training Loss: 0.0519\n",
      "Validation Loss: 2.5377\n",
      "Epoch [672/1000] Training Loss: 0.0366\n",
      "Validation Loss: 2.5476\n",
      "Epoch [673/1000] Training Loss: 0.0459\n",
      "Validation Loss: 2.5603\n",
      "Epoch [674/1000] Training Loss: 0.0517\n",
      "Validation Loss: 2.5371\n",
      "Epoch [675/1000] Training Loss: 0.0439\n",
      "Validation Loss: 2.5140\n",
      "Epoch [676/1000] Training Loss: 0.0564\n",
      "Validation Loss: 2.5249\n",
      "Epoch [677/1000] Training Loss: 0.0540\n",
      "Validation Loss: 2.5509\n",
      "Epoch [678/1000] Training Loss: 0.0427\n",
      "Validation Loss: 2.5397\n",
      "Epoch [679/1000] Training Loss: 0.0410\n",
      "Validation Loss: 2.5394\n",
      "Epoch [680/1000] Training Loss: 0.0449\n",
      "Validation Loss: 2.5482\n",
      "Epoch [681/1000] Training Loss: 0.0561\n",
      "Validation Loss: 1.9607\n",
      "Epoch [682/1000] Training Loss: 0.0371\n",
      "Validation Loss: 1.9649\n",
      "Epoch [683/1000] Training Loss: 0.0423\n",
      "Validation Loss: 1.9798\n",
      "Epoch [684/1000] Training Loss: 0.0608\n",
      "Validation Loss: 1.9588\n",
      "Epoch [685/1000] Training Loss: 0.0389\n",
      "Validation Loss: 1.9841\n",
      "Epoch [686/1000] Training Loss: 0.0461\n",
      "Validation Loss: 1.9659\n",
      "Epoch [687/1000] Training Loss: 0.0489\n",
      "Validation Loss: 2.5478\n",
      "Epoch [688/1000] Training Loss: 0.0641\n",
      "Validation Loss: 2.5422\n",
      "Epoch [689/1000] Training Loss: 0.0651\n",
      "Validation Loss: 1.9531\n",
      "Epoch [690/1000] Training Loss: 0.0537\n",
      "Validation Loss: 2.5322\n",
      "Epoch [691/1000] Training Loss: 0.0458\n",
      "Validation Loss: 2.5515\n",
      "Epoch [692/1000] Training Loss: 0.0390\n",
      "Validation Loss: 2.5551\n",
      "Epoch [693/1000] Training Loss: 0.0584\n",
      "Validation Loss: 2.5648\n",
      "Epoch [694/1000] Training Loss: 0.0630\n",
      "Validation Loss: 2.5302\n",
      "Epoch [695/1000] Training Loss: 0.0324\n",
      "Validation Loss: 2.5439\n",
      "Epoch [696/1000] Training Loss: 0.0318\n",
      "Validation Loss: 2.5668\n",
      "Epoch [697/1000] Training Loss: 0.0315\n",
      "Validation Loss: 2.5354\n",
      "Epoch [698/1000] Training Loss: 0.0483\n",
      "Validation Loss: 2.5341\n",
      "Epoch [699/1000] Training Loss: 0.0378\n",
      "Validation Loss: 2.5532\n",
      "Epoch [700/1000] Training Loss: 0.0494\n",
      "Validation Loss: 2.5746\n",
      "Epoch [701/1000] Training Loss: 0.0667\n",
      "Validation Loss: 2.5824\n",
      "Epoch [702/1000] Training Loss: 0.0410\n",
      "Validation Loss: 2.5931\n",
      "Epoch [703/1000] Training Loss: 0.0379\n",
      "Validation Loss: 2.5953\n",
      "Epoch [704/1000] Training Loss: 0.0440\n",
      "Validation Loss: 2.5937\n",
      "Epoch [705/1000] Training Loss: 0.0188\n",
      "Validation Loss: 2.5879\n",
      "Epoch [706/1000] Training Loss: 0.0457\n",
      "Validation Loss: 2.5732\n",
      "Epoch [707/1000] Training Loss: 0.0383\n",
      "Validation Loss: 2.5543\n",
      "Epoch [708/1000] Training Loss: 0.0363\n",
      "Validation Loss: 2.5483\n",
      "Epoch [709/1000] Training Loss: 0.0413\n",
      "Validation Loss: 2.5352\n",
      "Epoch [710/1000] Training Loss: 0.0351\n",
      "Validation Loss: 1.9407\n",
      "Epoch [711/1000] Training Loss: 0.0443\n",
      "Validation Loss: 2.5231\n",
      "Epoch [712/1000] Training Loss: 0.0429\n",
      "Validation Loss: 2.5420\n",
      "Epoch [713/1000] Training Loss: 0.0363\n",
      "Validation Loss: 2.5561\n",
      "Epoch [714/1000] Training Loss: 0.0554\n",
      "Validation Loss: 2.6085\n",
      "Epoch [715/1000] Training Loss: 0.0575\n",
      "Validation Loss: 2.5529\n",
      "Epoch [716/1000] Training Loss: 0.0511\n",
      "Validation Loss: 2.5685\n",
      "Epoch [717/1000] Training Loss: 0.0480\n",
      "Validation Loss: 2.5563\n",
      "Epoch [718/1000] Training Loss: 0.0382\n",
      "Validation Loss: 2.5693\n",
      "Epoch [719/1000] Training Loss: 0.0426\n",
      "Validation Loss: 2.5581\n",
      "Epoch [720/1000] Training Loss: 0.0364\n",
      "Validation Loss: 2.5767\n",
      "Epoch [721/1000] Training Loss: 0.0440\n",
      "Validation Loss: 2.5832\n",
      "Epoch [722/1000] Training Loss: 0.0437\n",
      "Validation Loss: 1.9991\n",
      "Epoch [723/1000] Training Loss: 0.0445\n",
      "Validation Loss: 1.9903\n",
      "Epoch [724/1000] Training Loss: 0.0893\n",
      "Validation Loss: 1.9579\n",
      "Epoch [725/1000] Training Loss: 0.0610\n",
      "Validation Loss: 2.5202\n",
      "Epoch [726/1000] Training Loss: 0.0517\n",
      "Validation Loss: 1.9480\n",
      "Epoch [727/1000] Training Loss: 0.0548\n",
      "Validation Loss: 1.9469\n",
      "Epoch [728/1000] Training Loss: 0.0496\n",
      "Validation Loss: 2.5264\n",
      "Epoch [729/1000] Training Loss: 0.0349\n",
      "Validation Loss: 2.5278\n",
      "Epoch [730/1000] Training Loss: 0.0474\n",
      "Validation Loss: 2.5437\n",
      "Epoch [731/1000] Training Loss: 0.0593\n",
      "Validation Loss: 2.5304\n",
      "Epoch [732/1000] Training Loss: 0.0500\n",
      "Validation Loss: 1.9629\n",
      "Epoch [733/1000] Training Loss: 0.0454\n",
      "Validation Loss: 1.9760\n",
      "Epoch [734/1000] Training Loss: 0.0364\n",
      "Validation Loss: 1.9913\n",
      "Epoch [735/1000] Training Loss: 0.0296\n",
      "Validation Loss: 2.0200\n",
      "Epoch [736/1000] Training Loss: 0.0307\n",
      "Validation Loss: 2.6156\n",
      "Epoch [737/1000] Training Loss: 0.0429\n",
      "Validation Loss: 2.0573\n",
      "Epoch [738/1000] Training Loss: 0.0520\n",
      "Validation Loss: 2.0786\n",
      "Epoch [739/1000] Training Loss: 0.0348\n",
      "Validation Loss: 2.6211\n",
      "Epoch [740/1000] Training Loss: 0.0350\n",
      "Validation Loss: 2.6103\n",
      "Epoch [741/1000] Training Loss: 0.0534\n",
      "Validation Loss: 2.5955\n",
      "Epoch [742/1000] Training Loss: 0.0367\n",
      "Validation Loss: 2.5939\n",
      "Epoch [743/1000] Training Loss: 0.0543\n",
      "Validation Loss: 2.5905\n",
      "Epoch [744/1000] Training Loss: 0.0465\n",
      "Validation Loss: 2.5684\n",
      "Epoch [745/1000] Training Loss: 0.0455\n",
      "Validation Loss: 2.5886\n",
      "Epoch [746/1000] Training Loss: 0.0424\n",
      "Validation Loss: 2.6221\n",
      "Epoch [747/1000] Training Loss: 0.0863\n",
      "Validation Loss: 2.6284\n",
      "Epoch [748/1000] Training Loss: 0.0332\n",
      "Validation Loss: 2.6132\n",
      "Epoch [749/1000] Training Loss: 0.0512\n",
      "Validation Loss: 2.6364\n",
      "Epoch [750/1000] Training Loss: 0.0543\n",
      "Validation Loss: 2.6642\n",
      "Epoch [751/1000] Training Loss: 0.0510\n",
      "Validation Loss: 2.6678\n",
      "Epoch [752/1000] Training Loss: 0.0583\n",
      "Validation Loss: 2.6643\n",
      "Epoch [753/1000] Training Loss: 0.0628\n",
      "Validation Loss: 2.0954\n",
      "Epoch [754/1000] Training Loss: 0.0425\n",
      "Validation Loss: 2.6473\n",
      "Epoch [755/1000] Training Loss: 0.0498\n",
      "Validation Loss: 2.0613\n",
      "Epoch [756/1000] Training Loss: 0.0521\n",
      "Validation Loss: 2.0426\n",
      "Epoch [757/1000] Training Loss: 0.0461\n",
      "Validation Loss: 2.0452\n",
      "Epoch [758/1000] Training Loss: 0.0392\n",
      "Validation Loss: 2.0624\n",
      "Epoch [759/1000] Training Loss: 0.0468\n",
      "Validation Loss: 2.0462\n",
      "Epoch [760/1000] Training Loss: 0.0299\n",
      "Validation Loss: 2.0280\n",
      "Epoch [761/1000] Training Loss: 0.0445\n",
      "Validation Loss: 2.0677\n",
      "Epoch [762/1000] Training Loss: 0.0684\n",
      "Validation Loss: 2.0695\n",
      "Epoch [763/1000] Training Loss: 0.0506\n",
      "Validation Loss: 2.0784\n",
      "Epoch [764/1000] Training Loss: 0.0489\n",
      "Validation Loss: 2.0814\n",
      "Epoch [765/1000] Training Loss: 0.0474\n",
      "Validation Loss: 2.0520\n",
      "Epoch [766/1000] Training Loss: 0.0314\n",
      "Validation Loss: 2.0574\n",
      "Epoch [767/1000] Training Loss: 0.0577\n",
      "Validation Loss: 2.0700\n",
      "Epoch [768/1000] Training Loss: 0.0446\n",
      "Validation Loss: 2.6480\n",
      "Epoch [769/1000] Training Loss: 0.0552\n",
      "Validation Loss: 2.6567\n",
      "Epoch [770/1000] Training Loss: 0.0417\n",
      "Validation Loss: 2.6324\n",
      "Epoch [771/1000] Training Loss: 0.0424\n",
      "Validation Loss: 2.6398\n",
      "Epoch [772/1000] Training Loss: 0.0575\n",
      "Validation Loss: 2.6666\n",
      "Epoch [773/1000] Training Loss: 0.0492\n",
      "Validation Loss: 2.0665\n",
      "Epoch [774/1000] Training Loss: 0.0440\n",
      "Validation Loss: 2.0602\n",
      "Epoch [775/1000] Training Loss: 0.0261\n",
      "Validation Loss: 2.6312\n",
      "Epoch [776/1000] Training Loss: 0.0884\n",
      "Validation Loss: 2.6048\n",
      "Epoch [777/1000] Training Loss: 0.0387\n",
      "Validation Loss: 2.6006\n",
      "Epoch [778/1000] Training Loss: 0.0374\n",
      "Validation Loss: 2.6160\n",
      "Epoch [779/1000] Training Loss: 0.0444\n",
      "Validation Loss: 2.6269\n",
      "Epoch [780/1000] Training Loss: 0.0528\n",
      "Validation Loss: 2.6254\n",
      "Epoch [781/1000] Training Loss: 0.0366\n",
      "Validation Loss: 2.6243\n",
      "Epoch [782/1000] Training Loss: 0.0475\n",
      "Validation Loss: 2.6172\n",
      "Epoch [783/1000] Training Loss: 0.0498\n",
      "Validation Loss: 2.6263\n",
      "Epoch [784/1000] Training Loss: 0.0314\n",
      "Validation Loss: 2.6543\n",
      "Epoch [785/1000] Training Loss: 0.0419\n",
      "Validation Loss: 2.6663\n",
      "Epoch [786/1000] Training Loss: 0.0298\n",
      "Validation Loss: 2.6674\n",
      "Epoch [787/1000] Training Loss: 0.0410\n",
      "Validation Loss: 2.6360\n",
      "Epoch [788/1000] Training Loss: 0.0383\n",
      "Validation Loss: 2.6221\n",
      "Epoch [789/1000] Training Loss: 0.0738\n",
      "Validation Loss: 2.6333\n",
      "Epoch [790/1000] Training Loss: 0.0368\n",
      "Validation Loss: 2.6808\n",
      "Epoch [791/1000] Training Loss: 0.0482\n",
      "Validation Loss: 2.6833\n",
      "Epoch [792/1000] Training Loss: 0.0595\n",
      "Validation Loss: 2.7036\n",
      "Epoch [793/1000] Training Loss: 0.0477\n",
      "Validation Loss: 2.7134\n",
      "Epoch [794/1000] Training Loss: 0.0447\n",
      "Validation Loss: 2.7000\n",
      "Epoch [795/1000] Training Loss: 0.0410\n",
      "Validation Loss: 2.7114\n",
      "Epoch [796/1000] Training Loss: 0.0345\n",
      "Validation Loss: 2.6983\n",
      "Epoch [797/1000] Training Loss: 0.0413\n",
      "Validation Loss: 2.6847\n",
      "Epoch [798/1000] Training Loss: 0.0436\n",
      "Validation Loss: 2.6856\n",
      "Epoch [799/1000] Training Loss: 0.0401\n",
      "Validation Loss: 2.6975\n",
      "Epoch [800/1000] Training Loss: 0.0408\n",
      "Validation Loss: 2.6848\n",
      "Epoch [801/1000] Training Loss: 0.0503\n",
      "Validation Loss: 2.6580\n",
      "Epoch [802/1000] Training Loss: 0.0261\n",
      "Validation Loss: 2.6633\n",
      "Epoch [803/1000] Training Loss: 0.0573\n",
      "Validation Loss: 2.6256\n",
      "Epoch [804/1000] Training Loss: 0.0351\n",
      "Validation Loss: 2.6310\n",
      "Epoch [805/1000] Training Loss: 0.0292\n",
      "Validation Loss: 2.6319\n",
      "Epoch [806/1000] Training Loss: 0.0413\n",
      "Validation Loss: 2.6574\n",
      "Epoch [807/1000] Training Loss: 0.0402\n",
      "Validation Loss: 2.6580\n",
      "Epoch [808/1000] Training Loss: 0.0370\n",
      "Validation Loss: 2.6388\n",
      "Epoch [809/1000] Training Loss: 0.0392\n",
      "Validation Loss: 2.6423\n",
      "Epoch [810/1000] Training Loss: 0.0388\n",
      "Validation Loss: 2.6571\n",
      "Epoch [811/1000] Training Loss: 0.0626\n",
      "Validation Loss: 2.7125\n",
      "Epoch [812/1000] Training Loss: 0.0461\n",
      "Validation Loss: 2.7473\n",
      "Epoch [813/1000] Training Loss: 0.0485\n",
      "Validation Loss: 2.7286\n",
      "Epoch [814/1000] Training Loss: 0.0357\n",
      "Validation Loss: 2.6882\n",
      "Epoch [815/1000] Training Loss: 0.0375\n",
      "Validation Loss: 2.6592\n",
      "Epoch [816/1000] Training Loss: 0.0351\n",
      "Validation Loss: 2.6411\n",
      "Epoch [817/1000] Training Loss: 0.0276\n",
      "Validation Loss: 2.6517\n",
      "Epoch [818/1000] Training Loss: 0.0301\n",
      "Validation Loss: 2.6453\n",
      "Epoch [819/1000] Training Loss: 0.0356\n",
      "Validation Loss: 2.6593\n",
      "Epoch [820/1000] Training Loss: 0.0435\n",
      "Validation Loss: 2.6556\n",
      "Epoch [821/1000] Training Loss: 0.0451\n",
      "Validation Loss: 2.6746\n",
      "Epoch [822/1000] Training Loss: 0.0617\n",
      "Validation Loss: 2.7041\n",
      "Epoch [823/1000] Training Loss: 0.0482\n",
      "Validation Loss: 2.6808\n",
      "Epoch [824/1000] Training Loss: 0.0227\n",
      "Validation Loss: 2.6826\n",
      "Epoch [825/1000] Training Loss: 0.0344\n",
      "Validation Loss: 2.6920\n",
      "Epoch [826/1000] Training Loss: 0.0559\n",
      "Validation Loss: 2.6761\n",
      "Epoch [827/1000] Training Loss: 0.0303\n",
      "Validation Loss: 2.6890\n",
      "Epoch [828/1000] Training Loss: 0.0301\n",
      "Validation Loss: 2.6909\n",
      "Epoch [829/1000] Training Loss: 0.0205\n",
      "Validation Loss: 2.6980\n",
      "Epoch [830/1000] Training Loss: 0.0496\n",
      "Validation Loss: 2.6983\n",
      "Epoch [831/1000] Training Loss: 0.0350\n",
      "Validation Loss: 2.6893\n",
      "Epoch [832/1000] Training Loss: 0.0447\n",
      "Validation Loss: 2.7188\n",
      "Epoch [833/1000] Training Loss: 0.0383\n",
      "Validation Loss: 2.7237\n",
      "Epoch [834/1000] Training Loss: 0.0342\n",
      "Validation Loss: 2.7310\n",
      "Epoch [835/1000] Training Loss: 0.0372\n",
      "Validation Loss: 2.7253\n",
      "Epoch [836/1000] Training Loss: 0.0441\n",
      "Validation Loss: 2.7292\n",
      "Epoch [837/1000] Training Loss: 0.0299\n",
      "Validation Loss: 2.7354\n",
      "Epoch [838/1000] Training Loss: 0.0386\n",
      "Validation Loss: 2.7426\n",
      "Epoch [839/1000] Training Loss: 0.0560\n",
      "Validation Loss: 2.7395\n",
      "Epoch [840/1000] Training Loss: 0.0326\n",
      "Validation Loss: 2.7251\n",
      "Epoch [841/1000] Training Loss: 0.0477\n",
      "Validation Loss: 2.7266\n",
      "Epoch [842/1000] Training Loss: 0.0414\n",
      "Validation Loss: 2.7027\n",
      "Epoch [843/1000] Training Loss: 0.0351\n",
      "Validation Loss: 2.1395\n",
      "Epoch [844/1000] Training Loss: 0.0318\n",
      "Validation Loss: 2.7322\n",
      "Epoch [845/1000] Training Loss: 0.0361\n",
      "Validation Loss: 2.7276\n",
      "Epoch [846/1000] Training Loss: 0.0373\n",
      "Validation Loss: 2.6540\n",
      "Epoch [847/1000] Training Loss: 0.0376\n",
      "Validation Loss: 2.6347\n",
      "Epoch [848/1000] Training Loss: 0.0384\n",
      "Validation Loss: 2.6427\n",
      "Epoch [849/1000] Training Loss: 0.0320\n",
      "Validation Loss: 2.6376\n",
      "Epoch [850/1000] Training Loss: 0.0463\n",
      "Validation Loss: 2.6469\n",
      "Epoch [851/1000] Training Loss: 0.0479\n",
      "Validation Loss: 2.6087\n",
      "Epoch [852/1000] Training Loss: 0.0628\n",
      "Validation Loss: 2.6038\n",
      "Epoch [853/1000] Training Loss: 0.0557\n",
      "Validation Loss: 2.6039\n",
      "Epoch [854/1000] Training Loss: 0.0319\n",
      "Validation Loss: 2.6173\n",
      "Epoch [855/1000] Training Loss: 0.0517\n",
      "Validation Loss: 2.6329\n",
      "Epoch [856/1000] Training Loss: 0.0496\n",
      "Validation Loss: 2.6311\n",
      "Epoch [857/1000] Training Loss: 0.0380\n",
      "Validation Loss: 2.6373\n",
      "Epoch [858/1000] Training Loss: 0.0423\n",
      "Validation Loss: 2.6527\n",
      "Epoch [859/1000] Training Loss: 0.0376\n",
      "Validation Loss: 2.6602\n",
      "Epoch [860/1000] Training Loss: 0.0460\n",
      "Validation Loss: 2.6663\n",
      "Epoch [861/1000] Training Loss: 0.0361\n",
      "Validation Loss: 2.6505\n",
      "Epoch [862/1000] Training Loss: 0.0315\n",
      "Validation Loss: 2.7084\n",
      "Epoch [863/1000] Training Loss: 0.0341\n",
      "Validation Loss: 2.7235\n",
      "Epoch [864/1000] Training Loss: 0.0309\n",
      "Validation Loss: 2.7032\n",
      "Epoch [865/1000] Training Loss: 0.0385\n",
      "Validation Loss: 2.7025\n",
      "Epoch [866/1000] Training Loss: 0.0333\n",
      "Validation Loss: 2.7059\n",
      "Epoch [867/1000] Training Loss: 0.0302\n",
      "Validation Loss: 2.7023\n",
      "Epoch [868/1000] Training Loss: 0.0332\n",
      "Validation Loss: 2.7104\n",
      "Epoch [869/1000] Training Loss: 0.0380\n",
      "Validation Loss: 2.7156\n",
      "Epoch [870/1000] Training Loss: 0.0512\n",
      "Validation Loss: 2.6848\n",
      "Epoch [871/1000] Training Loss: 0.0544\n",
      "Validation Loss: 2.1142\n",
      "Epoch [872/1000] Training Loss: 0.0312\n",
      "Validation Loss: 2.7030\n",
      "Epoch [873/1000] Training Loss: 0.0455\n",
      "Validation Loss: 2.1331\n",
      "Epoch [874/1000] Training Loss: 0.0628\n",
      "Validation Loss: 2.1148\n",
      "Epoch [875/1000] Training Loss: 0.0423\n",
      "Validation Loss: 2.1133\n",
      "Epoch [876/1000] Training Loss: 0.0304\n",
      "Validation Loss: 2.6923\n",
      "Epoch [877/1000] Training Loss: 0.0561\n",
      "Validation Loss: 2.6639\n",
      "Epoch [878/1000] Training Loss: 0.0269\n",
      "Validation Loss: 2.6677\n",
      "Epoch [879/1000] Training Loss: 0.0251\n",
      "Validation Loss: 2.6787\n",
      "Epoch [880/1000] Training Loss: 0.0326\n",
      "Validation Loss: 2.0694\n",
      "Epoch [881/1000] Training Loss: 0.0497\n",
      "Validation Loss: 2.0686\n",
      "Epoch [882/1000] Training Loss: 0.0313\n",
      "Validation Loss: 2.6634\n",
      "Epoch [883/1000] Training Loss: 0.0415\n",
      "Validation Loss: 2.0793\n",
      "Epoch [884/1000] Training Loss: 0.0473\n",
      "Validation Loss: 2.6208\n",
      "Epoch [885/1000] Training Loss: 0.0226\n",
      "Validation Loss: 2.0604\n",
      "Epoch [886/1000] Training Loss: 0.0315\n",
      "Validation Loss: 2.6681\n",
      "Epoch [887/1000] Training Loss: 0.0408\n",
      "Validation Loss: 2.0995\n",
      "Epoch [888/1000] Training Loss: 0.0421\n",
      "Validation Loss: 2.6589\n",
      "Epoch [889/1000] Training Loss: 0.0286\n",
      "Validation Loss: 2.6804\n",
      "Epoch [890/1000] Training Loss: 0.0357\n",
      "Validation Loss: 2.7015\n",
      "Epoch [891/1000] Training Loss: 0.0265\n",
      "Validation Loss: 2.6932\n",
      "Epoch [892/1000] Training Loss: 0.0213\n",
      "Validation Loss: 2.6935\n",
      "Epoch [893/1000] Training Loss: 0.0360\n",
      "Validation Loss: 2.6748\n",
      "Epoch [894/1000] Training Loss: 0.0488\n",
      "Validation Loss: 2.6815\n",
      "Epoch [895/1000] Training Loss: 0.0505\n",
      "Validation Loss: 2.7151\n",
      "Epoch [896/1000] Training Loss: 0.0443\n",
      "Validation Loss: 2.6982\n",
      "Epoch [897/1000] Training Loss: 0.0418\n",
      "Validation Loss: 2.6879\n",
      "Epoch [898/1000] Training Loss: 0.0405\n",
      "Validation Loss: 2.6636\n",
      "Epoch [899/1000] Training Loss: 0.0424\n",
      "Validation Loss: 2.6957\n",
      "Epoch [900/1000] Training Loss: 0.0407\n",
      "Validation Loss: 2.7234\n",
      "Epoch [901/1000] Training Loss: 0.0260\n",
      "Validation Loss: 2.7114\n",
      "Epoch [902/1000] Training Loss: 0.0446\n",
      "Validation Loss: 2.6921\n",
      "Epoch [903/1000] Training Loss: 0.0460\n",
      "Validation Loss: 2.6892\n",
      "Epoch [904/1000] Training Loss: 0.0474\n",
      "Validation Loss: 2.7098\n",
      "Epoch [905/1000] Training Loss: 0.0366\n",
      "Validation Loss: 2.6661\n",
      "Epoch [906/1000] Training Loss: 0.0433\n",
      "Validation Loss: 2.6769\n",
      "Epoch [907/1000] Training Loss: 0.0538\n",
      "Validation Loss: 2.6993\n",
      "Epoch [908/1000] Training Loss: 0.0438\n",
      "Validation Loss: 2.7157\n",
      "Epoch [909/1000] Training Loss: 0.0381\n",
      "Validation Loss: 2.7205\n",
      "Epoch [910/1000] Training Loss: 0.0395\n",
      "Validation Loss: 2.7445\n",
      "Epoch [911/1000] Training Loss: 0.0426\n",
      "Validation Loss: 2.7206\n",
      "Epoch [912/1000] Training Loss: 0.0349\n",
      "Validation Loss: 2.7260\n",
      "Epoch [913/1000] Training Loss: 0.0507\n",
      "Validation Loss: 2.7273\n",
      "Epoch [914/1000] Training Loss: 0.0440\n",
      "Validation Loss: 2.6887\n",
      "Epoch [915/1000] Training Loss: 0.0405\n",
      "Validation Loss: 2.6938\n",
      "Epoch [916/1000] Training Loss: 0.0373\n",
      "Validation Loss: 2.7063\n",
      "Epoch [917/1000] Training Loss: 0.0321\n",
      "Validation Loss: 2.7105\n",
      "Epoch [918/1000] Training Loss: 0.0593\n",
      "Validation Loss: 2.7083\n",
      "Epoch [919/1000] Training Loss: 0.0381\n",
      "Validation Loss: 2.7100\n",
      "Epoch [920/1000] Training Loss: 0.0430\n",
      "Validation Loss: 2.6832\n",
      "Epoch [921/1000] Training Loss: 0.0297\n",
      "Validation Loss: 2.6856\n",
      "Epoch [922/1000] Training Loss: 0.0318\n",
      "Validation Loss: 2.7127\n",
      "Epoch [923/1000] Training Loss: 0.0243\n",
      "Validation Loss: 2.7292\n",
      "Epoch [924/1000] Training Loss: 0.0299\n",
      "Validation Loss: 2.7376\n",
      "Epoch [925/1000] Training Loss: 0.0576\n",
      "Validation Loss: 2.7255\n",
      "Epoch [926/1000] Training Loss: 0.0712\n",
      "Validation Loss: 2.6916\n",
      "Epoch [927/1000] Training Loss: 0.0557\n",
      "Validation Loss: 2.6810\n",
      "Epoch [928/1000] Training Loss: 0.0458\n",
      "Validation Loss: 2.7037\n",
      "Epoch [929/1000] Training Loss: 0.0346\n",
      "Validation Loss: 2.6911\n",
      "Epoch [930/1000] Training Loss: 0.0264\n",
      "Validation Loss: 2.6967\n",
      "Epoch [931/1000] Training Loss: 0.0346\n",
      "Validation Loss: 2.7037\n",
      "Epoch [932/1000] Training Loss: 0.0396\n",
      "Validation Loss: 2.7030\n",
      "Epoch [933/1000] Training Loss: 0.0439\n",
      "Validation Loss: 2.7389\n",
      "Epoch [934/1000] Training Loss: 0.0338\n",
      "Validation Loss: 2.7499\n",
      "Epoch [935/1000] Training Loss: 0.0198\n",
      "Validation Loss: 2.7591\n",
      "Epoch [936/1000] Training Loss: 0.0364\n",
      "Validation Loss: 2.7646\n",
      "Epoch [937/1000] Training Loss: 0.0338\n",
      "Validation Loss: 2.7731\n",
      "Epoch [938/1000] Training Loss: 0.0635\n",
      "Validation Loss: 2.7397\n",
      "Epoch [939/1000] Training Loss: 0.0656\n",
      "Validation Loss: 2.7402\n",
      "Epoch [940/1000] Training Loss: 0.0467\n",
      "Validation Loss: 2.7453\n",
      "Epoch [941/1000] Training Loss: 0.0376\n",
      "Validation Loss: 2.7720\n",
      "Epoch [942/1000] Training Loss: 0.0523\n",
      "Validation Loss: 2.7661\n",
      "Epoch [943/1000] Training Loss: 0.0543\n",
      "Validation Loss: 2.7384\n",
      "Epoch [944/1000] Training Loss: 0.0245\n",
      "Validation Loss: 2.7169\n",
      "Epoch [945/1000] Training Loss: 0.0362\n",
      "Validation Loss: 2.7115\n",
      "Epoch [946/1000] Training Loss: 0.0231\n",
      "Validation Loss: 2.7228\n",
      "Epoch [947/1000] Training Loss: 0.0307\n",
      "Validation Loss: 2.7302\n",
      "Epoch [948/1000] Training Loss: 0.0386\n",
      "Validation Loss: 2.7514\n",
      "Epoch [949/1000] Training Loss: 0.2205\n",
      "Validation Loss: 2.7853\n",
      "Epoch [950/1000] Training Loss: 0.0309\n",
      "Validation Loss: 2.7836\n",
      "Epoch [951/1000] Training Loss: 0.0901\n",
      "Validation Loss: 2.7485\n",
      "Epoch [952/1000] Training Loss: 0.0354\n",
      "Validation Loss: 2.7586\n",
      "Epoch [953/1000] Training Loss: 0.0675\n",
      "Validation Loss: 2.7559\n",
      "Epoch [954/1000] Training Loss: 0.0302\n",
      "Validation Loss: 2.7716\n",
      "Epoch [955/1000] Training Loss: 0.0434\n",
      "Validation Loss: 2.7559\n",
      "Epoch [956/1000] Training Loss: 0.0629\n",
      "Validation Loss: 2.7084\n",
      "Epoch [957/1000] Training Loss: 0.0225\n",
      "Validation Loss: 2.1151\n",
      "Epoch [958/1000] Training Loss: 0.0394\n",
      "Validation Loss: 2.7062\n",
      "Epoch [959/1000] Training Loss: 0.0554\n",
      "Validation Loss: 2.6961\n",
      "Epoch [960/1000] Training Loss: 0.0310\n",
      "Validation Loss: 2.7039\n",
      "Epoch [961/1000] Training Loss: 0.0443\n",
      "Validation Loss: 2.6923\n",
      "Epoch [962/1000] Training Loss: 0.0526\n",
      "Validation Loss: 2.7018\n",
      "Epoch [963/1000] Training Loss: 0.0448\n",
      "Validation Loss: 2.7182\n",
      "Epoch [964/1000] Training Loss: 0.0277\n",
      "Validation Loss: 2.7292\n",
      "Epoch [965/1000] Training Loss: 0.0420\n",
      "Validation Loss: 2.7302\n",
      "Epoch [966/1000] Training Loss: 0.0661\n",
      "Validation Loss: 2.6893\n",
      "Epoch [967/1000] Training Loss: 0.0485\n",
      "Validation Loss: 2.6754\n",
      "Epoch [968/1000] Training Loss: 0.2066\n",
      "Validation Loss: 2.6936\n",
      "Epoch [969/1000] Training Loss: 0.0443\n",
      "Validation Loss: 2.7149\n",
      "Epoch [970/1000] Training Loss: 0.0315\n",
      "Validation Loss: 2.7196\n",
      "Epoch [971/1000] Training Loss: 0.0196\n",
      "Validation Loss: 2.7190\n",
      "Epoch [972/1000] Training Loss: 0.0380\n",
      "Validation Loss: 2.7181\n",
      "Epoch [973/1000] Training Loss: 0.0513\n",
      "Validation Loss: 2.7203\n",
      "Epoch [974/1000] Training Loss: 0.0260\n",
      "Validation Loss: 2.7093\n",
      "Epoch [975/1000] Training Loss: 0.0601\n",
      "Validation Loss: 2.7112\n",
      "Epoch [976/1000] Training Loss: 0.0338\n",
      "Validation Loss: 2.7132\n",
      "Epoch [977/1000] Training Loss: 0.0509\n",
      "Validation Loss: 2.7121\n",
      "Epoch [978/1000] Training Loss: 0.0652\n",
      "Validation Loss: 2.7206\n",
      "Epoch [979/1000] Training Loss: 0.0484\n",
      "Validation Loss: 2.7646\n",
      "Epoch [980/1000] Training Loss: 0.0365\n",
      "Validation Loss: 2.7979\n",
      "Epoch [981/1000] Training Loss: 0.0428\n",
      "Validation Loss: 2.8142\n",
      "Epoch [982/1000] Training Loss: 0.0455\n",
      "Validation Loss: 2.7691\n",
      "Epoch [983/1000] Training Loss: 0.0197\n",
      "Validation Loss: 2.7757\n",
      "Epoch [984/1000] Training Loss: 0.0523\n",
      "Validation Loss: 2.7796\n",
      "Epoch [985/1000] Training Loss: 0.0374\n",
      "Validation Loss: 2.8036\n",
      "Epoch [986/1000] Training Loss: 0.0356\n",
      "Validation Loss: 2.8229\n",
      "Epoch [987/1000] Training Loss: 0.0475\n",
      "Validation Loss: 2.8342\n",
      "Epoch [988/1000] Training Loss: 0.0375\n",
      "Validation Loss: 2.8336\n",
      "Epoch [989/1000] Training Loss: 0.0263\n",
      "Validation Loss: 2.8339\n",
      "Epoch [990/1000] Training Loss: 0.0416\n",
      "Validation Loss: 2.8188\n",
      "Epoch [991/1000] Training Loss: 0.0356\n",
      "Validation Loss: 2.8384\n",
      "Epoch [992/1000] Training Loss: 0.0366\n",
      "Validation Loss: 2.8194\n",
      "Epoch [993/1000] Training Loss: 0.0409\n",
      "Validation Loss: 2.7747\n",
      "Epoch [994/1000] Training Loss: 0.0296\n",
      "Validation Loss: 2.7627\n",
      "Epoch [995/1000] Training Loss: 0.0464\n",
      "Validation Loss: 2.1763\n",
      "Epoch [996/1000] Training Loss: 0.0307\n",
      "Validation Loss: 2.1744\n",
      "Epoch [997/1000] Training Loss: 0.0417\n",
      "Validation Loss: 2.1680\n",
      "Epoch [998/1000] Training Loss: 0.0693\n",
      "Validation Loss: 2.7513\n",
      "Epoch [999/1000] Training Loss: 0.0348\n",
      "Validation Loss: 2.7678\n",
      "Epoch [1000/1000] Training Loss: 0.0376\n",
      "Validation Loss: 2.7615\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss, total = 0.0, 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze() \n",
    "            labels = labels.float()  \n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            preds = (outputs > 0.5).float()\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "\n",
    "        compute_metrics(train_preds, train_labels, \"train\", epoch)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)  \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_total = 0.0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs).squeeze()  \n",
    "                labels = labels.float()             \n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                preds = (outputs > 0.5).float()\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        \n",
    "        compute_metrics(val_preds, val_labels, \"val\", epoch)\n",
    "        writer.add_scalar('Loss/val', val_epoch_loss, epoch) \n",
    "        print(f\"Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "        if val_epoch_loss < best_loss:\n",
    "            best_loss = val_epoch_loss\n",
    "            os.makedirs(\"./models\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"./models/predat_model_best.pth\")\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n",
    "torch.save(model.state_dict(), \"./models/codiax_model_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Heart Project)",
   "language": "python",
   "name": "heart-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
