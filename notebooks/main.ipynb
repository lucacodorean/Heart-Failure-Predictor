{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfedae6",
   "metadata": {},
   "source": [
    "# **Global Music Streaming Trends**\n",
    "Luca-Andrei Codorean, 30233-1 CTI-RO @2025\n",
    "\n",
    "This projects consists of an implementation of a text classifer that wishes to succesfully predict cases of heart failure.\n",
    "The used dataset can be found at: https://www.kaggle.com/datasets/atharvasoundankar/global-music-streaming-trends-and-listener-insights\n",
    "\n",
    "In order to proceed with the solution, the dependencies found in ``requirements.txt`` should be installed, using the following command ```pip install -r requirements.\n",
    "txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd937ee1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The really first step is realted to data-preprocessing and visualization. The first function will just take one of the three datasets obtained after ```scr/data_loader.py``` script has been run.\n",
    "The ```data_loader``` script splitted the dataset in three datasets as follows: train, test, and val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4fd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def preprocess_data(dataset_path: str):\n",
    "    dataset_df = pd.read_csv(dataset_path)\n",
    "\n",
    "    scaler  = StandardScaler()\n",
    "    le_dict = {}\n",
    "\n",
    "    for column in dataset_df.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder() \n",
    "        dataset_df[column] = le.fit_transform(dataset_df[column])  \n",
    "        le_dict[column] = le  \n",
    "\n",
    "    y = dataset_df[\"Listening Time (Morning/Afternoon/Night)\"]\n",
    "    X = dataset_df.drop(columns=[\"Listening Time (Morning/Afternoon/Night)\"])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    return X_scaled_df, y, le_dict, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633faa3",
   "metadata": {},
   "source": [
    "### Plotting the histograms and class distribution\n",
    "\n",
    "An issue reagrading plotting the histograms has been identified. In the early stages of the development, the columns containing strings instead of numbers were unable to be plotted as histograms. For that, they were plotted as class distribution diagrams, firstly bars, then pie charts. \n",
    "\n",
    "It's been a problem with understanding the meaning of these columns so they were mapped accordingly. See ```preprocess_data``` function.\n",
    "\n",
    "Basically, the histograms are used for numerical columns whereas the class distribution diagrams are used for the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1421f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_visualization(X, le_dict, scaler, output_dir: str):\n",
    "\n",
    "\n",
    "    temp = scaler.inverse_transform(X)\n",
    "    df = pd.DataFrame(temp, columns=X.columns)\n",
    "\n",
    "    \n",
    "    for key  in le_dict:\n",
    "        if key in df.columns:\n",
    "            label_encoder = le_dict[key]\n",
    "            df[key] = label_encoder.inverse_transform(df[key].astype(int))\n",
    "\n",
    "        \n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            plt.title(f\"Histogram of {column}\")\n",
    "            plt.xlabel(column)\n",
    "            df[column].plot(kind='hist', bins=30, color='skyblue', edgecolor='black')\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        else:\n",
    "            plt.title(f\"Class distribution of {column}\")\n",
    "            category_counts = df[column].value_counts()\n",
    "            category_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "            plt.ylabel(\"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir+\"/\"}{column}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5790652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_formatted_file_path = \"/home/luca/SI/Project/data/preprocessed/\"\n",
    "\n",
    "train_dataset_path = \"/home/luca/SI/Project/data/raw/train.csv\"\n",
    "dataset_formatted_file_name = \"train.csv\"\n",
    "\n",
    "validation_dataset_path = \"/home/luca/SI/Project/data/raw/val.csv\"\n",
    "val_formtted_file_name = \"val.csv\"\n",
    "\n",
    "test_dataset_path = \"/home/luca/SI/Project/data/raw/test.csv\"\n",
    "test_formtted_file_name = \"test.csv\"\n",
    "\n",
    "(X, y, le_dict, scaler) = preprocess_data(dataset_path=train_dataset_path)\n",
    "(X_val, y_val, _, _)    = preprocess_data(dataset_path=validation_dataset_path)  \n",
    "\n",
    "# plot_visualization(X=X, le_dict=le_dict, scaler=scaler, output_dir=\"/home/luca/SI/Project/outputs/data_vizualization\")\n",
    "X.to_csv(os.path.join(dataset_formatted_file_path, dataset_formatted_file_name), index=False)\n",
    "X_val.to_csv(os.path.join(dataset_formatted_file_path, val_formtted_file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e9e8a",
   "metadata": {},
   "source": [
    "## Constructing the model and the training process\n",
    " \n",
    "After the data has been pre-processed, the very first step is to combine the output of the pre-processing phase into a ```StreamingPreferencesDataset``` object. This way, we will be able to structure a MLNN easier. For this, the ```StreamingPreferencesDatasetMLP``` class has been created. It's implementation can be found in ```src.data_set.py```.\n",
    "\n",
    "Once the dataset object is set-up, it's attributes can be used to inialized the ```HeartFailureMLP``` object that is responsible to implement the training model. Its implementation is available in ```src.model.py```. \n",
    "\n",
    "An important hyperparameter for the training process is the ```batch_size``` used by the dataloader. The model should be tested using multiple values for the ```BATCH_SIZE``` parameter in order to get the best results. Same goes for the ```LEARNINIG_RATE``` parameter.\n",
    "\n",
    "```EPOCHS``` parameter denotes the number of times the algorithm goes through the dataset.\n",
    "\n",
    "Thus, the first code fragment will handle initalization of diferent hyperparameters and of the model.\n",
    "Another analisys will be done in order to observe model's reaction to different optimizers such as Adams, SDG, SDG with momentum.\n",
    "The ```scheduler``` is used to provide the model with an already-implemented learning-rate scheduler. Its purpose is to reduce LR's value in order to get better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57988a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamingPreferencesDatasetMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (13): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=16, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from src.data_set       import StreamingPreferencesDataset\n",
    "from src.model          import StreamingPreferencesDatasetMLP\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    " \n",
    "BATCH_SIZE                  = 16\n",
    "DROPOUT_PERCENTAGE          = 0.4\n",
    "LEARNING_RATE               = 2e-5\n",
    "OPTIMIZER_STEP_SIZE         = 50\n",
    "EARLY_STOPPING_PATIENCE     = 200\n",
    "EPOCHS                      = 1000\n",
    "DEVICE                      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "train_dataset   = StreamingPreferencesDataset(X, y)\n",
    "val_dataset     = StreamingPreferencesDataset(X_val, y_val)\n",
    "train_loader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader      = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels = [int(label) for _, label in train_dataset]\n",
    "classes = np.unique(all_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "model           = StreamingPreferencesDatasetMLP(dropout_percentage=DROPOUT_PERCENTAGE).to(DEVICE)\n",
    "print(model)\n",
    "criterion       = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.3)  \n",
    "\n",
    "optimizer       = optim.SGD(model.parameters(), momentum=0.9, lr=LEARNING_RATE) \n",
    "# optimizer     = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "# optimizer       = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.9, eps=1e-8)\n",
    "\n",
    "scheduler      = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.7, patience=20, threshold_mode='rel',threshold=1e-4)\n",
    "# scheduler     = optim.lr_scheduler.StepLR(step_size=OPTIMIZER_STEP_SIZE, optimizer=optimizer, gamma=0.1)\n",
    "# scheduler       = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f4fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics            import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.tensorboard    import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/64funnel,CrossEntropyLoss,dp=0.3,ReduceLROnPlateau,SGD(momentum=0.9, lr=1e-4),batch_size=16,EPOCHS=1000\")\n",
    "\n",
    "def compute_metrics(all_preds, all_labels, phase, epoch):\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds,   average='macro', zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds,      average='macro', zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds,          average='macro', zero_division=0)\n",
    "\n",
    "    writer.add_scalar(f\"{phase}/Accuracy\", acc, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Precision\", prec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Recall\", rec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/F1-Score\", f1, epoch)\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa3f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Training Loss: 1.1633 Validation Loss: 1.1252\n",
      "Epoch [2/1000] Training Loss: 1.1636 Validation Loss: 1.1211\n",
      "Epoch [3/1000] Training Loss: 1.1555 Validation Loss: 1.1220\n",
      "Epoch [4/1000] Training Loss: 1.1595 Validation Loss: 1.1211\n",
      "Epoch [5/1000] Training Loss: 1.1488 Validation Loss: 1.1218\n",
      "Epoch [6/1000] Training Loss: 1.1556 Validation Loss: 1.1213\n",
      "Epoch [7/1000] Training Loss: 1.1490 Validation Loss: 1.1192\n",
      "Epoch [8/1000] Training Loss: 1.1498 Validation Loss: 1.1214\n",
      "Epoch [9/1000] Training Loss: 1.1510 Validation Loss: 1.1187\n",
      "Epoch [10/1000] Training Loss: 1.1459 Validation Loss: 1.1176\n",
      "Epoch [11/1000] Training Loss: 1.1475 Validation Loss: 1.1173\n",
      "Epoch [12/1000] Training Loss: 1.1438 Validation Loss: 1.1179\n",
      "Epoch [13/1000] Training Loss: 1.1513 Validation Loss: 1.1164\n",
      "Epoch [14/1000] Training Loss: 1.1454 Validation Loss: 1.1150\n",
      "Epoch [15/1000] Training Loss: 1.1436 Validation Loss: 1.1135\n",
      "Epoch [16/1000] Training Loss: 1.1374 Validation Loss: 1.1137\n",
      "Epoch [17/1000] Training Loss: 1.1356 Validation Loss: 1.1133\n",
      "Epoch [18/1000] Training Loss: 1.1389 Validation Loss: 1.1136\n",
      "Epoch [19/1000] Training Loss: 1.1365 Validation Loss: 1.1145\n",
      "Epoch [20/1000] Training Loss: 1.1468 Validation Loss: 1.1133\n",
      "Epoch [21/1000] Training Loss: 1.1374 Validation Loss: 1.1144\n",
      "Epoch [22/1000] Training Loss: 1.1403 Validation Loss: 1.1120\n",
      "Epoch [23/1000] Training Loss: 1.1394 Validation Loss: 1.1126\n",
      "Epoch [24/1000] Training Loss: 1.1346 Validation Loss: 1.1115\n",
      "Epoch [25/1000] Training Loss: 1.1341 Validation Loss: 1.1119\n",
      "Epoch [26/1000] Training Loss: 1.1345 Validation Loss: 1.1120\n",
      "Epoch [27/1000] Training Loss: 1.1357 Validation Loss: 1.1111\n",
      "Epoch [28/1000] Training Loss: 1.1305 Validation Loss: 1.1104\n",
      "Epoch [29/1000] Training Loss: 1.1327 Validation Loss: 1.1092\n",
      "Epoch [30/1000] Training Loss: 1.1345 Validation Loss: 1.1115\n",
      "Epoch [31/1000] Training Loss: 1.1370 Validation Loss: 1.1081\n",
      "Epoch [32/1000] Training Loss: 1.1350 Validation Loss: 1.1099\n",
      "Epoch [33/1000] Training Loss: 1.1321 Validation Loss: 1.1102\n",
      "Epoch [34/1000] Training Loss: 1.1291 Validation Loss: 1.1100\n",
      "Epoch [35/1000] Training Loss: 1.1276 Validation Loss: 1.1097\n",
      "Epoch [36/1000] Training Loss: 1.1286 Validation Loss: 1.1093\n",
      "Epoch [37/1000] Training Loss: 1.1274 Validation Loss: 1.1085\n",
      "Epoch [38/1000] Training Loss: 1.1280 Validation Loss: 1.1093\n",
      "Epoch [39/1000] Training Loss: 1.1232 Validation Loss: 1.1095\n",
      "Epoch [40/1000] Training Loss: 1.1269 Validation Loss: 1.1084\n",
      "Epoch [41/1000] Training Loss: 1.1254 Validation Loss: 1.1079\n",
      "Epoch [42/1000] Training Loss: 1.1275 Validation Loss: 1.1076\n",
      "Epoch [43/1000] Training Loss: 1.1255 Validation Loss: 1.1081\n",
      "Epoch [44/1000] Training Loss: 1.1244 Validation Loss: 1.1071\n",
      "Epoch [45/1000] Training Loss: 1.1178 Validation Loss: 1.1080\n",
      "Epoch [46/1000] Training Loss: 1.1261 Validation Loss: 1.1070\n",
      "Epoch [47/1000] Training Loss: 1.1226 Validation Loss: 1.1068\n",
      "Epoch [48/1000] Training Loss: 1.1234 Validation Loss: 1.1077\n",
      "Epoch [49/1000] Training Loss: 1.1250 Validation Loss: 1.1071\n",
      "Epoch [50/1000] Training Loss: 1.1233 Validation Loss: 1.1068\n",
      "Epoch [51/1000] Training Loss: 1.1218 Validation Loss: 1.1057\n",
      "Epoch [52/1000] Training Loss: 1.1209 Validation Loss: 1.1072\n",
      "Epoch [53/1000] Training Loss: 1.1252 Validation Loss: 1.1068\n",
      "Epoch [54/1000] Training Loss: 1.1231 Validation Loss: 1.1067\n",
      "Epoch [55/1000] Training Loss: 1.1189 Validation Loss: 1.1063\n",
      "Epoch [56/1000] Training Loss: 1.1217 Validation Loss: 1.1068\n",
      "Epoch [57/1000] Training Loss: 1.1228 Validation Loss: 1.1060\n",
      "Epoch [58/1000] Training Loss: 1.1250 Validation Loss: 1.1066\n",
      "Epoch [59/1000] Training Loss: 1.1174 Validation Loss: 1.1055\n",
      "Epoch [60/1000] Training Loss: 1.1211 Validation Loss: 1.1054\n",
      "Epoch [61/1000] Training Loss: 1.1192 Validation Loss: 1.1056\n",
      "Epoch [62/1000] Training Loss: 1.1190 Validation Loss: 1.1053\n",
      "Epoch [63/1000] Training Loss: 1.1203 Validation Loss: 1.1050\n",
      "Epoch [64/1000] Training Loss: 1.1196 Validation Loss: 1.1058\n",
      "Epoch [65/1000] Training Loss: 1.1176 Validation Loss: 1.1054\n",
      "Epoch [66/1000] Training Loss: 1.1189 Validation Loss: 1.1052\n",
      "Epoch [67/1000] Training Loss: 1.1182 Validation Loss: 1.1056\n",
      "Epoch [68/1000] Training Loss: 1.1196 Validation Loss: 1.1053\n",
      "Epoch [69/1000] Training Loss: 1.1172 Validation Loss: 1.1057\n",
      "Epoch [70/1000] Training Loss: 1.1202 Validation Loss: 1.1049\n",
      "Epoch [71/1000] Training Loss: 1.1126 Validation Loss: 1.1053\n",
      "Epoch [72/1000] Training Loss: 1.1158 Validation Loss: 1.1043\n",
      "Epoch [73/1000] Training Loss: 1.1156 Validation Loss: 1.1048\n",
      "Epoch [74/1000] Training Loss: 1.1140 Validation Loss: 1.1054\n",
      "Epoch [75/1000] Training Loss: 1.1137 Validation Loss: 1.1048\n",
      "Epoch [76/1000] Training Loss: 1.1147 Validation Loss: 1.1050\n",
      "Epoch [77/1000] Training Loss: 1.1173 Validation Loss: 1.1046\n",
      "Epoch [78/1000] Training Loss: 1.1136 Validation Loss: 1.1047\n",
      "Epoch [79/1000] Training Loss: 1.1108 Validation Loss: 1.1042\n",
      "Epoch [80/1000] Training Loss: 1.1126 Validation Loss: 1.1040\n",
      "Epoch [81/1000] Training Loss: 1.1176 Validation Loss: 1.1043\n",
      "Epoch [82/1000] Training Loss: 1.1084 Validation Loss: 1.1043\n",
      "Epoch [83/1000] Training Loss: 1.1123 Validation Loss: 1.1045\n",
      "Epoch [84/1000] Training Loss: 1.1157 Validation Loss: 1.1040\n",
      "Epoch [85/1000] Training Loss: 1.1097 Validation Loss: 1.1045\n",
      "Epoch [86/1000] Training Loss: 1.1121 Validation Loss: 1.1038\n",
      "Epoch [87/1000] Training Loss: 1.1159 Validation Loss: 1.1042\n",
      "Epoch [88/1000] Training Loss: 1.1128 Validation Loss: 1.1035\n",
      "Epoch [89/1000] Training Loss: 1.1133 Validation Loss: 1.1034\n",
      "Epoch [90/1000] Training Loss: 1.1111 Validation Loss: 1.1045\n",
      "Epoch [91/1000] Training Loss: 1.1080 Validation Loss: 1.1036\n",
      "Epoch [92/1000] Training Loss: 1.1090 Validation Loss: 1.1036\n",
      "Epoch [93/1000] Training Loss: 1.1110 Validation Loss: 1.1040\n",
      "Epoch [94/1000] Training Loss: 1.1132 Validation Loss: 1.1040\n",
      "Epoch [95/1000] Training Loss: 1.1102 Validation Loss: 1.1041\n",
      "Epoch [96/1000] Training Loss: 1.1079 Validation Loss: 1.1036\n",
      "Epoch [97/1000] Training Loss: 1.1090 Validation Loss: 1.1035\n",
      "Epoch [98/1000] Training Loss: 1.1100 Validation Loss: 1.1029\n",
      "Epoch [99/1000] Training Loss: 1.1140 Validation Loss: 1.1034\n",
      "Epoch [100/1000] Training Loss: 1.1056 Validation Loss: 1.1036\n",
      "Epoch [101/1000] Training Loss: 1.1115 Validation Loss: 1.1033\n",
      "Epoch [102/1000] Training Loss: 1.1124 Validation Loss: 1.1033\n",
      "Epoch [103/1000] Training Loss: 1.1116 Validation Loss: 1.1034\n",
      "Epoch [104/1000] Training Loss: 1.1143 Validation Loss: 1.1038\n",
      "Epoch [105/1000] Training Loss: 1.1126 Validation Loss: 1.1038\n",
      "Epoch [106/1000] Training Loss: 1.1114 Validation Loss: 1.1033\n",
      "Epoch [107/1000] Training Loss: 1.1124 Validation Loss: 1.1031\n",
      "Epoch [108/1000] Training Loss: 1.1080 Validation Loss: 1.1032\n",
      "Epoch [109/1000] Training Loss: 1.1123 Validation Loss: 1.1030\n",
      "Epoch [110/1000] Training Loss: 1.1114 Validation Loss: 1.1028\n",
      "Epoch [111/1000] Training Loss: 1.1080 Validation Loss: 1.1027\n",
      "Epoch [112/1000] Training Loss: 1.1121 Validation Loss: 1.1028\n",
      "Epoch [113/1000] Training Loss: 1.1106 Validation Loss: 1.1027\n",
      "Epoch [114/1000] Training Loss: 1.1115 Validation Loss: 1.1025\n",
      "Epoch [115/1000] Training Loss: 1.1098 Validation Loss: 1.1026\n",
      "Epoch [116/1000] Training Loss: 1.1113 Validation Loss: 1.1023\n",
      "Epoch [117/1000] Training Loss: 1.1115 Validation Loss: 1.1025\n",
      "Epoch [118/1000] Training Loss: 1.1082 Validation Loss: 1.1028\n",
      "Epoch [119/1000] Training Loss: 1.1102 Validation Loss: 1.1027\n",
      "Epoch [120/1000] Training Loss: 1.1068 Validation Loss: 1.1026\n",
      "Epoch [121/1000] Training Loss: 1.1059 Validation Loss: 1.1027\n",
      "Epoch [122/1000] Training Loss: 1.1097 Validation Loss: 1.1029\n",
      "Epoch [123/1000] Training Loss: 1.1058 Validation Loss: 1.1027\n",
      "Epoch [124/1000] Training Loss: 1.1071 Validation Loss: 1.1025\n",
      "Epoch [125/1000] Training Loss: 1.1088 Validation Loss: 1.1026\n",
      "Epoch [126/1000] Training Loss: 1.1061 Validation Loss: 1.1027\n",
      "Epoch [127/1000] Training Loss: 1.1067 Validation Loss: 1.1021\n",
      "Epoch [128/1000] Training Loss: 1.1084 Validation Loss: 1.1024\n",
      "Epoch [129/1000] Training Loss: 1.1088 Validation Loss: 1.1025\n",
      "Epoch [130/1000] Training Loss: 1.1061 Validation Loss: 1.1019\n",
      "Epoch [131/1000] Training Loss: 1.1081 Validation Loss: 1.1015\n",
      "Epoch [132/1000] Training Loss: 1.1072 Validation Loss: 1.1022\n",
      "Epoch [133/1000] Training Loss: 1.1086 Validation Loss: 1.1021\n",
      "Epoch [134/1000] Training Loss: 1.1055 Validation Loss: 1.1016\n",
      "Epoch [135/1000] Training Loss: 1.1069 Validation Loss: 1.1015\n",
      "Epoch [136/1000] Training Loss: 1.1065 Validation Loss: 1.1019\n",
      "Epoch [137/1000] Training Loss: 1.1054 Validation Loss: 1.1023\n",
      "Epoch [138/1000] Training Loss: 1.1064 Validation Loss: 1.1019\n",
      "Epoch [139/1000] Training Loss: 1.1051 Validation Loss: 1.1022\n",
      "Epoch [140/1000] Training Loss: 1.1068 Validation Loss: 1.1018\n",
      "Epoch [141/1000] Training Loss: 1.1048 Validation Loss: 1.1020\n",
      "Epoch [142/1000] Training Loss: 1.1073 Validation Loss: 1.1020\n",
      "Epoch [143/1000] Training Loss: 1.1058 Validation Loss: 1.1018\n",
      "Epoch [144/1000] Training Loss: 1.1082 Validation Loss: 1.1020\n",
      "Epoch [145/1000] Training Loss: 1.1065 Validation Loss: 1.1014\n",
      "Epoch [146/1000] Training Loss: 1.1059 Validation Loss: 1.1016\n",
      "Epoch [147/1000] Training Loss: 1.1074 Validation Loss: 1.1020\n",
      "Epoch [148/1000] Training Loss: 1.1062 Validation Loss: 1.1019\n",
      "Epoch [149/1000] Training Loss: 1.1037 Validation Loss: 1.1018\n",
      "Epoch [150/1000] Training Loss: 1.1051 Validation Loss: 1.1018\n",
      "Epoch [151/1000] Training Loss: 1.1065 Validation Loss: 1.1015\n",
      "Epoch [152/1000] Training Loss: 1.1046 Validation Loss: 1.1017\n",
      "Epoch [153/1000] Training Loss: 1.1045 Validation Loss: 1.1018\n",
      "Epoch [154/1000] Training Loss: 1.1065 Validation Loss: 1.1013\n",
      "Epoch [155/1000] Training Loss: 1.1050 Validation Loss: 1.1016\n",
      "Epoch [156/1000] Training Loss: 1.1056 Validation Loss: 1.1015\n",
      "Epoch [157/1000] Training Loss: 1.1043 Validation Loss: 1.1014\n",
      "Epoch [158/1000] Training Loss: 1.1062 Validation Loss: 1.1014\n",
      "Epoch [159/1000] Training Loss: 1.1043 Validation Loss: 1.1016\n",
      "Epoch [160/1000] Training Loss: 1.1055 Validation Loss: 1.1014\n",
      "Epoch [161/1000] Training Loss: 1.1058 Validation Loss: 1.1015\n",
      "Epoch [162/1000] Training Loss: 1.1042 Validation Loss: 1.1017\n",
      "Epoch [163/1000] Training Loss: 1.1070 Validation Loss: 1.1012\n",
      "Epoch [164/1000] Training Loss: 1.1034 Validation Loss: 1.1013\n",
      "Epoch [165/1000] Training Loss: 1.1062 Validation Loss: 1.1017\n",
      "Epoch [166/1000] Training Loss: 1.1039 Validation Loss: 1.1013\n",
      "Epoch [167/1000] Training Loss: 1.1039 Validation Loss: 1.1013\n",
      "Epoch [168/1000] Training Loss: 1.1037 Validation Loss: 1.1016\n",
      "Epoch [169/1000] Training Loss: 1.1060 Validation Loss: 1.1013\n",
      "Epoch [170/1000] Training Loss: 1.1025 Validation Loss: 1.1013\n",
      "Epoch [171/1000] Training Loss: 1.1044 Validation Loss: 1.1008\n",
      "Epoch [172/1000] Training Loss: 1.1028 Validation Loss: 1.1014\n",
      "Epoch [173/1000] Training Loss: 1.1045 Validation Loss: 1.1012\n",
      "Epoch [174/1000] Training Loss: 1.1057 Validation Loss: 1.1009\n",
      "Epoch [175/1000] Training Loss: 1.1041 Validation Loss: 1.1014\n",
      "Epoch [176/1000] Training Loss: 1.1051 Validation Loss: 1.1009\n",
      "Epoch [177/1000] Training Loss: 1.1039 Validation Loss: 1.1009\n",
      "Epoch [178/1000] Training Loss: 1.1031 Validation Loss: 1.1008\n",
      "Epoch [179/1000] Training Loss: 1.1033 Validation Loss: 1.1011\n",
      "Epoch [180/1000] Training Loss: 1.1020 Validation Loss: 1.1012\n",
      "Epoch [181/1000] Training Loss: 1.1045 Validation Loss: 1.1013\n",
      "Epoch [182/1000] Training Loss: 1.1035 Validation Loss: 1.1012\n",
      "Epoch [183/1000] Training Loss: 1.1053 Validation Loss: 1.1012\n",
      "Epoch [184/1000] Training Loss: 1.1034 Validation Loss: 1.1011\n",
      "Epoch [185/1000] Training Loss: 1.1035 Validation Loss: 1.1011\n",
      "Epoch [186/1000] Training Loss: 1.1025 Validation Loss: 1.1008\n",
      "Epoch [187/1000] Training Loss: 1.1052 Validation Loss: 1.1012\n",
      "Epoch [188/1000] Training Loss: 1.1023 Validation Loss: 1.1009\n",
      "Epoch [189/1000] Training Loss: 1.1037 Validation Loss: 1.1009\n",
      "Epoch [190/1000] Training Loss: 1.1043 Validation Loss: 1.1008\n",
      "Epoch [191/1000] Training Loss: 1.1022 Validation Loss: 1.1011\n",
      "Epoch [192/1000] Training Loss: 1.1037 Validation Loss: 1.1011\n",
      "Epoch [193/1000] Training Loss: 1.1023 Validation Loss: 1.1009\n",
      "Epoch [194/1000] Training Loss: 1.1017 Validation Loss: 1.1012\n",
      "Epoch [195/1000] Training Loss: 1.1022 Validation Loss: 1.1007\n",
      "Epoch [196/1000] Training Loss: 1.1020 Validation Loss: 1.1010\n",
      "Epoch [197/1000] Training Loss: 1.1042 Validation Loss: 1.1010\n",
      "Epoch [198/1000] Training Loss: 1.1047 Validation Loss: 1.1011\n",
      "Epoch [199/1000] Training Loss: 1.1038 Validation Loss: 1.1010\n",
      "Epoch [200/1000] Training Loss: 1.1030 Validation Loss: 1.1007\n",
      "Epoch [201/1000] Training Loss: 1.1022 Validation Loss: 1.1008\n",
      "Epoch [202/1000] Training Loss: 1.1028 Validation Loss: 1.1008\n",
      "Epoch [203/1000] Training Loss: 1.1013 Validation Loss: 1.1008\n",
      "Epoch [204/1000] Training Loss: 1.1025 Validation Loss: 1.1012\n",
      "Epoch [205/1000] Training Loss: 1.1026 Validation Loss: 1.1009\n",
      "Epoch [206/1000] Training Loss: 1.1028 Validation Loss: 1.1008\n",
      "Epoch [207/1000] Training Loss: 1.1025 Validation Loss: 1.1011\n",
      "Epoch [208/1000] Training Loss: 1.1013 Validation Loss: 1.1006\n",
      "Epoch [209/1000] Training Loss: 1.1035 Validation Loss: 1.1007\n",
      "Epoch [210/1000] Training Loss: 1.1017 Validation Loss: 1.1008\n",
      "Epoch [211/1000] Training Loss: 1.1019 Validation Loss: 1.1007\n",
      "Epoch [212/1000] Training Loss: 1.1007 Validation Loss: 1.1010\n",
      "Epoch [213/1000] Training Loss: 1.1014 Validation Loss: 1.1007\n",
      "Epoch [214/1000] Training Loss: 1.1010 Validation Loss: 1.1007\n",
      "Epoch [215/1000] Training Loss: 1.1016 Validation Loss: 1.1007\n",
      "Epoch [216/1000] Training Loss: 1.1023 Validation Loss: 1.1007\n",
      "Epoch [217/1000] Training Loss: 1.1022 Validation Loss: 1.1007\n",
      "Epoch [218/1000] Training Loss: 1.1037 Validation Loss: 1.1007\n",
      "Epoch [219/1000] Training Loss: 1.1025 Validation Loss: 1.1009\n",
      "Epoch [220/1000] Training Loss: 1.1027 Validation Loss: 1.1007\n",
      "Epoch [221/1000] Training Loss: 1.1028 Validation Loss: 1.1006\n",
      "Epoch [222/1000] Training Loss: 1.1014 Validation Loss: 1.1006\n",
      "Epoch [223/1000] Training Loss: 1.1016 Validation Loss: 1.1007\n",
      "Epoch [224/1000] Training Loss: 1.1027 Validation Loss: 1.1009\n",
      "Epoch [225/1000] Training Loss: 1.1028 Validation Loss: 1.1003\n",
      "Epoch [226/1000] Training Loss: 1.1024 Validation Loss: 1.1006\n",
      "Epoch [227/1000] Training Loss: 1.1043 Validation Loss: 1.1005\n",
      "Epoch [228/1000] Training Loss: 1.1015 Validation Loss: 1.1007\n",
      "Epoch [229/1000] Training Loss: 1.1009 Validation Loss: 1.1006\n",
      "Epoch [230/1000] Training Loss: 1.1027 Validation Loss: 1.1004\n",
      "Epoch [231/1000] Training Loss: 1.1022 Validation Loss: 1.1007\n",
      "Epoch [232/1000] Training Loss: 1.1016 Validation Loss: 1.1005\n",
      "Epoch [233/1000] Training Loss: 1.1027 Validation Loss: 1.1005\n",
      "Epoch [234/1000] Training Loss: 1.1006 Validation Loss: 1.1008\n",
      "Epoch [235/1000] Training Loss: 1.1037 Validation Loss: 1.1006\n",
      "Epoch [236/1000] Training Loss: 1.1038 Validation Loss: 1.1006\n",
      "Epoch [237/1000] Training Loss: 1.1019 Validation Loss: 1.1003\n",
      "Epoch [238/1000] Training Loss: 1.1026 Validation Loss: 1.1007\n",
      "Epoch [239/1000] Training Loss: 1.1031 Validation Loss: 1.1005\n",
      "Epoch [240/1000] Training Loss: 1.1022 Validation Loss: 1.1005\n",
      "Epoch [241/1000] Training Loss: 1.1016 Validation Loss: 1.1005\n",
      "Epoch [242/1000] Training Loss: 1.1030 Validation Loss: 1.1005\n",
      "Epoch [243/1000] Training Loss: 1.1012 Validation Loss: 1.1007\n",
      "Epoch [244/1000] Training Loss: 1.1026 Validation Loss: 1.1007\n",
      "Epoch [245/1000] Training Loss: 1.1009 Validation Loss: 1.1006\n",
      "Epoch [246/1000] Training Loss: 1.1016 Validation Loss: 1.1004\n",
      "Epoch [247/1000] Training Loss: 1.1022 Validation Loss: 1.1005\n",
      "Epoch [248/1000] Training Loss: 1.1027 Validation Loss: 1.1005\n",
      "Epoch [249/1000] Training Loss: 1.1023 Validation Loss: 1.1004\n",
      "Epoch [250/1000] Training Loss: 1.1022 Validation Loss: 1.1005\n",
      "Epoch [251/1000] Training Loss: 1.1010 Validation Loss: 1.1004\n",
      "Epoch [252/1000] Training Loss: 1.1017 Validation Loss: 1.1007\n",
      "Epoch [253/1000] Training Loss: 1.1025 Validation Loss: 1.1005\n",
      "Epoch [254/1000] Training Loss: 1.1020 Validation Loss: 1.1005\n",
      "Epoch [255/1000] Training Loss: 1.1020 Validation Loss: 1.1001\n",
      "Epoch [256/1000] Training Loss: 1.1023 Validation Loss: 1.1004\n",
      "Epoch [257/1000] Training Loss: 1.1031 Validation Loss: 1.1004\n",
      "Epoch [258/1000] Training Loss: 1.1004 Validation Loss: 1.1003\n",
      "Epoch [259/1000] Training Loss: 1.1033 Validation Loss: 1.1005\n",
      "Epoch [260/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [261/1000] Training Loss: 1.0994 Validation Loss: 1.1005\n",
      "Epoch [262/1000] Training Loss: 1.1033 Validation Loss: 1.1004\n",
      "Epoch [263/1000] Training Loss: 1.1022 Validation Loss: 1.1005\n",
      "Epoch [264/1000] Training Loss: 1.1017 Validation Loss: 1.1008\n",
      "Epoch [265/1000] Training Loss: 1.1030 Validation Loss: 1.1004\n",
      "Epoch [266/1000] Training Loss: 1.1022 Validation Loss: 1.1004\n",
      "Epoch [267/1000] Training Loss: 1.1035 Validation Loss: 1.1006\n",
      "Epoch [268/1000] Training Loss: 1.1012 Validation Loss: 1.1005\n",
      "Epoch [269/1000] Training Loss: 1.1011 Validation Loss: 1.1004\n",
      "Epoch [270/1000] Training Loss: 1.1024 Validation Loss: 1.1008\n",
      "Epoch [271/1000] Training Loss: 1.1006 Validation Loss: 1.1005\n",
      "Epoch [272/1000] Training Loss: 1.1038 Validation Loss: 1.1007\n",
      "Epoch [273/1000] Training Loss: 1.1022 Validation Loss: 1.1004\n",
      "Epoch [274/1000] Training Loss: 1.1022 Validation Loss: 1.1003\n",
      "Epoch [275/1000] Training Loss: 1.1020 Validation Loss: 1.1002\n",
      "Epoch [276/1000] Training Loss: 1.1017 Validation Loss: 1.1003\n",
      "Epoch [277/1000] Training Loss: 1.1016 Validation Loss: 1.1005\n",
      "Epoch [278/1000] Training Loss: 1.1021 Validation Loss: 1.1006\n",
      "Epoch [279/1000] Training Loss: 1.1025 Validation Loss: 1.1004\n",
      "Epoch [280/1000] Training Loss: 1.1026 Validation Loss: 1.1002\n",
      "Epoch [281/1000] Training Loss: 1.1028 Validation Loss: 1.1004\n",
      "Epoch [282/1000] Training Loss: 1.1009 Validation Loss: 1.1001\n",
      "Epoch [283/1000] Training Loss: 1.1029 Validation Loss: 1.1005\n",
      "Epoch [284/1000] Training Loss: 1.1011 Validation Loss: 1.1005\n",
      "Epoch [285/1000] Training Loss: 1.1017 Validation Loss: 1.1003\n",
      "Epoch [286/1000] Training Loss: 1.1018 Validation Loss: 1.1004\n",
      "Epoch [287/1000] Training Loss: 1.1039 Validation Loss: 1.1004\n",
      "Epoch [288/1000] Training Loss: 1.1034 Validation Loss: 1.1002\n",
      "Epoch [289/1000] Training Loss: 1.1005 Validation Loss: 1.1006\n",
      "Epoch [290/1000] Training Loss: 1.1012 Validation Loss: 1.1005\n",
      "Epoch [291/1000] Training Loss: 1.1026 Validation Loss: 1.1002\n",
      "Epoch [292/1000] Training Loss: 1.1025 Validation Loss: 1.1002\n",
      "Epoch [293/1000] Training Loss: 1.1012 Validation Loss: 1.1004\n",
      "Epoch [294/1000] Training Loss: 1.1018 Validation Loss: 1.1004\n",
      "Epoch [295/1000] Training Loss: 1.1021 Validation Loss: 1.1005\n",
      "Epoch [296/1000] Training Loss: 1.1020 Validation Loss: 1.1004\n",
      "Epoch [297/1000] Training Loss: 1.1014 Validation Loss: 1.1003\n",
      "Epoch [298/1000] Training Loss: 1.1025 Validation Loss: 1.1004\n",
      "Epoch [299/1000] Training Loss: 1.1019 Validation Loss: 1.1002\n",
      "Epoch [300/1000] Training Loss: 1.1010 Validation Loss: 1.1001\n",
      "Epoch [301/1000] Training Loss: 1.1016 Validation Loss: 1.1005\n",
      "Epoch [302/1000] Training Loss: 1.1017 Validation Loss: 1.1004\n",
      "Epoch [303/1000] Training Loss: 1.1009 Validation Loss: 1.1003\n",
      "Epoch [304/1000] Training Loss: 1.1021 Validation Loss: 1.1003\n",
      "Epoch [305/1000] Training Loss: 1.1018 Validation Loss: 1.1003\n",
      "Epoch [306/1000] Training Loss: 1.1018 Validation Loss: 1.1004\n",
      "Epoch [307/1000] Training Loss: 1.1004 Validation Loss: 1.1005\n",
      "Epoch [308/1000] Training Loss: 1.1000 Validation Loss: 1.1004\n",
      "Epoch [309/1000] Training Loss: 1.1013 Validation Loss: 1.1005\n",
      "Epoch [310/1000] Training Loss: 1.1003 Validation Loss: 1.1003\n",
      "Epoch [311/1000] Training Loss: 1.1019 Validation Loss: 1.1003\n",
      "Epoch [312/1000] Training Loss: 1.1016 Validation Loss: 1.1004\n",
      "Epoch [313/1000] Training Loss: 1.1004 Validation Loss: 1.1005\n",
      "Epoch [314/1000] Training Loss: 1.1013 Validation Loss: 1.1002\n",
      "Epoch [315/1000] Training Loss: 1.1027 Validation Loss: 1.1003\n",
      "Epoch [316/1000] Training Loss: 1.1005 Validation Loss: 1.1004\n",
      "Epoch [317/1000] Training Loss: 1.1021 Validation Loss: 1.1002\n",
      "Epoch [318/1000] Training Loss: 1.1005 Validation Loss: 1.1005\n",
      "Epoch [319/1000] Training Loss: 1.1031 Validation Loss: 1.1001\n",
      "Epoch [320/1000] Training Loss: 1.1016 Validation Loss: 1.1004\n",
      "Epoch [321/1000] Training Loss: 1.1028 Validation Loss: 1.1003\n",
      "Epoch [322/1000] Training Loss: 1.1007 Validation Loss: 1.1006\n",
      "Epoch [323/1000] Training Loss: 1.1002 Validation Loss: 1.1004\n",
      "Epoch [324/1000] Training Loss: 1.1039 Validation Loss: 1.1004\n",
      "Epoch [325/1000] Training Loss: 1.1006 Validation Loss: 1.1003\n",
      "Epoch [326/1000] Training Loss: 1.1015 Validation Loss: 1.1002\n",
      "Epoch [327/1000] Training Loss: 1.1012 Validation Loss: 1.1005\n",
      "Epoch [328/1000] Training Loss: 1.1004 Validation Loss: 1.1004\n",
      "Epoch [329/1000] Training Loss: 1.1023 Validation Loss: 1.1004\n",
      "Epoch [330/1000] Training Loss: 1.1014 Validation Loss: 1.1005\n",
      "Epoch [331/1000] Training Loss: 1.1024 Validation Loss: 1.1006\n",
      "Epoch [332/1000] Training Loss: 1.0988 Validation Loss: 1.1002\n",
      "Epoch [333/1000] Training Loss: 1.0999 Validation Loss: 1.1004\n",
      "Epoch [334/1000] Training Loss: 1.1027 Validation Loss: 1.1007\n",
      "Epoch [335/1000] Training Loss: 1.1015 Validation Loss: 1.1003\n",
      "Epoch [336/1000] Training Loss: 1.1008 Validation Loss: 1.1002\n",
      "Epoch [337/1000] Training Loss: 1.1033 Validation Loss: 1.1004\n",
      "Epoch [338/1000] Training Loss: 1.1006 Validation Loss: 1.1002\n",
      "Epoch [339/1000] Training Loss: 1.1021 Validation Loss: 1.1002\n",
      "Epoch [340/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [341/1000] Training Loss: 1.1009 Validation Loss: 1.1003\n",
      "Epoch [342/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [343/1000] Training Loss: 1.1008 Validation Loss: 1.1003\n",
      "Epoch [344/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [345/1000] Training Loss: 1.1007 Validation Loss: 1.1003\n",
      "Epoch [346/1000] Training Loss: 1.1021 Validation Loss: 1.1001\n",
      "Epoch [347/1000] Training Loss: 1.1019 Validation Loss: 1.1003\n",
      "Epoch [348/1000] Training Loss: 1.1015 Validation Loss: 1.1004\n",
      "Epoch [349/1000] Training Loss: 1.0995 Validation Loss: 1.1000\n",
      "Epoch [350/1000] Training Loss: 1.1008 Validation Loss: 1.1002\n",
      "Epoch [351/1000] Training Loss: 1.1024 Validation Loss: 1.1001\n",
      "Epoch [352/1000] Training Loss: 1.1032 Validation Loss: 1.1001\n",
      "Epoch [353/1000] Training Loss: 1.1021 Validation Loss: 1.1004\n",
      "Epoch [354/1000] Training Loss: 1.1012 Validation Loss: 1.1004\n",
      "Epoch [355/1000] Training Loss: 1.1022 Validation Loss: 1.1003\n",
      "Epoch [356/1000] Training Loss: 1.1016 Validation Loss: 1.1005\n",
      "Epoch [357/1000] Training Loss: 1.1017 Validation Loss: 1.1003\n",
      "Epoch [358/1000] Training Loss: 1.1013 Validation Loss: 1.1003\n",
      "Epoch [359/1000] Training Loss: 1.1014 Validation Loss: 1.1001\n",
      "Epoch [360/1000] Training Loss: 1.1020 Validation Loss: 1.1002\n",
      "Epoch [361/1000] Training Loss: 1.1025 Validation Loss: 1.1004\n",
      "Epoch [362/1000] Training Loss: 1.1002 Validation Loss: 1.1004\n",
      "Epoch [363/1000] Training Loss: 1.1016 Validation Loss: 1.1006\n",
      "Epoch [364/1000] Training Loss: 1.1014 Validation Loss: 1.1005\n",
      "Epoch [365/1000] Training Loss: 1.1009 Validation Loss: 1.1004\n",
      "Epoch [366/1000] Training Loss: 1.1005 Validation Loss: 1.1005\n",
      "Epoch [367/1000] Training Loss: 1.1011 Validation Loss: 1.1002\n",
      "Epoch [368/1000] Training Loss: 1.1013 Validation Loss: 1.1004\n",
      "Epoch [369/1000] Training Loss: 1.1030 Validation Loss: 1.1003\n",
      "Epoch [370/1000] Training Loss: 1.1016 Validation Loss: 1.1004\n",
      "Epoch [371/1000] Training Loss: 1.1009 Validation Loss: 1.1000\n",
      "Epoch [372/1000] Training Loss: 1.1014 Validation Loss: 1.1006\n",
      "Epoch [373/1000] Training Loss: 1.1021 Validation Loss: 1.1001\n",
      "Epoch [374/1000] Training Loss: 1.1006 Validation Loss: 1.1003\n",
      "Epoch [375/1000] Training Loss: 1.1030 Validation Loss: 1.1002\n",
      "Epoch [376/1000] Training Loss: 1.1022 Validation Loss: 1.1002\n",
      "Epoch [377/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [378/1000] Training Loss: 1.1030 Validation Loss: 1.1004\n",
      "Epoch [379/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [380/1000] Training Loss: 1.1032 Validation Loss: 1.1003\n",
      "Epoch [381/1000] Training Loss: 1.1003 Validation Loss: 1.1003\n",
      "Epoch [382/1000] Training Loss: 1.1017 Validation Loss: 1.1001\n",
      "Epoch [383/1000] Training Loss: 1.1010 Validation Loss: 1.1004\n",
      "Epoch [384/1000] Training Loss: 1.1005 Validation Loss: 1.1007\n",
      "Epoch [385/1000] Training Loss: 1.1024 Validation Loss: 1.1004\n",
      "Epoch [386/1000] Training Loss: 1.1009 Validation Loss: 1.1003\n",
      "Epoch [387/1000] Training Loss: 1.1015 Validation Loss: 1.1004\n",
      "Epoch [388/1000] Training Loss: 1.1027 Validation Loss: 1.1001\n",
      "Epoch [389/1000] Training Loss: 1.1008 Validation Loss: 1.1001\n",
      "Epoch [390/1000] Training Loss: 1.1009 Validation Loss: 1.1001\n",
      "Epoch [391/1000] Training Loss: 1.1013 Validation Loss: 1.1002\n",
      "Epoch [392/1000] Training Loss: 1.1021 Validation Loss: 1.1005\n",
      "Epoch [393/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [394/1000] Training Loss: 1.1020 Validation Loss: 1.1001\n",
      "Epoch [395/1000] Training Loss: 1.0987 Validation Loss: 1.1004\n",
      "Epoch [396/1000] Training Loss: 1.1010 Validation Loss: 1.1003\n",
      "Epoch [397/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [398/1000] Training Loss: 1.1023 Validation Loss: 1.1005\n",
      "Epoch [399/1000] Training Loss: 1.1019 Validation Loss: 1.1003\n",
      "Epoch [400/1000] Training Loss: 1.1021 Validation Loss: 1.1002\n",
      "Epoch [401/1000] Training Loss: 1.1003 Validation Loss: 1.1002\n",
      "Epoch [402/1000] Training Loss: 1.1005 Validation Loss: 1.1003\n",
      "Epoch [403/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [404/1000] Training Loss: 1.1004 Validation Loss: 1.1000\n",
      "Epoch [405/1000] Training Loss: 1.1025 Validation Loss: 1.1003\n",
      "Epoch [406/1000] Training Loss: 1.1011 Validation Loss: 1.1003\n",
      "Epoch [407/1000] Training Loss: 1.1011 Validation Loss: 1.1003\n",
      "Epoch [408/1000] Training Loss: 1.1016 Validation Loss: 1.0998\n",
      "Epoch [409/1000] Training Loss: 1.1021 Validation Loss: 1.1004\n",
      "Epoch [410/1000] Training Loss: 1.1022 Validation Loss: 1.1003\n",
      "Epoch [411/1000] Training Loss: 1.1010 Validation Loss: 1.1001\n",
      "Epoch [412/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [413/1000] Training Loss: 1.1014 Validation Loss: 1.1003\n",
      "Epoch [414/1000] Training Loss: 1.1005 Validation Loss: 1.1003\n",
      "Epoch [415/1000] Training Loss: 1.1012 Validation Loss: 1.1004\n",
      "Epoch [416/1000] Training Loss: 1.1014 Validation Loss: 1.1004\n",
      "Epoch [417/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [418/1000] Training Loss: 1.1011 Validation Loss: 1.1002\n",
      "Epoch [419/1000] Training Loss: 1.1015 Validation Loss: 1.1002\n",
      "Epoch [420/1000] Training Loss: 1.1019 Validation Loss: 1.1004\n",
      "Epoch [421/1000] Training Loss: 1.1015 Validation Loss: 1.1005\n",
      "Epoch [422/1000] Training Loss: 1.0986 Validation Loss: 1.1003\n",
      "Epoch [423/1000] Training Loss: 1.1016 Validation Loss: 1.1002\n",
      "Epoch [424/1000] Training Loss: 1.1013 Validation Loss: 1.1001\n",
      "Epoch [425/1000] Training Loss: 1.0990 Validation Loss: 1.1003\n",
      "Epoch [426/1000] Training Loss: 1.1016 Validation Loss: 1.1004\n",
      "Epoch [427/1000] Training Loss: 1.1020 Validation Loss: 1.1002\n",
      "Epoch [428/1000] Training Loss: 1.1012 Validation Loss: 1.1003\n",
      "Epoch [429/1000] Training Loss: 1.1022 Validation Loss: 1.1004\n",
      "Epoch [430/1000] Training Loss: 1.1020 Validation Loss: 1.1002\n",
      "Epoch [431/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [432/1000] Training Loss: 1.1016 Validation Loss: 1.1004\n",
      "Epoch [433/1000] Training Loss: 1.1021 Validation Loss: 1.1005\n",
      "Epoch [434/1000] Training Loss: 1.1005 Validation Loss: 1.1001\n",
      "Epoch [435/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [436/1000] Training Loss: 1.1010 Validation Loss: 1.1002\n",
      "Epoch [437/1000] Training Loss: 1.1011 Validation Loss: 1.1002\n",
      "Epoch [438/1000] Training Loss: 1.1019 Validation Loss: 1.1002\n",
      "Epoch [439/1000] Training Loss: 1.1014 Validation Loss: 1.1004\n",
      "Epoch [440/1000] Training Loss: 1.1010 Validation Loss: 1.1003\n",
      "Epoch [441/1000] Training Loss: 1.1009 Validation Loss: 1.1002\n",
      "Epoch [442/1000] Training Loss: 1.1008 Validation Loss: 1.1004\n",
      "Epoch [443/1000] Training Loss: 1.1025 Validation Loss: 1.1000\n",
      "Epoch [444/1000] Training Loss: 1.1004 Validation Loss: 1.1001\n",
      "Epoch [445/1000] Training Loss: 1.1009 Validation Loss: 1.1005\n",
      "Epoch [446/1000] Training Loss: 1.1027 Validation Loss: 1.1002\n",
      "Epoch [447/1000] Training Loss: 1.0994 Validation Loss: 1.1002\n",
      "Epoch [448/1000] Training Loss: 1.1013 Validation Loss: 1.1003\n",
      "Epoch [449/1000] Training Loss: 1.1004 Validation Loss: 1.1003\n",
      "Epoch [450/1000] Training Loss: 1.1014 Validation Loss: 1.1002\n",
      "Epoch [451/1000] Training Loss: 1.1006 Validation Loss: 1.1001\n",
      "Epoch [452/1000] Training Loss: 1.1029 Validation Loss: 1.1002\n",
      "Epoch [453/1000] Training Loss: 1.1010 Validation Loss: 1.1000\n",
      "Epoch [454/1000] Training Loss: 1.1000 Validation Loss: 1.1002\n",
      "Epoch [455/1000] Training Loss: 1.1008 Validation Loss: 1.1000\n",
      "Epoch [456/1000] Training Loss: 1.1010 Validation Loss: 1.1004\n",
      "Epoch [457/1000] Training Loss: 1.1030 Validation Loss: 1.1003\n",
      "Epoch [458/1000] Training Loss: 1.1017 Validation Loss: 1.1001\n",
      "Epoch [459/1000] Training Loss: 1.1022 Validation Loss: 1.1004\n",
      "Epoch [460/1000] Training Loss: 1.1015 Validation Loss: 1.1001\n",
      "Epoch [461/1000] Training Loss: 1.1013 Validation Loss: 1.1002\n",
      "Epoch [462/1000] Training Loss: 1.1017 Validation Loss: 1.1001\n",
      "Epoch [463/1000] Training Loss: 1.1016 Validation Loss: 1.1001\n",
      "Epoch [464/1000] Training Loss: 1.1015 Validation Loss: 1.1002\n",
      "Epoch [465/1000] Training Loss: 1.0999 Validation Loss: 1.1002\n",
      "Epoch [466/1000] Training Loss: 1.1006 Validation Loss: 1.1004\n",
      "Epoch [467/1000] Training Loss: 1.1018 Validation Loss: 1.1002\n",
      "Epoch [468/1000] Training Loss: 1.1007 Validation Loss: 1.1004\n",
      "Epoch [469/1000] Training Loss: 1.1002 Validation Loss: 1.1002\n",
      "Epoch [470/1000] Training Loss: 1.0997 Validation Loss: 1.1003\n",
      "Epoch [471/1000] Training Loss: 1.0990 Validation Loss: 1.1004\n",
      "Epoch [472/1000] Training Loss: 1.1015 Validation Loss: 1.1002\n",
      "Epoch [473/1000] Training Loss: 1.1008 Validation Loss: 1.1001\n",
      "Epoch [474/1000] Training Loss: 1.1011 Validation Loss: 1.1003\n",
      "Epoch [475/1000] Training Loss: 1.1027 Validation Loss: 1.1000\n",
      "Epoch [476/1000] Training Loss: 1.1006 Validation Loss: 1.1001\n",
      "Epoch [477/1000] Training Loss: 1.1003 Validation Loss: 1.1001\n",
      "Epoch [478/1000] Training Loss: 1.1013 Validation Loss: 1.1001\n",
      "Epoch [479/1000] Training Loss: 1.0991 Validation Loss: 1.1004\n",
      "Epoch [480/1000] Training Loss: 1.1007 Validation Loss: 1.1002\n",
      "Epoch [481/1000] Training Loss: 1.1033 Validation Loss: 1.1003\n",
      "Epoch [482/1000] Training Loss: 1.1012 Validation Loss: 1.1006\n",
      "Epoch [483/1000] Training Loss: 1.1009 Validation Loss: 1.1002\n",
      "Epoch [484/1000] Training Loss: 1.1021 Validation Loss: 1.1001\n",
      "Epoch [485/1000] Training Loss: 1.1015 Validation Loss: 1.1001\n",
      "Epoch [486/1000] Training Loss: 1.1018 Validation Loss: 1.1004\n",
      "Epoch [487/1000] Training Loss: 1.1012 Validation Loss: 1.1004\n",
      "Epoch [488/1000] Training Loss: 1.1014 Validation Loss: 1.1001\n",
      "Epoch [489/1000] Training Loss: 1.1025 Validation Loss: 1.1004\n",
      "Epoch [490/1000] Training Loss: 1.1009 Validation Loss: 1.1003\n",
      "Epoch [491/1000] Training Loss: 1.1035 Validation Loss: 1.1004\n",
      "Epoch [492/1000] Training Loss: 1.1003 Validation Loss: 1.1003\n",
      "Epoch [493/1000] Training Loss: 1.1009 Validation Loss: 1.1002\n",
      "Epoch [494/1000] Training Loss: 1.1007 Validation Loss: 1.1000\n",
      "Epoch [495/1000] Training Loss: 1.1015 Validation Loss: 1.1003\n",
      "Epoch [496/1000] Training Loss: 1.1027 Validation Loss: 1.1004\n",
      "Epoch [497/1000] Training Loss: 1.1035 Validation Loss: 1.1004\n",
      "Epoch [498/1000] Training Loss: 1.1001 Validation Loss: 1.1002\n",
      "Epoch [499/1000] Training Loss: 1.1011 Validation Loss: 1.1004\n",
      "Epoch [500/1000] Training Loss: 1.1020 Validation Loss: 1.1002\n",
      "Epoch [501/1000] Training Loss: 1.1028 Validation Loss: 1.1004\n",
      "Epoch [502/1000] Training Loss: 1.1009 Validation Loss: 1.1003\n",
      "Epoch [503/1000] Training Loss: 1.0998 Validation Loss: 1.1001\n",
      "Epoch [504/1000] Training Loss: 1.1023 Validation Loss: 1.1001\n",
      "Epoch [505/1000] Training Loss: 1.1006 Validation Loss: 1.1003\n",
      "Epoch [506/1000] Training Loss: 1.1012 Validation Loss: 1.1002\n",
      "Epoch [507/1000] Training Loss: 1.1023 Validation Loss: 1.1002\n",
      "Epoch [508/1000] Training Loss: 1.1007 Validation Loss: 1.1004\n",
      "Epoch [509/1000] Training Loss: 1.1002 Validation Loss: 1.1000\n",
      "Epoch [510/1000] Training Loss: 1.1015 Validation Loss: 1.1003\n",
      "Epoch [511/1000] Training Loss: 1.1007 Validation Loss: 1.1001\n",
      "Epoch [512/1000] Training Loss: 1.0993 Validation Loss: 1.1004\n",
      "Epoch [513/1000] Training Loss: 1.1021 Validation Loss: 1.1003\n",
      "Epoch [514/1000] Training Loss: 1.1011 Validation Loss: 1.1003\n",
      "Epoch [515/1000] Training Loss: 1.1006 Validation Loss: 1.1003\n",
      "Epoch [516/1000] Training Loss: 1.1002 Validation Loss: 1.1002\n",
      "Epoch [517/1000] Training Loss: 1.1010 Validation Loss: 1.1004\n",
      "Epoch [518/1000] Training Loss: 1.1003 Validation Loss: 1.1001\n",
      "Epoch [519/1000] Training Loss: 1.1003 Validation Loss: 1.1001\n",
      "Epoch [520/1000] Training Loss: 1.0996 Validation Loss: 1.1003\n",
      "Epoch [521/1000] Training Loss: 1.1011 Validation Loss: 1.1005\n",
      "Epoch [522/1000] Training Loss: 1.1002 Validation Loss: 1.1001\n",
      "Epoch [523/1000] Training Loss: 1.0997 Validation Loss: 1.1002\n",
      "Epoch [524/1000] Training Loss: 1.1006 Validation Loss: 1.1004\n",
      "Epoch [525/1000] Training Loss: 1.1009 Validation Loss: 1.1002\n",
      "Epoch [526/1000] Training Loss: 1.1028 Validation Loss: 1.1003\n",
      "Epoch [527/1000] Training Loss: 1.1021 Validation Loss: 1.1006\n",
      "Epoch [528/1000] Training Loss: 1.1013 Validation Loss: 1.1001\n",
      "Epoch [529/1000] Training Loss: 1.1023 Validation Loss: 1.1005\n",
      "Epoch [530/1000] Training Loss: 1.1014 Validation Loss: 1.1003\n",
      "Epoch [531/1000] Training Loss: 1.1014 Validation Loss: 1.1002\n",
      "Epoch [532/1000] Training Loss: 1.1008 Validation Loss: 1.1002\n",
      "Epoch [533/1000] Training Loss: 1.1024 Validation Loss: 1.1002\n",
      "Epoch [534/1000] Training Loss: 1.1013 Validation Loss: 1.1004\n",
      "Epoch [535/1000] Training Loss: 1.1009 Validation Loss: 1.1002\n",
      "Epoch [536/1000] Training Loss: 1.1020 Validation Loss: 1.1003\n",
      "Epoch [537/1000] Training Loss: 1.1017 Validation Loss: 1.1004\n",
      "Epoch [538/1000] Training Loss: 1.1014 Validation Loss: 1.1002\n",
      "Epoch [539/1000] Training Loss: 1.1001 Validation Loss: 1.1001\n",
      "Epoch [540/1000] Training Loss: 1.1023 Validation Loss: 1.1004\n",
      "Epoch [541/1000] Training Loss: 1.1008 Validation Loss: 1.1003\n",
      "Epoch [542/1000] Training Loss: 1.1013 Validation Loss: 1.1004\n",
      "Epoch [543/1000] Training Loss: 1.0998 Validation Loss: 1.1002\n",
      "Epoch [544/1000] Training Loss: 1.1013 Validation Loss: 1.1004\n",
      "Epoch [545/1000] Training Loss: 1.1018 Validation Loss: 1.1002\n",
      "Epoch [546/1000] Training Loss: 1.1017 Validation Loss: 1.1004\n",
      "Epoch [547/1000] Training Loss: 1.1002 Validation Loss: 1.1004\n",
      "Epoch [548/1000] Training Loss: 1.1000 Validation Loss: 1.1002\n",
      "Epoch [549/1000] Training Loss: 1.0998 Validation Loss: 1.1003\n",
      "Epoch [550/1000] Training Loss: 1.1020 Validation Loss: 1.1004\n",
      "Epoch [551/1000] Training Loss: 1.1016 Validation Loss: 1.1001\n",
      "Epoch [552/1000] Training Loss: 1.1030 Validation Loss: 1.1002\n",
      "Epoch [553/1000] Training Loss: 1.1000 Validation Loss: 1.1002\n",
      "Epoch [554/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [555/1000] Training Loss: 1.1001 Validation Loss: 1.1002\n",
      "Epoch [556/1000] Training Loss: 1.1012 Validation Loss: 1.1005\n",
      "Epoch [557/1000] Training Loss: 1.1000 Validation Loss: 1.1003\n",
      "Epoch [558/1000] Training Loss: 1.1010 Validation Loss: 1.1005\n",
      "Epoch [559/1000] Training Loss: 1.1035 Validation Loss: 1.1001\n",
      "Epoch [560/1000] Training Loss: 1.1013 Validation Loss: 1.1001\n",
      "Epoch [561/1000] Training Loss: 1.1019 Validation Loss: 1.1000\n",
      "Epoch [562/1000] Training Loss: 1.1008 Validation Loss: 1.1005\n",
      "Epoch [563/1000] Training Loss: 1.1021 Validation Loss: 1.1005\n",
      "Epoch [564/1000] Training Loss: 1.1024 Validation Loss: 1.0999\n",
      "Epoch [565/1000] Training Loss: 1.1009 Validation Loss: 1.1003\n",
      "Epoch [566/1000] Training Loss: 1.1003 Validation Loss: 1.1001\n",
      "Epoch [567/1000] Training Loss: 1.1025 Validation Loss: 1.1004\n",
      "Epoch [568/1000] Training Loss: 1.1010 Validation Loss: 1.1002\n",
      "Epoch [569/1000] Training Loss: 1.1021 Validation Loss: 1.1003\n",
      "Epoch [570/1000] Training Loss: 1.1020 Validation Loss: 1.1006\n",
      "Epoch [571/1000] Training Loss: 1.1021 Validation Loss: 1.1003\n",
      "Epoch [572/1000] Training Loss: 1.0990 Validation Loss: 1.1004\n",
      "Epoch [573/1000] Training Loss: 1.1012 Validation Loss: 1.1002\n",
      "Epoch [574/1000] Training Loss: 1.1018 Validation Loss: 1.1004\n",
      "Epoch [575/1000] Training Loss: 1.1003 Validation Loss: 1.1002\n",
      "Epoch [576/1000] Training Loss: 1.1012 Validation Loss: 1.1005\n",
      "Epoch [577/1000] Training Loss: 1.1020 Validation Loss: 1.1002\n",
      "Epoch [578/1000] Training Loss: 1.1010 Validation Loss: 1.1004\n",
      "Epoch [579/1000] Training Loss: 1.1006 Validation Loss: 1.1003\n",
      "Epoch [580/1000] Training Loss: 1.1024 Validation Loss: 1.1003\n",
      "Epoch [581/1000] Training Loss: 1.1031 Validation Loss: 1.1002\n",
      "Epoch [582/1000] Training Loss: 1.1017 Validation Loss: 1.1002\n",
      "Epoch [583/1000] Training Loss: 1.0995 Validation Loss: 1.1003\n",
      "Epoch [584/1000] Training Loss: 1.1007 Validation Loss: 1.1004\n",
      "Epoch [585/1000] Training Loss: 1.0999 Validation Loss: 1.1000\n",
      "Epoch [586/1000] Training Loss: 1.1019 Validation Loss: 1.1001\n",
      "Epoch [587/1000] Training Loss: 1.1015 Validation Loss: 1.1002\n",
      "Epoch [588/1000] Training Loss: 1.1018 Validation Loss: 1.1003\n",
      "Epoch [589/1000] Training Loss: 1.1013 Validation Loss: 1.1005\n",
      "Epoch [590/1000] Training Loss: 1.0997 Validation Loss: 1.1005\n",
      "Epoch [591/1000] Training Loss: 1.1014 Validation Loss: 1.1002\n",
      "Epoch [592/1000] Training Loss: 1.0991 Validation Loss: 1.1003\n",
      "Epoch [593/1000] Training Loss: 1.1015 Validation Loss: 1.1002\n",
      "Epoch [594/1000] Training Loss: 1.1004 Validation Loss: 1.1003\n",
      "Epoch [595/1000] Training Loss: 1.1019 Validation Loss: 1.1002\n",
      "Epoch [596/1000] Training Loss: 1.1014 Validation Loss: 1.1001\n",
      "Epoch [597/1000] Training Loss: 1.1007 Validation Loss: 1.1002\n",
      "Epoch [598/1000] Training Loss: 1.1000 Validation Loss: 1.1003\n",
      "Epoch [599/1000] Training Loss: 1.1016 Validation Loss: 1.1003\n",
      "Epoch [600/1000] Training Loss: 1.1011 Validation Loss: 1.1000\n",
      "Epoch [601/1000] Training Loss: 1.1014 Validation Loss: 1.1001\n",
      "Epoch [602/1000] Training Loss: 1.1006 Validation Loss: 1.1002\n",
      "Epoch [603/1000] Training Loss: 1.1008 Validation Loss: 1.1000\n",
      "Epoch [604/1000] Training Loss: 1.1003 Validation Loss: 1.1002\n",
      "Epoch [605/1000] Training Loss: 1.1005 Validation Loss: 1.1002\n",
      "Epoch [606/1000] Training Loss: 1.1027 Validation Loss: 1.1005\n",
      "Epoch [607/1000] Training Loss: 1.0995 Validation Loss: 1.1002\n",
      "Epoch [608/1000] Training Loss: 1.1012 Validation Loss: 1.1002\n",
      "Early stopping at epoch 608\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    epochs_without_improvement = 0 \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss, total = 0.0, 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)  \n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        compute_metrics(train_preds, train_labels, \"train\", epoch)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_total = 0.0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.long())\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)  \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        compute_metrics(val_preds, val_labels, \"val\", epoch)\n",
    "\n",
    "        scheduler.step(val_epoch_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar('LearningRate', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/val', val_epoch_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {epoch_loss:.4f} Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "        if val_epoch_loss < best_loss:\n",
    "            best_loss = val_epoch_loss\n",
    "            epochs_without_improvement = 0\n",
    "            os.makedirs(\"./models\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"./models/predat_model_best.pth\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        if epoch > 30 and epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), \"./models/codiax_model_final.pth\")\n",
    "\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, EARLY_STOPPING_PATIENCE)\n",
    "torch.save(model.state_dict(), \"./models/codiax_model_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Heart Project)",
   "language": "python",
   "name": "heart-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
