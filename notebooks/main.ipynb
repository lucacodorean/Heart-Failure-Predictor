{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfedae6",
   "metadata": {},
   "source": [
    "# **Global Music Streaming Trends**\n",
    "Luca-Andrei Codorean, 30233-1 CTI-RO @2025\n",
    "\n",
    "This projects consists of an implementation of a text classifer that wishes to succesfully predict cases of heart failure.\n",
    "The used dataset can be found at: https://www.kaggle.com/datasets/atharvasoundankar/global-music-streaming-trends-and-listener-insights\n",
    "\n",
    "In order to proceed with the solution, the dependencies found in ``requirements.txt`` should be installed, using the following command ```pip install -r requirements.\n",
    "txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd937ee1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The really first step is realted to data-preprocessing and visualization. The first function will just take one of the three datasets obtained after ```scr/data_loader.py``` script has been run.\n",
    "The ```data_loader``` script splitted the dataset in three datasets as follows: train, test, and val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def preprocess_data(dataset_path: str):\n",
    "    dataset_df = pd.read_csv(dataset_path)\n",
    "\n",
    "    scaler  = MinMaxScaler()\n",
    "    le_dict = {}\n",
    "\n",
    "    for column in dataset_df.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder() \n",
    "        dataset_df[column] = le.fit_transform(dataset_df[column])  \n",
    "        le_dict[column] = le  \n",
    "\n",
    "    y = dataset_df[\"Listening Time (Morning/Afternoon/Night)\"]\n",
    "    X = dataset_df.drop(columns=[\"Listening Time (Morning/Afternoon/Night)\"])\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    return X_scaled_df, y, le_dict, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633faa3",
   "metadata": {},
   "source": [
    "## Plotting the histograms and class distribution\n",
    "\n",
    "An issue reagrading plotting the histograms has been identified. In the early stages of the development, the columns containing strings instead of numbers were unable to be plotted as histograms. For that, they were plotted as class distribution diagrams, firstly bars, then pie charts. \n",
    "\n",
    "It's been a problem with understanding the meaning of these columns so they were mapped accordingly. See ```preprocess_data``` function.\n",
    "\n",
    "Basically, the histograms are used for numerical columns whereas the class distribution diagrams are used for the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_visualization(X, le_dict, scaler, output_dir: str):\n",
    "\n",
    "\n",
    "    temp = scaler.inverse_transform(X)\n",
    "    df = pd.DataFrame(temp, columns=X.columns)\n",
    "\n",
    "    \n",
    "    for key  in le_dict:\n",
    "        if key in df.columns:\n",
    "            label_encoder = le_dict[key]\n",
    "            df[key] = label_encoder.inverse_transform(df[key].astype(int))\n",
    "\n",
    "        \n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            plt.title(f\"Histogram of {column}\")\n",
    "            plt.xlabel(column)\n",
    "            df[column].plot(kind='hist', bins=30, color='skyblue', edgecolor='black')\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        else:\n",
    "            plt.title(f\"Class distribution of {column}\")\n",
    "            category_counts = df[column].value_counts()\n",
    "            category_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "            plt.ylabel(\"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir+\"/\"}{column}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_formatted_file_path = \"/home/luca/SI/Project/data/preprocessed/\"\n",
    "\n",
    "train_dataset_path = \"/home/luca/SI/Project/data/raw/train.csv\"\n",
    "dataset_formatted_file_name = \"train.csv\"\n",
    "\n",
    "validation_dataset_path = \"/home/luca/SI/Project/data/raw/val.csv\"\n",
    "val_formtted_file_name = \"val.csv\"\n",
    "\n",
    "test_dataset_path = \"/home/luca/SI/Project/data/raw/test.csv\"\n",
    "test_formtted_file_name = \"test.csv\"\n",
    "\n",
    "(X, y, le_dict, scaler) = preprocess_data(dataset_path=train_dataset_path)\n",
    "(X_val, y_val, _, _)    = preprocess_data(dataset_path=validation_dataset_path)  \n",
    "(X_test, y_test, _, _)  = preprocess_data(dataset_path=test_dataset_path)\n",
    "\n",
    "# plot_visualization(X=X, le_dict=le_dict, scaler=scaler, output_dir=\"/home/luca/SI/Project/outputs/data_vizualization\")\n",
    "X.to_csv(os.path.join(dataset_formatted_file_path, dataset_formatted_file_name), index=False)\n",
    "X_val.to_csv(os.path.join(dataset_formatted_file_path, val_formtted_file_name), index=False)\n",
    "X_test.to_csv(os.path.join(dataset_formatted_file_path, test_formtted_file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e9e8a",
   "metadata": {},
   "source": [
    "## Constructing the model and the training process\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed17f0",
   "metadata": {},
   "source": [
    "### Training using a custom neuronal network\n",
    "\n",
    "After the data has been pre-processed, the very first step is to combine the output of the pre-processing phase into a ```StreamingPreferencesDataset``` object. This way, we will be able to structure a MLNN easier. For this, the ```StreamingPreferencesDatasetMLP``` class has been created. It's implementation can be found in ```src.data_set.py```.\n",
    "\n",
    "Once the dataset object is set-up, it's attributes can be used to inialized the ```HeartFailureMLP``` object that is responsible to implement the training model. Its implementation is available in ```src.model.py```. \n",
    "\n",
    "An important hyperparameter for the training process is the ```batch_size``` used by the dataloader. The model should be tested using multiple values for the ```BATCH_SIZE``` parameter in order to get the best results. Same goes for the ```LEARNINIG_RATE``` parameter.\n",
    "\n",
    "```EPOCHS``` parameter denotes the number of times the algorithm goes through the dataset.\n",
    "\n",
    "Thus, the first code fragment will handle initalization of diferent hyperparameters and of the model.\n",
    "Another analisys will be done in order to observe model's reaction to different optimizers such as Adams, SDG, SDG with momentum.\n",
    "The ```scheduler``` is used to provide the model with an already-implemented learning-rate scheduler. Its purpose is to reduce LR's value in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics            import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.tensorboard    import SummaryWriter\n",
    "\n",
    "def compute_metrics(writer, all_preds, all_labels, phase, epoch):\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds,   average='macro', zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds,      average='macro', zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds,          average='macro', zero_division=0)\n",
    "\n",
    "    writer.add_scalar(f\"{phase}/Accuracy\", acc, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Precision\", prec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/Recall\", rec, epoch)\n",
    "    writer.add_scalar(f\"{phase}/F1-Score\", f1, epoch)\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57988a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from src.data_set       import StreamingPreferencesDataset\n",
    "from src.model          import StreamingPreferencesDatasetMLP\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    " \n",
    "BATCH_SIZE                  = 16\n",
    "DROPOUT_PERCENTAGE          = 0.2\n",
    "LEARNING_RATE               = 1e-2\n",
    "OPTIMIZER_STEP_SIZE         = 70\n",
    "EARLY_STOPPING_PATIENCE     = 1000\n",
    "EPOCHS                      = 20\n",
    "DEVICE                      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "train_dataset   = StreamingPreferencesDataset(X, y)\n",
    "val_dataset     = StreamingPreferencesDataset(X_val, y_val)\n",
    "train_loader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader      = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"temp\")\n",
    "\n",
    "\n",
    "all_labels = [int(label) for _, label in train_dataset]\n",
    "classes = np.unique(all_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "model           = StreamingPreferencesDatasetMLP(dropout_percentage=DROPOUT_PERCENTAGE).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "criterion       = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.3)  \n",
    "# optimizer       = optim.SGD(model.parameters(), lr=LEARNING_RATE) #momentum=0.9, \n",
    "optimizer     = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "# optimizer       = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.9, eps=1e-8)\n",
    "\n",
    "scheduler      = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.7, patience=10, threshold_mode='rel',threshold=1e-4)\n",
    "# scheduler     = optim.lr_scheduler.StepLR(step_size=OPTIMIZER_STEP_SIZE, optimizer=optimizer, gamma=0.4)\n",
    "# scheduler       = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c62ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def log_confusion_matrix_tensorboard(y_true, y_pred, label_map, writer, global_step):\n",
    " \n",
    "   \n",
    "    if torch.is_tensor(y_true):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if torch.is_tensor(y_pred):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "\n",
    " \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = [label_map[i] for i in range(len(label_map))]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=classes,\n",
    "        yticklabels=classes,\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label',\n",
    "        title='Confusion Matrix'\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    writer.add_figure(\"Confusion Matrix\", fig, global_step)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, writer):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss, total = 0.0, 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)  \n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        compute_metrics(writer, train_preds, train_labels, \"train\", epoch)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_total = 0.0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)  \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        compute_metrics(writer, val_preds, val_labels, \"val\", epoch)\n",
    "\n",
    "        scheduler.step(val_epoch_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar('LearningRate', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/val', val_epoch_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {epoch_loss:.4f} Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion=criterion, optimizer=optimizer, scheduler=scheduler, epochs=EPOCHS, writer=writer)\n",
    "torch.save(model.state_dict(), \"./models/codiax_model_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "\n",
    "def log_confusion_matrix_tensorboard(y_true, y_pred, label_map, writer, global_step=0):\n",
    "    num_classes = len(label_map)\n",
    "    print(num_classes)\n",
    "\n",
    "    cm_metric = MulticlassConfusionMatrix(num_classes=3)\n",
    "    confmat = cm_metric(y_pred, y_true)  \n",
    "\n",
    "\n",
    "    confmat = confmat.cpu().numpy()\n",
    "    labels = list(label_map.values())\n",
    "    df_cm = pd.DataFrame(confmat, index=labels, columns=labels)\n",
    "\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "    \n",
    "    writer.add_figure(\"ConfusionMatrix\", fig, global_step=global_step)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10200360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate_model(model, dataset, batch_size, device, label_map, y_true=None):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "    \n",
    "    final_preds = torch.cat(all_preds)\n",
    "    final_preds_np = final_preds.numpy()\n",
    "\n",
    "    decoded_preds = [label_map[int(p)] for p in final_preds_np]\n",
    "    label_counts = Counter(decoded_preds)\n",
    "    total = sum(label_counts.values())\n",
    "    label_percentages = {label: (count / total) * 100 for label, count in label_counts.items()}\n",
    "\n",
    "    df_percentages = pd.DataFrame({\n",
    "        'Label': list(label_percentages.keys()),\n",
    "        'Percentage': list(label_percentages.values())\n",
    "    }).sort_values(by='Label')\n",
    "\n",
    "    accuracy = None\n",
    "    if y_true is not None:\n",
    "        y_true_np = y_true.cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "        accuracy = accuracy_score(y_true_np, final_preds_np)\n",
    "\n",
    "    return df_percentages, accuracy, final_preds_np, final_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {0: \"Morning\", 1: \"Afternoon\", 2: \"Night\"}\n",
    "\n",
    "# test_dataset   = StreamingPreferencesDataset(X_test, y_test)\n",
    "\n",
    "# df_percentages, acc, final_preds_np, final_preds = evaluate_model(model, test_dataset, BATCH_SIZE, DEVICE, label_map, y_true=y_test)\n",
    "\n",
    "# print(df_percentages)\n",
    "# print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# y_true_np = final_preds_np.cpu().numpy() if torch.is_tensor(final_preds_np) else final_preds_np\n",
    "# y_pred_np = final_preds.cpu().numpy() if torch.is_tensor(final_preds) else final_preds\n",
    "\n",
    "# y_true_tensor = torch.tensor(y_true_np, dtype=torch.long)\n",
    "# y_pred_tensor = torch.tensor(y_pred_np, dtype=torch.long)\n",
    "\n",
    "# log_confusion_matrix_tensorboard(y_true=y_true_tensor, y_pred=y_pred_tensor, label_map=label_map, writer=writer, global_step=0)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "predictions = []\n",
    "\n",
    "test_dataset     = StreamingPreferencesDataset(X_test, y_test)\n",
    "test_loader    = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:  \n",
    "        inputs, _ = batch  \n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  \n",
    "        predictions.append(preds.cpu())\n",
    "\n",
    "final_preds = torch.cat(predictions)\n",
    "final_preds_np = final_preds.numpy()\n",
    "\n",
    "label_map = {0: \"Morning\", 1: \"Afternoon\", 2: \"Night\"}\n",
    "decoded_preds = [label_map[int(p)] for p in final_preds_np]\n",
    "\n",
    "label_counts = Counter(decoded_preds)\n",
    "\n",
    "total = sum(label_counts.values())\n",
    "label_percentages = {label: (count / total) * 100 for label, count in label_counts.items()}\n",
    "\n",
    "percentages = {\n",
    "    'Label': list(label_percentages.keys()),\n",
    "    'Percentage': list(label_percentages.values())\n",
    "}\n",
    "\n",
    "df_percentages = pd.DataFrame(percentages)\n",
    "df_percentages_sorted = df_percentages.sort_values(by='Label')\n",
    "\n",
    "print(df_percentages_sorted)\n",
    "\n",
    "accuracy = accuracy_score(y_test, final_preds_np)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16719f8b",
   "metadata": {},
   "source": [
    "### Training using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e03d63",
   "metadata": {},
   "source": [
    "Logistic Regression is a classic method of classification in machine learning. The difference between Logistic Regression and Liniar Regression is the capability of predicting continous outcomes. For multi-class clasification such as this one, Logistic Regression uses the softmax function. In this training process ```CrossEntropyLoss``` will be used as criterion. This removes the need of the explicity softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a33f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logistic_regression import LogisticRegression\n",
    "\n",
    "model           = LogisticRegression().to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "criterion      = nn.CrossEntropyLoss(label_smoothing=0.1)  \n",
    "# optimizer      = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer     = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler      = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.7, patience=10, threshold_mode='rel',threshold=1e-4)\n",
    "\n",
    "logisticWriter = SummaryWriter(log_dir=\"runs/Logistic,CrossEntropyLoss,ReduceLROnPlateau(patience=10, factor=0.7, threshold=1e-4),Adam(lr=1e-2),batch_size=64\")\n",
    "\n",
    "\n",
    "train_model(model=model, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer, scheduler=scheduler, epochs=EPOCHS, writer=logisticWriter)\n",
    "torch.save(model.state_dict(), \"./models/logistic_model_final.pth\")\n",
    "\n",
    "df_percentages, acc = evaluate_model(model, test_dataset, BATCH_SIZE, DEVICE, label_map, y_true=y_test)\n",
    "print(df_percentages)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Heart Project)",
   "language": "python",
   "name": "heart-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
